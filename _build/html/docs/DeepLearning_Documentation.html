

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Deep Learning &mdash; Pyspark_Documentation 0.1 documentation</title>
  

  
  
  
  

  
  <script type="text/javascript" src="../_static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
        <script src="../_static/language_data.js"></script>
        <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link href="../_static/style.css" rel="stylesheet" type="text/css">

</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../index.html" class="icon icon-home"> Pyspark_Documentation
          

          
          </a>

          
            
            
              <div class="version">
                0.1
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <ul>
<li class="toctree-l1"><a class="reference internal" href="../Pandas_Documentation.html">Python, Jupyter and Pandas</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Sklearn_Documentation.html">Scikit-learn</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Pyspark_Documentation.html">Pyspark</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Text_Mining_Documentation.html">Text Mining in Python</a></li>
<li class="toctree-l1"><a class="reference internal" href="../DeepLearning_Documentation.html">Deep Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../ReinforcementLearning_Documentation.html">Reinforcement Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Time_Series_inPython_Documentation.html">Time Series in Python</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Algorithms_Documentation.html">Algorithms</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Statistics_probabilities_Documentation.html">Statistics and Probabilities</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Plot_libraries_Documentation.html">Great plot libraries</a></li>
<li class="toctree-l1"><a class="reference internal" href="../SQL_Documentation.html">SQL Server documentation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Bash_Documentation.html">Useful Bash commands (or batch)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Bash_Documentation.html#useful-git-commands">Useful GIT commands</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Bash_Documentation.html#useful-vim-commands">Useful VIM commands</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Bash_Documentation.html#data-types">Data types</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Business_models_Documentation.html">Interesting business models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Sphinx_stuff.html">Sphinx</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">Pyspark_Documentation</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html">Docs</a> &raquo;</li>
        
      <li>Deep Learning</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="../_sources/docs/DeepLearning_Documentation.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="deep-learning">
<h1>Deep Learning<a class="headerlink" href="#deep-learning" title="Permalink to this headline">¶</a></h1>
<div class="section" id="introduction">
<h2>Introduction<a class="headerlink" href="#introduction" title="Permalink to this headline">¶</a></h2>
<p>The different “beasts” in deep learning:</p>
<div class="figure align-default">
<a class="reference internal image-reference" href="../_images/Networks1.png"><img alt="map to buried treasure" src="../_images/Networks1.png" style="width: 600.0px; height: 900.0px;" /></a>
</div>
<p>Deep learning is performing a universal function approximation. Provided that we have enough data, we can approximate any function.</p>
<div class="section" id="good-general-introduction-links">
<h3>Good general introduction links:<a class="headerlink" href="#good-general-introduction-links" title="Permalink to this headline">¶</a></h3>
<p>Intro to DL by playing, for both classification and regression: <a class="reference external" href="http://playground.tensorflow.org/">http://playground.tensorflow.org/</a></p>
<p><a class="reference external" href="https://towardsdatascience.com/a-weird-introduction-to-deep-learning-7828803693b0">https://towardsdatascience.com/a-weird-introduction-to-deep-learning-7828803693b0</a></p>
<p><a class="reference external" href="http://neuralnetworksanddeeplearning.com/chap1.html">http://neuralnetworksanddeeplearning.com/chap1.html</a> (Free online book)</p>
<p><a class="reference external" href="https://medium.com/&#64;dataturks/deep-learning-and-computer-vision-from-basic-implementation-to-efficient-methods-3ca994d50e90">https://medium.com/&#64;dataturks/deep-learning-and-computer-vision-from-basic-implementation-to-efficient-methods-3ca994d50e90</a></p>
<p>General intro: <a class="reference external" href="https://towardsdatascience.com/deep-learning-with-python-703e26853820">https://towardsdatascience.com/deep-learning-with-python-703e26853820</a></p>
<p>Vanishing gradient: <a class="reference external" href="https://www.quora.com/What-is-the-vanishing-gradient-problem">https://www.quora.com/What-is-the-vanishing-gradient-problem</a>
Vanishing gradient example: <a class="reference external" href="https://cs224d.stanford.edu/notebooks/vanishing_grad_example.html">https://cs224d.stanford.edu/notebooks/vanishing_grad_example.html</a> (exercise of clustering)</p>
<p>terminology: <a class="reference external" href="https://towardsdatascience.com/epoch-vs-iterations-vs-batch-size-4dfb9c7ce9c9">https://towardsdatascience.com/epoch-vs-iterations-vs-batch-size-4dfb9c7ce9c9</a></p>
</div>
<div class="section" id="terminology">
<h3>Terminology<a class="headerlink" href="#terminology" title="Permalink to this headline">¶</a></h3>
<p>epoch: One Epoch is when an ENTIRE dataset is passed forward and backward through the neural network only ONCE. We need indeed more than one epoch in order the optimizer (SGD or else) to update the weights of the net. One such iteration is not enough.</p>
<p>batch: the batch is a subset of the total dataset. The bacth size is the number of training examples present in a single batch. The total dataset is often divided into batches because it can sometimes be impossible to load the entire dataset in the neural net at once.</p>
<p>iterations: the iterations is the number of batches needed to complete one epoch. Note: The number of batches is equal to number of iterations for one epoch.</p>
<p>Let’s say we have dataset = 2000 training examples that we are going to use . We can divide the dataset of 2000 examples into batches of 500 then it will take 4 iterations to complete 1 epoch.</p>
</div>
<div class="section" id="activation-functions">
<h3>Activation functions<a class="headerlink" href="#activation-functions" title="Permalink to this headline">¶</a></h3>
<p>An activation function is a mapping of summed weighted input to the output of the neuron. It is called an activation/ transfer function because it governs the inception at which the neuron is activated and the strength of the output signal.
Mathematically,</p>
<p>Y = SUM (weight * input) + bias</p>
<p>We have many activation functions, out of which most used are relu, tanh, solfPlus.</p>
<p>Here is a list of most useful activation functions:</p>
<div class="figure align-default">
<a class="reference internal image-reference" href="../_images/ActivationFunction1.png"><img alt="ActivationFunction" src="../_images/ActivationFunction1.png" style="width: 1005.0px; height: 783.0px;" /></a>
</div>
<p>Softmax: The softmax function ensures that the output (most of time the last output, so the prediction) sums to one so that it can be interpreted as a probability.</p>
<p>ReLu</p>
<p>Leaky ReLu: almost same as ReLu, but in the negative side the values are not zero but decreasing slightly when going to the negative side. This gives better results than ReLu (Andrew Ng).</p>
<div class="figure align-default">
<a class="reference internal image-reference" href="../_images/ActivationFunction21.png"><img alt="ActivationFunction2" src="../_images/ActivationFunction21.png" style="width: 1246.0px; height: 632.0px;" /></a>
</div>
<p>Why do we need non-linear activation functions? Why not to take the identity function? Simply because that would leave the model to be a simple linear operation, such as there are no hidden layers. And adding a sigmoid function at the end would make it equal as a simple logistic regression.</p>
<div class="figure align-default">
<a class="reference internal image-reference" href="docs/Images/WhyNonLinearActivationFunctionsNeeded.png"><img alt="WhyNonLinearActivationFunctionsNeeded" src="docs/Images/WhyNonLinearActivationFunctionsNeeded.png" /></a>
</div>
</div>
<div class="section" id="back-propagation">
<h3>Back Propagation<a class="headerlink" href="#back-propagation" title="Permalink to this headline">¶</a></h3>
<p>The predicted value of the network is compared to the expected output, and an error is calculated using a function. This error is then propagated back within the whole network, one layer at a time, and the weights are updated according to the value that they contributed to the error. This clever bit of math is called the Back-Propagation algorithm. The process is repeated for all of the examples in your training data. One round of updating the network for the entire training dataset is called an epoch. A network may be trained for tens, hundreds or many thousands of epochs.</p>
<div class="figure align-default">
<a class="reference internal image-reference" href="docs/Images/BackPropagation.png"><img alt="Back-Propagation" src="docs/Images/BackPropagation.png" /></a>
</div>
</div>
<div class="section" id="loss-cost-functions">
<h3>Loss / Cost functions<a class="headerlink" href="#loss-cost-functions" title="Permalink to this headline">¶</a></h3>
<p>A loss (or cost) function, also known as an objective function, will specify the objective of minimizing loss/error, which our model will leverage to get the best performance over multiple epochsiterations. It again can be a string identifier to some pre-implemented loss functions like cross-entropy loss (classification) or mean squared error (regression) or it can be a custom loss function that we can develop.
The loss/cost function is the measure of “how good” a neural network did for it’s given training input and the expected output. It also may depend on attributes such as weights and biases.
A cost function is single-valued, not a vector because it rates how well the neural network performed as a whole. Using the Gradient Descent optimization algorithm, the weights are updated incrementally after each epoch.</p>
<ul class="simple">
<li><p>Regression: mean_squared_error</p></li>
<li><p>Classification: categorical_crossentropy (lower score is better)</p></li>
</ul>
<p>The full list for Keras is here: <a class="reference external" href="https://keras.io/losses/">https://keras.io/losses/</a></p>
</div>
<div class="section" id="convolutional-neural-networks-cnn">
<h3>Convolutional Neural Networks (CNN)<a class="headerlink" href="#convolutional-neural-networks-cnn" title="Permalink to this headline">¶</a></h3>
<p>The first few conv layers extract features like edges. The deeper conv layers extract complicated features like face, digits etc, that is the object of interest. This statement is an overgeneralization, but on a broader level this is true.</p>
<p>Here is a list of image classification datasets: <a class="reference external" href="https://dataturks.com/projects/Trending?type=IMAGE_CLASSIFICATION">https://dataturks.com/projects/Trending?type=IMAGE_CLASSIFICATION</a></p>
<div class="figure align-default">
<a class="reference internal image-reference" href="../_images/CNN1.png"><img alt="CNN" src="../_images/CNN1.png" style="width: 1000.0px; height: 341.0px;" /></a>
</div>
<p>Here is from scratch CNN (only Numpy needed): <a class="reference external" href="https://www.kdnuggets.com/2018/04/building-convolutional-neural-network-numpy-scratch.html">https://www.kdnuggets.com/2018/04/building-convolutional-neural-network-numpy-scratch.html</a></p>
</div>
<div class="section" id="convolution">
<h3>Convolution<a class="headerlink" href="#convolution" title="Permalink to this headline">¶</a></h3>
<p>On the other hand, Deep Learning simplifies the process of feature extraction through the process of convolution. Convolution is a mathematical operation, which maps out an energy function, which is a measure of similarity between two signals, or in our case images. So, when we use a blue filter and convolve it with white light, the resultant energy spectrum is that of blue light. Hence, the convolution of white light with a blue filter results in blue light. Hence term Convolutional Neural Networks, where feature extraction is done via the process of convolution.</p>
</div>
<div class="section" id="pooling">
<h3>Pooling<a class="headerlink" href="#pooling" title="Permalink to this headline">¶</a></h3>
<p>Pooling is a sub-sampling technique. The use of pooling is to reduce the dimension of the input image after getting convolved. There are two types, max pooling, and average pooling.</p>
<div class="figure align-default">
<a class="reference internal image-reference" href="docs/Images/MaxPooling.png"><img alt="MaxPooling" src="docs/Images/MaxPooling.png" /></a>
</div>
</div>
<div class="section" id="batch-normalization">
<h3>Batch Normalization<a class="headerlink" href="#batch-normalization" title="Permalink to this headline">¶</a></h3>
<p>We add this layer initially, to normalize all the features. Technically, batch normalization normalizes the output of a previous activation layer(initially, input layer) by subtracting the batch mean and dividing by the batch standard deviation. This makes the model more robust and learns effectively. Intuitively, we are preventing overfitting!</p>
</div>
<div class="section" id="dropout">
<h3>Dropout<a class="headerlink" href="#dropout" title="Permalink to this headline">¶</a></h3>
<p>This is another regularization technique that was used before Batch Norm. The way this works is, the weights are randomly juggled around by very small amounts… the model ends up learning variations and again prevents overfitting. Individual nodes are either dropped out of the net with probability 1-p or kept with probability p so that a reduced network is left; incoming and outgoing edges to a dropped-out node are also removed.</p>
<div class="figure align-default">
<a class="reference internal image-reference" href="docs/Images/Dropout.png"><img alt="Dropout" src="docs/Images/Dropout.png" /></a>
</div>
<p>Note on dropout:</p>
<ul class="simple">
<li><p>In general: <a class="reference external" href="https://machinelearningmastery.com/dropout-regularization-deep-learning-models-keras/">https://machinelearningmastery.com/dropout-regularization-deep-learning-models-keras/</a></p></li>
<li><p>For LSTM: <a class="reference external" href="https://machinelearningmastery.com/use-dropout-lstm-networks-time-series-forecasting/">https://machinelearningmastery.com/use-dropout-lstm-networks-time-series-forecasting/</a></p></li>
</ul>
</div>
<div class="section" id="zero-padding">
<h3>Zero-Padding<a class="headerlink" href="#zero-padding" title="Permalink to this headline">¶</a></h3>
<p>This helps prevent dimensionality loss, during convolution. Thus, for very deep networks, we usually prefer this. The zeros don’t add to the energy quotient during the convolution and help maintain dimensionality at a required level.</p>
</div>
<div class="section" id="fully-connected-networks-or-multiperceptron">
<h3>Fully Connected Networks, or MultiPerceptron<a class="headerlink" href="#fully-connected-networks-or-multiperceptron" title="Permalink to this headline">¶</a></h3>
<p>The output from the convolutional layers represents high-level features in the data. While that output could be flattened and connected to the output layer, adding a fully-connected layer is a way of learning non-linear combinations of these features. Essentially the convolutional layers are providing a meaningful, low-dimensional, and somewhat invariant feature space and the fully-connected layer is learning a nonlinear function induced by the activation functions, in that space. Similar to an artificial neural network architecture.</p>
<p>Somewhat confusingly, and for historical reasons, such multiple layer networks are sometimes called multilayer perceptrons or MLPs, despite being made up of sigmoid neurons, not perceptrons.</p>
<div class="figure align-default">
<a class="reference internal image-reference" href="../_images/MLP1.png"><img alt="MLP" src="../_images/MLP1.png" style="width: 619.0px; height: 355.0px;" /></a>
</div>
<p>See <a class="reference external" href="http://neuralnetworksanddeeplearning.com/chap1.html">http://neuralnetworksanddeeplearning.com/chap1.html</a></p>
</div>
</div>
<div class="section" id="keras">
<h2>Keras<a class="headerlink" href="#keras" title="Permalink to this headline">¶</a></h2>
<p>Keras is a high-level Deep Learning framework for Python, which is capable of running on top of both Theano and Tensorflow. Keras allows us to use the constructs offered by Tensorflow and Theano in a much more intuitive and easy-to-use way without writing excess boilerplate code for building neural network based models.</p>
<div class="figure align-default" id="id1">
<a class="reference internal image-reference" href="docs/Cheatsheets/Keras_Cheat_Sheet_Python.png"><img alt="map to buried treasure" src="docs/Cheatsheets/Keras_Cheat_Sheet_Python.png" /></a>
<p class="caption"><span class="caption-text">This Cheatsheet is taken from DataCamp.</span><a class="headerlink" href="#id1" title="Permalink to this image">¶</a></p>
</div>
<p>Install: on top of tensorflow: <a class="reference external" href="http://inmachineswetrust.com/posts/deep-learning-setup/">http://inmachineswetrust.com/posts/deep-learning-setup/</a></p>
<div class="section" id="loss-functions">
<h3>Loss functions<a class="headerlink" href="#loss-functions" title="Permalink to this headline">¶</a></h3>
<p>A loss function, also known as an objective function, will specify the objective of minimizing loss/error, which our model will leverage to get the best performance over multiple epochsiterations. It again can be a string identifier to some pre-implemented loss functions like cross-entropy loss (classification) or mean squared error (regression) or it can be a custom loss function that we can develop.</p>
<ul class="simple">
<li><p>Regression: mean_squared_error</p></li>
<li><p>Classification: categorical_crossentropy (lower score is better)</p></li>
</ul>
<p>The full list for Keras is here: <a class="reference external" href="https://keras.io/losses/">https://keras.io/losses/</a></p>
</div>
<div class="section" id="optimizers">
<h3>Optimizers<a class="headerlink" href="#optimizers" title="Permalink to this headline">¶</a></h3>
<p>The role of the optimizer is to find the weights parameters that minimize the loss function.</p>
<p>One could use a simple Gradient Descent algorithm, but experience shows that it can be very long before reaching the Global/Local minimum. The Stochastic Gradient Descent (SGD) was introduced to reduce the time of convergence, still keeping an acceptable accuracy. Stochastic gradient descent maintains a single learning rate (termed alpha) for all weight updates and the learning rate does not change during training.</p>
<p>Three main variants of the SGD are available:</p>
<ul class="simple">
<li><p>Adaptive Gradient Algorithm (AdaGrad) that maintains a per-parameter learning rate that improves performance on problems with sparse gradients (e.g. natural language and computer vision problems).</p></li>
<li><p>Root Mean Square Propagation (RMSProp) that also maintains per-parameter learning rates that are adapted based on the average of recent magnitudes of the gradients for the weight (e.g. how quickly it is changing). This means the algorithm does well on online and non-stationary problems (e.g. noisy).</p></li>
<li><p>Adam (the prefered one as to 2018, see here for a discussion: see <a class="reference external" href="https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/">https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/</a>. ):</p></li>
</ul>
<p>Why Adam?</p>
<ul class="simple">
<li><p>Adam combines the best properties of the AdaGrad and RMSProp algorithms to provide an optimization algorithm that can handle sparse gradients on noisy problems.</p></li>
<li><p>Adam is relatively easy to configure where the default configuration parameters do well on most problems.</p></li>
</ul>
<p>Instead of adapting the parameter learning rates based on the average first moment (the mean) as in RMSProp, Adam also makes use of the average of the second moments of the gradients (the uncentered variance).</p>
<ul class="simple">
<li><p>Intro to Adam: <a class="reference external" href="https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/">https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/</a></p></li>
<li><p>For a thorough review, see <a class="reference external" href="http://ruder.io/optimizing-gradient-descent/">http://ruder.io/optimizing-gradient-descent/</a></p></li>
<li><p>Nice post: <a class="reference external" href="https://medium.com/&#64;nishantnikhil/adam-optimizer-notes-ddac4fd7218">https://medium.com/&#64;nishantnikhil/adam-optimizer-notes-ddac4fd7218</a></p></li>
<li><p>The initial paper: <a class="reference external" href="https://arxiv.org/pdf/1412.6980.pdf">https://arxiv.org/pdf/1412.6980.pdf</a></p></li>
<li><p>For an intro to SGD: <a class="reference external" href="http://neuralnetworksanddeeplearning.com/chap1.html">http://neuralnetworksanddeeplearning.com/chap1.html</a> and associated code: <a class="reference external" href="https://github.com/mnielsen/neural-networks-and-deep-learning/blob/master/src/network.py">https://github.com/mnielsen/neural-networks-and-deep-learning/blob/master/src/network.py</a></p></li>
<li><p>comparison of optimizers: <a class="reference external" href="https://shaoanlu.wordpress.com/2017/05/29/sgd-all-which-one-is-the-best-optimizer-dogs-vs-cats-toy-experiment/">https://shaoanlu.wordpress.com/2017/05/29/sgd-all-which-one-is-the-best-optimizer-dogs-vs-cats-toy-experiment/</a></p></li>
</ul>
</div>
<div class="section" id="conversion-the-label-to-categorical-one-hot-encoding">
<h3>Conversion the label to categorical: One-Hot-Encoding<a class="headerlink" href="#conversion-the-label-to-categorical-one-hot-encoding" title="Permalink to this headline">¶</a></h3>
<p>The OHE is used to convert the labels to categorical columns, one column per category, as seen here:</p>
<div class="figure align-default">
<a class="reference internal image-reference" href="../_images/OneHotEncoding1.png"><img alt="Converting labels to categorical OHE labels" src="../_images/OneHotEncoding1.png" style="width: 408.0px; height: 232.0px;" /></a>
</div>
</div>
<div class="section" id="rnn-recurrent-neural-networks">
<h3>RNN: Recurrent Neural Networks<a class="headerlink" href="#rnn-recurrent-neural-networks" title="Permalink to this headline">¶</a></h3>
<p>In RNNs, connections form a cycle: they are able to retain state from one iteration to the next by using their own input for the next step.</p>
<p>Here are the different types of RNNs:</p>
<div class="figure align-default">
<a class="reference internal image-reference" href="../_images/one_multiple1.PNG"><img alt="autoencoders" src="../_images/one_multiple1.PNG" style="width: 650.4000000000001px; height: 346.40000000000003px;" /></a>
</div>
<p>Here is the vanilla RNN:</p>
<div class="figure align-default">
<a class="reference internal image-reference" href="../_images/RNN_vanilla_11.png"><img alt="autoencoders" src="../_images/RNN_vanilla_11.png" style="width: 583.2px; height: 332.0px;" /></a>
</div>
<p>w, u weights do not depend on t: same weights at all times.</p>
<p>Deep networks can be built by stacking recurrent units onto one another:</p>
<div class="figure align-default">
<a class="reference internal image-reference" href="../_images/RNN_vanilla_21.png"><img alt="autoencoders" src="../_images/RNN_vanilla_21.png" style="width: 566.4px; height: 337.6px;" /></a>
</div>
<p>RNN scheme is short term; it does not have a “memory” for far-past events.</p>
<p>Also, RNN suffers from vanishing gradients problem.</p>
<p>Good links: <a class="reference external" href="https://www.deeplearningbook.org/contents/rnn.html">https://www.deeplearningbook.org/contents/rnn.html</a></p>
<p>An RNN from scratch in Numpy ONLY (great): <a class="reference external" href="https://www.analyticsvidhya.com/blog/2019/01/fundamentals-deep-learning-recurrent-neural-networks-scratch-python/?utm_source=linkedin.com&amp;utm_medium=social">https://www.analyticsvidhya.com/blog/2019/01/fundamentals-deep-learning-recurrent-neural-networks-scratch-python/?utm_source=linkedin.com&amp;utm_medium=social</a></p>
</div>
<div class="section" id="lstm-long-short-term-memory">
<h3>LSTM: Long Short-Term Memory<a class="headerlink" href="#lstm-long-short-term-memory" title="Permalink to this headline">¶</a></h3>
<p>Here is the comparison of RNN vs LSTM</p>
<div class="figure align-default">
<a class="reference internal image-reference" href="docs/Images/RNN_vs_LSTM.png"><img alt="RNN_vs_LSTM" src="docs/Images/RNN_vs_LSTM.png" /></a>
</div>
<p>Here are details of the LSTM scheme:</p>
<div class="figure align-default">
<a class="reference internal image-reference" href="docs/Images/LSTM_details.png"><img alt="LSTM_details" src="docs/Images/LSTM_details.png" /></a>
</div>
<div class="figure align-default">
<a class="reference internal image-reference" href="docs/Images/LSTM_scheme.png"><img alt="LSTM scheme" src="docs/Images/LSTM_scheme.png" /></a>
</div>
<p>Good links:</p>
<p><a class="reference external" href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/">http://colah.github.io/posts/2015-08-Understanding-LSTMs/</a></p>
<p><a class="reference external" href="http://adventuresinmachinelearning.com/recurrent-neural-networks-lstm-tutorial-tensorflow/">http://adventuresinmachinelearning.com/recurrent-neural-networks-lstm-tutorial-tensorflow/</a></p>
<p>Post with Keras example: <a class="reference external" href="https://www.analyticsvidhya.com/blog/2017/12/fundamentals-of-deep-learning-introduction-to-lstm/">https://www.analyticsvidhya.com/blog/2017/12/fundamentals-of-deep-learning-introduction-to-lstm/</a></p>
<p>GRU (Gated Recurrent Unit): variant of LSTM</p>
</div>
<div class="section" id="autoencoders">
<h3>Autoencoders<a class="headerlink" href="#autoencoders" title="Permalink to this headline">¶</a></h3>
<p>Principle:</p>
<div class="figure align-default">
<a class="reference internal image-reference" href="../_images/Encoder_Decoder_idea1.PNG"><img alt="autoencoders" src="../_images/Encoder_Decoder_idea1.PNG" style="width: 452.8px; height: 206.4px;" /></a>
</div>
<p>Links:</p>
<p><a class="reference external" href="https://machinelearningmastery.com/encoder-decoder-long-short-term-memory-networks/">https://machinelearningmastery.com/encoder-decoder-long-short-term-memory-networks/</a></p>
<p><a class="reference external" href="https://machinelearningmastery.com/develop-encoder-decoder-model-sequence-sequence-prediction-keras/">https://machinelearningmastery.com/develop-encoder-decoder-model-sequence-sequence-prediction-keras/</a></p>
<p><a class="reference external" href="https://machinelearningmastery.com/define-encoder-decoder-sequence-sequence-model-neural-machine-translation-keras/">https://machinelearningmastery.com/define-encoder-decoder-sequence-sequence-model-neural-machine-translation-keras/</a> (more sequence to sequence)</p>
<p><a class="reference external" href="http://rickyhan.com/jekyll/update/2017/09/14/autoencoders.html">http://rickyhan.com/jekyll/update/2017/09/14/autoencoders.html</a></p>
<p><a class="reference external" href="https://machinelearningmastery.com/timedistributed-layer-for-long-short-term-memory-networks-in-python/">https://machinelearningmastery.com/timedistributed-layer-for-long-short-term-memory-networks-in-python/</a></p>
<p><a class="reference external" href="https://github.com/keras-team/keras/issues/1029">https://github.com/keras-team/keras/issues/1029</a></p>
<p><a class="reference external" href="https://github.com/keras-team/keras/issues/5203">https://github.com/keras-team/keras/issues/5203</a> #!!!</p>
<p><a class="reference external" href="https://keras.io/layers/wrappers/">https://keras.io/layers/wrappers/</a></p>
<p><a class="reference external" href="https://keras.io/layers/core/">https://keras.io/layers/core/</a></p>
<p><a class="reference external" href="https://blog.keras.io/building-autoencoders-in-keras.html">https://blog.keras.io/building-autoencoders-in-keras.html</a></p>
<p><a class="reference external" href="https://machinelearningmastery.com/lstm-autoencoders/">https://machinelearningmastery.com/lstm-autoencoders/</a></p>
<p>Autoencoder with CNN: <a class="reference external" href="https://machinelearningmastery.com/how-to-develop-convolutional-neural-network-models-for-time-series-forecasting/">https://machinelearningmastery.com/how-to-develop-convolutional-neural-network-models-for-time-series-forecasting/</a></p>
<p>TimeDistributed layer: allows to apply a layer to every temporal slice of an input.</p>
<p>There are two key points to remember when using the TimeDistributed wrapper layer:</p>
<ul class="simple">
<li><p>The input must be (at least) 3D. This often means that you will need to configure your last LSTM layer prior to your TimeDistributed wrapped Dense layer to return sequences (e.g. set the “return_sequences” argument to “True”).</p></li>
<li><p>The output will be 3D. This means that if your TimeDistributed wrapped Dense layer is your output layer and you are predicting a sequence, you will need to resize your y array into a 3D vector.</p></li>
</ul>
<p>TimeDistributedDense applies a same Dense (fully-connected) operation to every timestep of a 3D tensor.</p>
<p>Here taken from <a class="reference external" href="https://github.com/keras-team/keras/issues/1029">https://github.com/keras-team/keras/issues/1029</a>:
But I think you still don’t catch the point. The most common scenario for using TimeDistributedDense is using a recurrent NN for tagging task.e.g. POS labeling or slot filling task.</p>
<p>In this kind of task:
For each sample, the input is a sequence (a1,a2,a3,a4…aN) and the output is a sequence (b1,b2,b3,b4…bN) with the same length. bi could be viewed as the label of ai.
Push a1 into a recurrent nn to get output b1. Than push a2 and the hidden output of a1 to get b2…</p>
<p>If you want to model this by Keras, you just need to used a TimeDistributedDense after a RNN or LSTM layer(with return_sequence=True) to make the cost function is calculated on all time-step output. If you don’t use TimeDistributedDense ans set the return_sequence of RNN=False, then the cost is calculated on the last time-step output and you could only get the last bN.</p>
<p>I am also new to Keras, but I am trying to use it to do sequence labeling and I find this could only be done by using TimeDistributedDense. If I make something wrong, please correct me.</p>
<p>Link: <a class="reference external" href="https://machinelearningmastery.com/timedistributed-layer-for-long-short-term-memory-networks-in-python/">https://machinelearningmastery.com/timedistributed-layer-for-long-short-term-memory-networks-in-python/</a></p>
</div>
<div class="section" id="variational-autoencoders">
<h3>Variational autoencoders<a class="headerlink" href="#variational-autoencoders" title="Permalink to this headline">¶</a></h3>
<p><a class="reference external" href="https://towardsdatascience.com/intuitively-understanding-variational-autoencoders-1bfe67eb5daf">https://towardsdatascience.com/intuitively-understanding-variational-autoencoders-1bfe67eb5daf</a></p>
<p><a class="reference external" href="https://medium.com/datadriveninvestor/variational-autoencoder-vae-d1cf436e1e8f">https://medium.com/datadriveninvestor/variational-autoencoder-vae-d1cf436e1e8f</a></p>
<p>Variational Autoencoders (VAEs) have one fundamentally unique property that separates them from vanilla autoencoders, and it is this property that makes them so useful for generative modeling: their latent spaces are, by design, continuous, allowing easy random sampling and interpolation.</p>
<p>It achieves this by doing something that seems rather surprising at first: making its encoder not output an encoding vector of size n, rather, outputting two vectors of size n: a vector of means, μ, and another vector of standard deviations, σ.</p>
<div class="figure align-default">
<a class="reference internal image-reference" href="../_images/VAE11.PNG"><img alt="VAE encoder" src="../_images/VAE11.PNG" style="width: 456.4px; height: 510.29999999999995px;" /></a>
</div>
<p>They form the parameters of a vector of random variables of length n, with the i th element of μ and σ being the mean and standard deviation of the i th random variable, X i, from which we sample, to obtain the sampled encoding which we pass onward to the decoder:</p>
<div class="figure align-default">
<a class="reference internal image-reference" href="../_images/VAE21.PNG"><img alt="VAE encoder" src="../_images/VAE21.PNG" style="width: 480.9px; height: 282.09999999999997px;" /></a>
</div>
<p>This stochastic generation means, that even for the same input, while the mean and standard deviations remain the same, the actual encoding will somewhat vary on every single pass simply due to sampling.</p>
<div class="figure align-default">
<a class="reference internal image-reference" href="../_images/Autoencoder_vs_VAE1.PNG"><img alt="Autoencoder_vs_VAE" src="../_images/Autoencoder_vs_VAE1.PNG" style="width: 464.79999999999995px; height: 266.7px;" /></a>
</div>
<p>Intuitively, the mean vector controls where the encoding of an input should be centered around, while the standard deviation controls the “area”, how much from the mean the encoding can vary. As encodings are generated at random from anywhere inside the “circle” (the distribution), the decoder learns that not only is a single point in latent space referring to a sample of that class, but all nearby points refer to the same as well. This allows the decoder to not just decode single, specific encodings in the latent space (leaving the decodable latent space discontinuous), but ones that slightly vary too, as the decoder is exposed to a range of variations of the encoding of the same input during training</p>
<p>See also p 298 of “Deep learning in Python” from Chollet.</p>
</div>
<div class="section" id="genetic-algorithm-hyperparameters-tuning">
<h3>Genetic Algorithm hyperparameters tuning<a class="headerlink" href="#genetic-algorithm-hyperparameters-tuning" title="Permalink to this headline">¶</a></h3>
<p>Problem: the discovery of the best hyperparameters of a neural network is very time consuming, especially when it is done brute force.
Here, we try to improve upon the brute force method by applying a genetic algorithm to evolve a network with the goal of achieving optimal hyperparameters in a fraction the time of a brute force search.</p>
<p>It is said that a 80% time saving can be obtained: <a class="reference external" href="https://blog.coast.ai/lets-evolve-a-neural-network-with-a-genetic-algorithm-code-included-8809bece164">https://blog.coast.ai/lets-evolve-a-neural-network-with-a-genetic-algorithm-code-included-8809bece164</a>
(assuming best parameters are found…)</p>
<p>What’s a genetic algorithm? Genetic algorithms are commonly used to generate high-quality solutions to optimization and search problems by relying on bio-inspired operators such as mutation, crossover and selection. — Wikipedia
(See here: <a class="reference external" href="https://lethain.com/genetic-algorithms-cool-name-damn-simple/">https://lethain.com/genetic-algorithms-cool-name-damn-simple/</a>)</p>
<p>See even simpler example here, based on Numpy only: <a class="reference external" href="https://www.kdnuggets.com/2018/07/genetic-algorithm-implementation-python.html">https://www.kdnuggets.com/2018/07/genetic-algorithm-implementation-python.html</a></p>
<p>First: how do genetic algorithms work? At its core, a genetic algorithm…</p>
<ol class="arabic simple">
<li><p>Creates a population of (randomly generated) members</p></li>
<li><p>Scores each member of the population based on some goal. This score is called a fitness function.</p></li>
<li><p>Selects and breeds the best members of the population to produce more like them</p></li>
<li><p>Mutates some members randomly to attempt to find even better candidates</p></li>
<li><p>Kills off the rest (survival of the fittest and all), and</p></li>
<li><p>Repeats from step 2. Each iteration through these steps is called a generation.</p></li>
</ol>
<p>Repeat this process enough times and you should be left with the very best possible members of a population.</p>
<p>See <a class="reference external" href="https://github.com/harvitronix/neural-network-genetic-algorithm">https://github.com/harvitronix/neural-network-genetic-algorithm</a> for a code intro.</p>
</div>
<div class="section" id="gan-generative-adversarial-networks">
<h3>GAN: Generative Adversarial Networks<a class="headerlink" href="#gan-generative-adversarial-networks" title="Permalink to this headline">¶</a></h3>
<p>Intro: <a class="reference external" href="https://medium.com/&#64;devnag/generative-adversarial-networks-gans-in-50-lines-of-code-pytorch-e81b79659e3f">https://medium.com/&#64;devnag/generative-adversarial-networks-gans-in-50-lines-of-code-pytorch-e81b79659e3f</a></p>
<p><a class="reference external" href="https://www.dlology.com/blog/how-to-do-novelty-detection-in-keras-with-generative-adversarial-network/">https://www.dlology.com/blog/how-to-do-novelty-detection-in-keras-with-generative-adversarial-network/</a></p>
<p>GAN is a family of Neural Network (NN) models that have two or more NN components (Generator/Discriminator) competing adversarially with each other that result in component NNs get better over time.</p>
<p>The models play two distinct (literally, adversarial) roles. Given some real data set R, G is the generator, trying to create fake data that looks just like the genuine data, while D is the discriminator, getting data from either the real set or G and labeling the difference. Goodfellow’s metaphor (and a fine one it is) was that G was like a team of forgers trying to match real paintings with their output, while D was the team of detectives trying to tell the difference. (Except that in this case, the forgers G never get to see the original data — only the judgments of D. They’re like blind forgers.)</p>
<p>Here is the metaphor of Ian Goodfellow:</p>
<div class="figure align-default">
<a class="reference internal image-reference" href="../_images/GAN_metaphor1.PNG"><img alt="Here is the metaphor of Ian Goodfellow" src="../_images/GAN_metaphor1.PNG" style="width: 421.5px; height: 193.0px;" /></a>
</div>
<p>R: The original, genuine data set</p>
<p>I: The random noise that goes into the generator as a source of entropy</p>
<p>G: The generator which tries to copy/mimic the original data set</p>
<p>D: The discriminator which tries to tell apart G’s output from R</p>
<p>The actual ‘training’ loop where we teach G to trick D and D to beware G.</p>
<p>The GAN can be seen as a minmax game:</p>
<div class="figure align-default">
<a class="reference internal image-reference" href="../_images/GAN_minmax_game_11.PNG"><img alt="GAN_minmax_game_1.PNG" src="../_images/GAN_minmax_game_11.PNG" style="width: 734.5px; height: 288.5px;" /></a>
</div>
<div class="figure align-default">
<a class="reference internal image-reference" href="../_images/GAN_minmax_game_21.PNG"><img alt="GAN_minmax_game_2.PNG" src="../_images/GAN_minmax_game_21.PNG" style="width: 510.5px; height: 226.0px;" /></a>
</div>
<p>GANs can be used for anomaly detection:</p>
<div class="figure align-default">
<a class="reference internal image-reference" href="../_images/Gan_anomaly_detection1.PNG"><img alt="Gan_anomaly_detection.PNG" src="../_images/Gan_anomaly_detection1.PNG" style="width: 692.5px; height: 334.5px;" /></a>
</div>
</div>
<div class="section" id="simple-nn-examples">
<h3>Simple NN examples<a class="headerlink" href="#simple-nn-examples" title="Permalink to this headline">¶</a></h3>
<ol class="arabic simple">
<li><p><strong>CLASSIFICATION</strong>: Example making new class predictions for a classification problem</p></li>
</ol>
<p>#Taken from <a class="reference external" href="https://machinelearningmastery.com/how-to-make-classification-and-regression-predictions-for-deep-learning-models-in-keras/">https://machinelearningmastery.com/how-to-make-classification-and-regression-predictions-for-deep-learning-models-in-keras/</a></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">keras.models</span> <span class="kn">import</span> <span class="n">Sequential</span>
<span class="kn">from</span> <span class="nn">keras.layers</span> <span class="kn">import</span> <span class="n">Dense</span>
<span class="kn">from</span> <span class="nn">sklearn.datasets.samples_generator</span> <span class="kn">import</span> <span class="n">make_blobs</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">MinMaxScaler</span>

<span class="c1"># generate 2d classification dataset</span>
<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">make_blobs</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">centers</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">n_features</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">scalar</span> <span class="o">=</span> <span class="n">MinMaxScaler</span><span class="p">()</span>
<span class="n">scalar</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">scalar</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

<span class="c1"># define and fit the final model</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="n">input_dim</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;sigmoid&#39;</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="s1">&#39;binary_crossentropy&#39;</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="s1">&#39;adam&#39;</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="c1"># new instances where we do not know the answer</span>
<span class="n">Xnew</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">make_blobs</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">centers</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">n_features</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">Xnew</span> <span class="o">=</span> <span class="n">scalar</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">Xnew</span><span class="p">)</span>
<span class="c1"># make a prediction</span>
<span class="n">ynew</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict_classes</span><span class="p">(</span><span class="n">Xnew</span><span class="p">)</span>
<span class="c1"># show the inputs and predicted outputs</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">Xnew</span><span class="p">)):</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;X=</span><span class="si">%s</span><span class="s2">, Predicted=</span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">Xnew</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">ynew</span><span class="p">[</span><span class="n">i</span><span class="p">]))</span>

<span class="c1">#Here just for visual check</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span><span class="n">X</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span><span class="s1">&#39;bo&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">Xnew</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span><span class="n">Xnew</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span><span class="s1">&#39;ro&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="c1">#output:</span>
<span class="n">X</span><span class="o">=</span><span class="p">[</span><span class="mf">0.89337759</span> <span class="mf">0.65864154</span><span class="p">],</span> <span class="n">Predicted</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">X</span><span class="o">=</span><span class="p">[</span><span class="mf">0.29097707</span> <span class="mf">0.12978982</span><span class="p">],</span> <span class="n">Predicted</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
<span class="n">X</span><span class="o">=</span><span class="p">[</span><span class="mf">0.78082614</span> <span class="mf">0.75391697</span><span class="p">],</span> <span class="n">Predicted</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</pre></div>
</div>
<div class="figure align-default">
<a class="reference internal image-reference" href="../_images/Classification_keras1.png"><img alt="Check of the classification of the new entries" src="../_images/Classification_keras1.png" style="width: 300.0px; height: 201.60000000000002px;" /></a>
</div>
<p>Note: Another type of prediction you may wish to make is the probability of the data instance belonging to each class:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">Same</span><span class="p">,</span> <span class="n">use</span> <span class="n">ynew</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">Xnew</span><span class="p">)</span> <span class="ow">in</span> <span class="n">place</span> <span class="n">of</span> <span class="n">ynew</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict_classes</span><span class="p">(</span><span class="n">Xnew</span><span class="p">)</span>
</pre></div>
</div>
<ol class="arabic simple" start="2">
<li><p><strong>REGRESSION</strong>: Example of making predictions for a regression problem</p></li>
</ol>
<p>#Taken from <a class="reference external" href="https://machinelearningmastery.com/how-to-make-classification-and-regression-predictions-for-deep-learning-models-in-keras/">https://machinelearningmastery.com/how-to-make-classification-and-regression-predictions-for-deep-learning-models-in-keras/</a></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">keras.models</span> <span class="kn">import</span> <span class="n">Sequential</span>
<span class="kn">from</span> <span class="nn">keras.layers</span> <span class="kn">import</span> <span class="n">Dense</span>
<span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_regression</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">MinMaxScaler</span>

<span class="c1"># generate regression dataset</span>
<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">make_regression</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">n_features</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">noise</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">scalarX</span><span class="p">,</span> <span class="n">scalarY</span> <span class="o">=</span> <span class="n">MinMaxScaler</span><span class="p">(),</span> <span class="n">MinMaxScaler</span><span class="p">()</span>
<span class="n">scalarX</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">scalarY</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">y</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">scalarX</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">scalarY</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">y</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span>

<span class="c1"># define and fit the final model</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="n">input_dim</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;linear&#39;</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="s1">&#39;mse&#39;</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="s1">&#39;adam&#39;</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="c1"># new instances where we do not know the answer</span>
<span class="n">Xnew</span><span class="p">,</span> <span class="n">a</span> <span class="o">=</span> <span class="n">make_regression</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">n_features</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">noise</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">Xnew</span> <span class="o">=</span> <span class="n">scalarX</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">Xnew</span><span class="p">)</span>

<span class="c1"># make a prediction</span>
<span class="n">ynew</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">Xnew</span><span class="p">)</span>
<span class="c1"># show the inputs and predicted outputs</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">Xnew</span><span class="p">)):</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;X=</span><span class="si">%s</span><span class="s2">, Predicted=</span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">Xnew</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">ynew</span><span class="p">[</span><span class="n">i</span><span class="p">]))</span>

<span class="c1">#Here just for visual check</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="n">ax</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span><span class="n">y</span><span class="p">,</span><span class="s1">&#39;bo&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">Xnew</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span><span class="n">ynew</span><span class="p">,</span><span class="s1">&#39;ro&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;y&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;X[:,0]&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span><span class="n">y</span><span class="p">,</span><span class="s1">&#39;bo&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">Xnew</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span><span class="n">ynew</span><span class="p">,</span><span class="s1">&#39;ro&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;X[:,1]&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="c1">#output:</span>
<span class="n">X</span><span class="o">=</span><span class="p">[</span><span class="mf">0.29466096</span> <span class="mf">0.30317302</span><span class="p">],</span> <span class="n">Predicted</span><span class="o">=</span><span class="p">[</span><span class="mf">0.17338811</span><span class="p">]</span>
<span class="n">X</span><span class="o">=</span><span class="p">[</span><span class="mf">0.39445118</span> <span class="mf">0.79390858</span><span class="p">],</span> <span class="n">Predicted</span><span class="o">=</span><span class="p">[</span><span class="mf">0.7450506</span><span class="p">]</span>
<span class="n">X</span><span class="o">=</span><span class="p">[</span><span class="mf">0.02884127</span> <span class="mf">0.6208843</span> <span class="p">],</span> <span class="n">Predicted</span><span class="o">=</span><span class="p">[</span><span class="mf">0.4035678</span><span class="p">]</span>
</pre></div>
</div>
<div class="figure align-default">
<a class="reference internal image-reference" href="../_images/Regression_keras1.png"><img alt="Check of the regression of the new entries" src="../_images/Regression_keras1.png" style="width: 312.0px; height: 212.8px;" /></a>
</div>
<ol class="arabic simple" start="3">
<li><p>LSTM example</p></li>
</ol>
<p>Taken from: <a class="reference external" href="https://machinelearningmastery.com/make-predictions-long-short-term-memory-models-keras/">https://machinelearningmastery.com/make-predictions-long-short-term-memory-models-keras/</a></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">keras.models</span> <span class="kn">import</span> <span class="n">Sequential</span>
<span class="kn">from</span> <span class="nn">keras.layers</span> <span class="kn">import</span> <span class="n">Dense</span>
<span class="kn">from</span> <span class="nn">keras.layers</span> <span class="kn">import</span> <span class="n">LSTM</span>
<span class="kn">from</span> <span class="nn">numpy</span> <span class="kn">import</span> <span class="n">array</span>
<span class="kn">from</span> <span class="nn">keras.models</span> <span class="kn">import</span> <span class="n">load_model</span>

<span class="c1"># return training data</span>
<span class="k">def</span> <span class="nf">get_train</span><span class="p">():</span>
    <span class="n">seq</span> <span class="o">=</span> <span class="p">[[</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.4</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.4</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">]]</span>
    <span class="n">seq</span> <span class="o">=</span> <span class="n">array</span><span class="p">(</span><span class="n">seq</span><span class="p">)</span>
    <span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">seq</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">seq</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="p">),</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span>

<span class="c1"># define model</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">LSTM</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="n">input_shape</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;linear&#39;</span><span class="p">))</span>
<span class="c1"># compile model</span>
<span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="s1">&#39;mse&#39;</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="s1">&#39;adam&#39;</span><span class="p">)</span>
<span class="c1"># fit model</span>
<span class="n">X</span><span class="p">,</span><span class="n">y</span> <span class="o">=</span> <span class="n">get_train</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">300</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="c1"># save model to single file</span>
<span class="n">model</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="s1">&#39;lstm_model.h5&#39;</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</pre></div>
</div>
<p>Then the model can be loaded again (from a different script in a different Python session) using the load_model() function.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">keras.models</span> <span class="kn">import</span> <span class="n">load_model</span>
<span class="c1"># load model from single file</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">load_model</span><span class="p">(</span><span class="s1">&#39;lstm_model.h5&#39;</span><span class="p">)</span>
<span class="c1"># make predictions</span>
<span class="n">yhat</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">yhat</span><span class="p">)</span>

<span class="c1">#output</span>
<span class="p">[[</span><span class="mf">0.23529154</span><span class="p">]</span>
 <span class="p">[</span><span class="mf">0.27136612</span><span class="p">]</span>
 <span class="p">[</span><span class="mf">0.3086475</span> <span class="p">]</span>
 <span class="p">[</span><span class="mf">0.34707576</span><span class="p">]</span>
 <span class="p">[</span><span class="mf">0.38658726</span><span class="p">]]</span>
</pre></div>
</div>
<p>Again we can distinguish between predict(), predict_proba() and predict_classes():</p>
</div>
<div class="section" id="petastorm">
<h3>Petastorm<a class="headerlink" href="#petastorm" title="Permalink to this headline">¶</a></h3>
<p>#https://github.com/uber/petastorm
#https://docs.azuredatabricks.net/applications/deep-learning/data-prep/petastorm.html
#https://petastorm.readthedocs.io/en/latest/readme_include.html</p>
<p>Note that Petastorm produces Datasets that deliver data in batches that depends
entirely on the Parquet files’ row group size. To control the batch size for
training, it’s necessary to use Tensorflow’s unbatch() and batch() operations
to re-batch the data into the right size. Also, note the small workaround that’s
currently necessary to avoid a problem in reading Parquet files via Arrow in
Petastorm.</p>
</div>
</div>
<div class="section" id="tensorflow">
<h2>Tensorflow<a class="headerlink" href="#tensorflow" title="Permalink to this headline">¶</a></h2>
<div class="section" id="installation">
<h3>Installation<a class="headerlink" href="#installation" title="Permalink to this headline">¶</a></h3>
<p>Here is a cheatsheet taken from Tensorflow.</p>
<p>better: <a class="reference external" href="http://inmachineswetrust.com/posts/deep-learning-setup/">http://inmachineswetrust.com/posts/deep-learning-setup/</a></p>
<div class="figure align-default">
<a class="reference internal image-reference" href="../_images/TS_Linux1.png"><img alt="Installation in Linux/Mac" src="../_images/TS_Linux1.png" style="width: 391.5px; height: 162.0px;" /></a>
</div>
<div class="figure align-default">
<a class="reference internal image-reference" href="../_images/TS_Windows1.png"><img alt="Installation in Windows" src="../_images/TS_Windows1.png" style="width: 393.0px; height: 164.5px;" /></a>
</div>
<p>Here is a cheatsheet taken from Tensorflow.</p>
<div class="figure align-default">
<a class="reference internal image-reference" href="../_images/TensorFlow_cheatsheet1.png"><img alt="map to buried treasure" src="../_images/TensorFlow_cheatsheet1.png" style="width: 2000.0px; height: 2812.0px;" /></a>
</div>
</div>
<div class="section" id="main-tensorflow-outline">
<h3>Main Tensorflow outline<a class="headerlink" href="#main-tensorflow-outline" title="Permalink to this headline">¶</a></h3>
<p>Here we will introduce the general flow of TensorFlow algorithms. Most recipes will follow this outline:</p>
<ol class="arabic simple">
<li><p><strong>Import or generate datasets</strong></p></li>
<li><p><strong>Transform and normalize data</strong>: We will have to transform our data before we can use it, put in TensorFlow format. Most algorithms also expect normalized data. TensorFlow has built-in functions that can normalize the data for you as follows:</p></li>
</ol>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">data</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">batch_norm_with_global_normalization</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
</pre></div>
</div>
<ol class="arabic simple" start="3">
<li><p><strong>Partition datasets into train, test, and validation sets</strong></p></li>
<li><p><strong>Set algorithm parameters (hyperparameters)</strong>: Our algorithms usually have a set of parameters that we hold constant throughout the procedure. For example, this can be the number of iterations, the learning rate, or other fixed parameters of our choosing. It is considered good form to initialize these together so the user can easily find them, as follows:</p></li>
</ol>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">0.01</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">iterations</span> <span class="o">=</span> <span class="mi">1000</span>
</pre></div>
</div>
<p>5. <strong>Initialize variables and placeholders</strong>: TensorFlow depends on knowing what it can and cannot modify. TensorFlow will modify/adjust the variables and weight/bias during optimization to minimize a
loss function. To accomplish this, we feed in data through placeholders. We need to initialize both of these variables and placeholders with size and type, so that TensorFlow knows what to expect. See the following code:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">a_var</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">x_input</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="n">input_size</span><span class="p">])</span>
<span class="n">y_input</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">])</span>
</pre></div>
</div>
<ol class="arabic simple" start="6">
<li><p><strong>Define the model structure</strong>: This is done by building a computational graph. TensorFlow chooses what operations and values must be the variables and placeholders to arrive at our model outcomes. For example, for a linear model:</p></li>
</ol>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">y_pred</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">mul</span><span class="p">(</span><span class="n">x_input</span><span class="p">,</span> <span class="n">weight_matrix</span><span class="p">),</span> <span class="n">b_matrix</span><span class="p">)</span>
</pre></div>
</div>
<ol class="arabic simple" start="7">
<li><p><strong>Declare the loss functions</strong>: After defining the model, we must be able to evaluate the output. This is where we declare the loss function. The loss function is very important as it tells us how far off our predictions are from the actual values. Here is an example of loss function:</p></li>
</ol>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span>loss = tf.reduce_mean(tf.square(y_actual – y_pred))
</pre></div>
</div>
<ol class="arabic simple" start="8">
<li><p><strong>Initialize and train the model</strong>: Now that we have everything in place, we need to create an instance of our graph, feed in the data through the placeholders, and let TensorFlow change the variables to better predict our training data. Here is one way to initialize the computational graph:</p></li>
</ol>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">Session</span><span class="p">(</span><span class="n">graph</span><span class="o">=</span><span class="n">graph</span><span class="p">)</span> <span class="k">as</span> <span class="n">session</span><span class="p">:</span>
  <span class="o">...</span>
  <span class="n">session</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
  <span class="o">...</span>
</pre></div>
</div>
<ol class="arabic simple" start="9">
<li><p><strong>Evaluate the model</strong>: Once we have built and trained the model, we should evaluate the model by looking at how well it does with new data through some specified criteria. We evaluate on the train and test set and these evaluations will allow us to see if the model is underfit or overfit.</p></li>
<li><p><strong>Tune hyperparameters</strong>: Most of the time, we will want to go back and change some of the hyperparamters, based on the model performance. We then repeat the previous steps with different hyperparameters and evaluate the model on the validation set.</p></li>
<li><p><strong>Deploy/predict new outcomes</strong>: It is also important to know how to make predictions on new, unseen, data. We can do this with all of our models, once we have them trained.</p></li>
</ol>
</div>
<div class="section" id="graph-session">
<h3>Graph, Session<a class="headerlink" href="#graph-session" title="Permalink to this headline">¶</a></h3>
<p>The structure of TensorFlow programs is made of Graph and Session:</p>
<div class="figure align-default">
<a class="reference internal image-reference" href="../_images/Graph1.png"><img alt="Session is Graph with Data" src="../_images/Graph1.png" style="width: 446.5px; height: 235.5px;" /></a>
</div>
<div class="figure align-default">
<a class="reference internal image-reference" href="../_images/Graph21.png"><img alt="Session is Graph with Data" src="../_images/Graph21.png" style="width: 463.5px; height: 216.5px;" /></a>
</div>
<p>A graph is made of:</p>
<ul class="simple">
<li><p>Placeholders: gates where we introduce example</p></li>
<li><p>Model: makes predictions. Set of variables and operations</p></li>
<li><p>Cost function: function that computes the model error</p></li>
<li><p>Optimizer: algorithm that optimizes the variables so the cost would be zero</p></li>
</ul>
<p>Then the session is the Graph with the Data entered:</p>
<div class="figure align-default">
<a class="reference internal image-reference" href="../_images/Session1.png"><img alt="Session is Graph with Data" src="../_images/Session1.png" style="width: 431.0px; height: 213.0px;" /></a>
</div>
<p>For example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1">##### GRAPH #####</span>
<span class="n">a</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
<span class="n">sum_graph</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
<span class="c1">##### DATA #####</span>
<span class="n">num1</span> <span class="o">=</span> <span class="mi">3</span>
<span class="n">num2</span> <span class="o">=</span> <span class="mi">8</span>
<span class="c1">##### SESSION #####</span>
<span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">Session</span><span class="p">()</span> <span class="k">as</span> <span class="n">sess</span><span class="p">:</span>
  <span class="n">sum_outcome</span> <span class="o">=</span> <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">sum_graph</span><span class="p">,</span> <span class="n">feed_dict</span><span class="o">=</span><span class="p">{</span>
  <span class="n">a</span><span class="p">:</span> <span class="n">num1</span><span class="p">,</span>
  <span class="n">b</span><span class="p">:</span> <span class="n">num2</span>
  <span class="p">})</span>
  <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;The sum of </span><span class="si">{}</span><span class="s2"> and </span><span class="si">{}</span><span class="s2"> is </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">num1</span><span class="p">,</span><span class="n">num2</span><span class="p">,</span><span class="n">sum_outcome</span><span class="p">))</span>

<span class="n">The</span> <span class="nb">sum</span> <span class="n">of</span> <span class="mi">3</span> <span class="ow">and</span> <span class="mi">8</span> <span class="ow">is</span> <span class="mi">11</span>
</pre></div>
</div>
</div>
<div class="section" id="data-types">
<h3>Data types<a class="headerlink" href="#data-types" title="Permalink to this headline">¶</a></h3>
<p><strong>Matrices</strong>: here we create 5 matrices (2D arrays):</p>
<ul class="simple">
<li><p>identity matrix</p></li>
<li><p>truncated normal distribution</p></li>
<li><p>an array with one fixed value</p></li>
<li><p>a uniform distribution array</p></li>
<li><p>an array conversion from Numpy</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">identity_matrix</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">diag</span><span class="p">([</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">])</span>
<span class="n">A</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">truncated_normal</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span> <span class="c1">#or A = tf.truncated_normal([row_dim, col_dim],mean=0.0, stddev=1.0)</span>
<span class="n">B</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">fill</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">],</span> <span class="mf">5.0</span><span class="p">)</span>
<span class="n">C</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">random_uniform</span><span class="p">([</span><span class="mi">3</span><span class="p">,</span><span class="mi">2</span><span class="p">])</span>
<span class="n">D</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">2.</span><span class="p">,</span> <span class="mf">3.</span><span class="p">],[</span><span class="o">-</span><span class="mf">3.</span><span class="p">,</span> <span class="o">-</span><span class="mf">7.</span><span class="p">,</span><span class="o">-</span><span class="mf">1.</span><span class="p">],[</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">5.</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.</span><span class="p">]]))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">identity_matrix</span><span class="p">))</span>
<span class="p">[[</span> <span class="mf">1.</span>  <span class="mf">0.</span>  <span class="mf">0.</span><span class="p">]</span>
 <span class="p">[</span> <span class="mf">0.</span>  <span class="mf">1.</span>  <span class="mf">0.</span><span class="p">]</span>
 <span class="p">[</span> <span class="mf">0.</span>  <span class="mf">0.</span>  <span class="mf">1.</span><span class="p">]]</span>
<span class="nb">print</span><span class="p">(</span><span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">A</span><span class="p">))</span>
<span class="p">[[</span> <span class="mf">0.96751703</span>  <span class="mf">0.11397751</span> <span class="o">-</span><span class="mf">0.3438891</span> <span class="p">]</span>
 <span class="p">[</span><span class="o">-</span><span class="mf">0.10132604</span> <span class="o">-</span><span class="mf">0.8432678</span>   <span class="mf">0.29810596</span><span class="p">]]</span>
<span class="nb">print</span><span class="p">(</span><span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">B</span><span class="p">))</span>
<span class="p">[[</span> <span class="mf">5.</span>  <span class="mf">5.</span>  <span class="mf">5.</span><span class="p">]</span>
 <span class="p">[</span> <span class="mf">5.</span>  <span class="mf">5.</span>  <span class="mf">5.</span><span class="p">]]</span>
<span class="nb">print</span><span class="p">(</span><span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">C</span><span class="p">))</span>
<span class="p">[[</span> <span class="mf">0.33184157</span>  <span class="mf">0.08907614</span><span class="p">]</span>
 <span class="p">[</span> <span class="mf">0.53189191</span>  <span class="mf">0.67605299</span><span class="p">]</span>
 <span class="p">[</span> <span class="mf">0.95889051</span>  <span class="mf">0.67061249</span><span class="p">]]</span>
<span class="nb">print</span><span class="p">(</span><span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">D</span><span class="p">))</span>
<span class="p">[[</span> <span class="mf">1.</span>  <span class="mf">2.</span>  <span class="mf">3.</span><span class="p">]</span>
 <span class="p">[</span><span class="o">-</span><span class="mf">3.</span> <span class="o">-</span><span class="mf">7.</span> <span class="o">-</span><span class="mf">1.</span><span class="p">]</span>
 <span class="p">[</span> <span class="mf">0.</span>  <span class="mf">5.</span> <span class="o">-</span><span class="mf">2.</span><span class="p">]]</span>
</pre></div>
</div>
<p>And for +,-,*, transposition, Determinant, Inverse operations:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">A</span><span class="o">+</span><span class="n">B</span><span class="p">))</span>
<span class="p">[[</span> <span class="mf">4.61596632</span>  <span class="mf">5.39771316</span>  <span class="mf">4.4325695</span> <span class="p">]</span>
 <span class="p">[</span> <span class="mf">3.26702736</span>  <span class="mf">5.14477345</span>  <span class="mf">4.98265553</span><span class="p">]]</span>
<span class="nb">print</span><span class="p">(</span><span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">B</span><span class="o">-</span><span class="n">B</span><span class="p">))</span>
<span class="p">[[</span> <span class="mf">0.</span>  <span class="mf">0.</span>  <span class="mf">0.</span><span class="p">]</span>
 <span class="p">[</span> <span class="mf">0.</span>  <span class="mf">0.</span>  <span class="mf">0.</span><span class="p">]]</span>
<span class="nb">print</span><span class="p">(</span><span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">identity_matrix</span><span class="p">)))</span>
<span class="p">[[</span> <span class="mf">5.</span>  <span class="mf">5.</span>  <span class="mf">5.</span><span class="p">]</span>
 <span class="p">[</span> <span class="mf">5.</span>  <span class="mf">5.</span>  <span class="mf">5.</span><span class="p">]]</span>
<span class="nb">print</span><span class="p">(</span><span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">C</span><span class="p">)))</span>
<span class="p">[[</span> <span class="mf">0.67124544</span>  <span class="mf">0.26766731</span>  <span class="mf">0.99068872</span><span class="p">]</span>
 <span class="p">[</span> <span class="mf">0.25006068</span>  <span class="mf">0.86560275</span>  <span class="mf">0.58411312</span><span class="p">]]</span>
<span class="nb">print</span><span class="p">(</span><span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">matrix_determinant</span><span class="p">(</span><span class="n">D</span><span class="p">)))</span>
<span class="o">-</span><span class="mf">38.0</span>
<span class="nb">print</span><span class="p">(</span><span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">matrix_inverse</span><span class="p">(</span><span class="n">D</span><span class="p">)))</span>
<span class="p">[[</span><span class="o">-</span><span class="mf">0.5</span>        <span class="o">-</span><span class="mf">0.5</span>        <span class="o">-</span><span class="mf">0.5</span>       <span class="p">]</span>
 <span class="p">[</span> <span class="mf">0.15789474</span>  <span class="mf">0.05263158</span>  <span class="mf">0.21052632</span><span class="p">]</span>
 <span class="p">[</span> <span class="mf">0.39473684</span>  <span class="mf">0.13157895</span>  <span class="mf">0.02631579</span><span class="p">]]</span>
</pre></div>
</div>
<p>Eigenvalues and Eigenvectors:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">self_adjoint_eig</span><span class="p">(</span><span class="n">D</span><span class="p">))</span>
<span class="p">[[</span><span class="o">-</span><span class="mf">10.65907521</span>  <span class="o">-</span><span class="mf">0.22750691</span>   <span class="mf">2.88658212</span><span class="p">]</span>
 <span class="p">[</span>  <span class="mf">0.21749542</span>   <span class="mf">0.63250104</span>  <span class="o">-</span><span class="mf">0.74339638</span><span class="p">]</span>
 <span class="p">[</span>  <span class="mf">0.84526515</span>   <span class="mf">0.2587998</span>    <span class="mf">0.46749277</span><span class="p">]</span>
 <span class="p">[</span> <span class="o">-</span><span class="mf">0.4880805</span>    <span class="mf">0.73004459</span>   <span class="mf">0.47834331</span><span class="p">]]</span>
<span class="p">(</span><span class="n">The</span> <span class="n">function</span> <span class="n">self_adjoint_eig</span><span class="p">()</span> <span class="n">outputs</span> <span class="n">the</span> <span class="n">eigenvalues</span> <span class="ow">in</span> <span class="n">the</span> <span class="n">first</span> <span class="n">row</span> <span class="ow">and</span> <span class="n">the</span> <span class="n">subsequent</span> <span class="n">vectors</span> <span class="ow">in</span> <span class="n">the</span> <span class="n">remaining</span> <span class="n">vectors</span><span class="o">.</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="regression">
<h3>Regression<a class="headerlink" href="#regression" title="Permalink to this headline">¶</a></h3>
<p>Here is a simple example of a regression exercise: let’s “learn” to a machine how to sum numbers! We give inputs and outputs, and it has to infer how to sum.</p>
<p>#Example taken from <a class="reference external" href="https://github.com/alesolano/mastering_tensorflow">https://github.com/alesolano/mastering_tensorflow</a></p>
<div class="figure align-default">
<a class="reference internal image-reference" href="../_images/Regression_exo11.png"><img alt="Regression exercise" src="../_images/Regression_exo11.png" style="width: 355.5px; height: 198.5px;" /></a>
</div>
<p>We will use a linear model, with a weight matrix and a bias vector.</p>
<div class="figure align-default">
<a class="reference internal image-reference" href="../_images/Regression_exo21.png"><img alt="Regression exercise" src="../_images/Regression_exo21.png" style="width: 390.0px; height: 194.0px;" /></a>
</div>
<p>We could use different layers. The first ones are hidden, the last one is the output.</p>
<div class="figure align-default">
<a class="reference internal image-reference" href="../_images/Regression_exo31.png"><img alt="Regression exercise" src="../_images/Regression_exo31.png" style="width: 293.0px; height: 200.0px;" /></a>
</div>
<p>We could even put non-linear functions in the hidden layers:</p>
<div class="figure align-default">
<a class="reference internal image-reference" href="../_images/Regression_exo41.png"><img alt="Regression exercise" src="../_images/Regression_exo41.png" style="width: 343.5px; height: 193.5px;" /></a>
</div>
<p>Here is the full code for this exercise: <code class="xref download docutils literal notranslate"><span class="pre">this</span> <span class="pre">example</span> <span class="pre">script</span></code>.</p>
</div>
<div class="section" id="classification">
<h3>Classification<a class="headerlink" href="#classification" title="Permalink to this headline">¶</a></h3>
<p>Exercise: let’s try to classify sums of 2 numbers, above 10 or not.</p>
<p>#Example taken from <a class="reference external" href="https://github.com/alesolano/mastering_tensorflow">https://github.com/alesolano/mastering_tensorflow</a></p>
<div class="figure align-default">
<a class="reference internal image-reference" href="../_images/Classification_exo11.png"><img alt="Classification exercise" src="../_images/Classification_exo11.png" style="width: 414.0px; height: 98.5px;" /></a>
</div>
<div class="figure align-default">
<a class="reference internal image-reference" href="../_images/Classification_exo21.png"><img alt="Classification exercise" src="../_images/Classification_exo21.png" style="width: 433.0px; height: 191.5px;" /></a>
</div>
<div class="figure align-default">
<a class="reference internal image-reference" href="../_images/Classification_exo31.png"><img alt="Classification exercise" src="../_images/Classification_exo31.png" style="width: 420.5px; height: 183.5px;" /></a>
</div>
<div class="figure align-default">
<a class="reference internal image-reference" href="../_images/Classification_exo41.png"><img alt="Classification exercise" src="../_images/Classification_exo41.png" style="width: 277.0px; height: 121.0px;" /></a>
</div>
</div>
</div>
</div>


           </div>
           
          </div>
          <footer>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2017, pmeu

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>