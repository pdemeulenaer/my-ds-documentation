

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Pyspark &mdash; Pyspark_Documentation 0.1 documentation</title>
  

  
  
  
  

  
  <script type="text/javascript" src="../_static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
        <script src="../_static/language_data.js"></script>
        <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link href="../_static/style.css" rel="stylesheet" type="text/css">

</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../index.html" class="icon icon-home"> Pyspark_Documentation
          

          
          </a>

          
            
            
              <div class="version">
                0.1
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <ul>
<li class="toctree-l1"><a class="reference internal" href="../Pandas_Documentation.html">Python, Jupyter and Pandas</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Sklearn_Documentation.html">Scikit-learn</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Pyspark_Documentation.html">Pyspark</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Text_Mining_Documentation.html">Text Mining in Python</a></li>
<li class="toctree-l1"><a class="reference internal" href="../DeepLearning_Documentation.html">Deep Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../ReinforcementLearning_Documentation.html">Reinforcement Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Time_Series_inPython_Documentation.html">Time Series in Python</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Algorithms_Documentation.html">Algorithms</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Statistics_probabilities_Documentation.html">Statistics and Probabilities</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Plot_libraries_Documentation.html">Great plot libraries</a></li>
<li class="toctree-l1"><a class="reference internal" href="../SQL_Documentation.html">SQL Server documentation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Bash_Documentation.html">Useful Bash commands (or batch)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Bash_Documentation.html#useful-git-commands">Useful GIT commands</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Bash_Documentation.html#useful-vim-commands">Useful VIM commands</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Bash_Documentation.html#data-types">Data types</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Business_models_Documentation.html">Interesting business models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Sphinx_stuff.html">Sphinx</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">Pyspark_Documentation</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html">Docs</a> &raquo;</li>
        
      <li>Pyspark</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="../_sources/docs/Pyspark_Documentation.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="pyspark">
<h1>Pyspark<a class="headerlink" href="#pyspark" title="Permalink to this headline">¶</a></h1>
<div class="section" id="basic-pyspark-documentation">
<h2>Basic Pyspark documentation<a class="headerlink" href="#basic-pyspark-documentation" title="Permalink to this headline">¶</a></h2>
<div class="topic">
<p class="topic-title">Introduction</p>
<p>The objective here is to have everything useful for the projects, not to make a complete documentation of the whole package. Here I will try to document both version 1.6 and &gt;2.0. A special emphase will be done on machine learning module ml (mllib is outdated).
We will not review the full Pyspark documentation. For that, look at <a class="reference external" href="http://spark.apache.org/docs/1.6.0/programming-guide.html">http://spark.apache.org/docs/1.6.0/programming-guide.html</a> for version 1.6, <a class="reference external" href="http://spark.apache.org/docs/2.1.0/programming-guide.html">http://spark.apache.org/docs/2.1.0/programming-guide.html</a> for version 2.1.</p>
</div>
<p>General Spark resources:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://sparkbyexamples.com/">https://sparkbyexamples.com/</a></p></li>
<li><p><a class="reference external" href="https://data-flair.training/blogs/dag-in-apache-spark/">https://data-flair.training/blogs/dag-in-apache-spark/</a> (internal of Spark)</p></li>
</ul>
<div class="section" id="spark-submit-tasks">
<h3>Spark-submit tasks<a class="headerlink" href="#spark-submit-tasks" title="Permalink to this headline">¶</a></h3>
<p>How to navigate between versions using spark-submit:</p>
<p>To check the spark version that currently uses spark-submit:</p>
<p>spark-submit –version</p>
<p>In bash, if we need spark-submit for spark 2.X, we can use:</p>
<p>export SPARK_MAJOR_VERSION=2</p>
<p>export SPARK_MAJOR_VERSION=2
export PYSPARK_PYTHON=/var/opt/teradata/anaconda2/bin/python</p>
<p># Launch:
spark-submit –name CashflowModel –executor-memory 60G –master yarn –driver-memory 2G –executor-cores 30 Pipeline_cashflow_standalone.py &amp;&gt;&gt; run_log.txt</p>
<p>See here for configuration parameters: <a class="reference external" href="https://spark.apache.org/docs/2.2.0/configuration.html">https://spark.apache.org/docs/2.2.0/configuration.html</a></p>
</div>
<div class="section" id="killing-yarn-applications">
<h3>Killing YARN applications:<a class="headerlink" href="#killing-yarn-applications" title="Permalink to this headline">¶</a></h3>
<p>yarn application -kill application_NNN</p>
</div>
<div class="section" id="importing-pyspark-modules">
<h3>Importing Pyspark modules<a class="headerlink" href="#importing-pyspark-modules" title="Permalink to this headline">¶</a></h3>
<p>There are many different ones. among the most commonly used:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">pyspark.sql.functions</span> <span class="kn">import</span> <span class="o">*</span>
</pre></div>
</div>
</div>
<div class="section" id="creation-of-pyspark-objects">
<h3>Creation of Pyspark objects<a class="headerlink" href="#creation-of-pyspark-objects" title="Permalink to this headline">¶</a></h3>
<p>First we need to create a Spark context (version 1.6):</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">pyspark</span> <span class="kn">import</span> <span class="n">SparkContext</span><span class="p">,</span> <span class="n">SparkConf</span>
<span class="n">appName</span> <span class="o">=</span> <span class="s2">&quot;Your App Name&quot;</span>
<span class="n">master</span> <span class="o">=</span> <span class="s2">&quot;local&quot;</span>
<span class="n">conf</span> <span class="o">=</span> <span class="n">SparkConf</span><span class="p">()</span><span class="o">.</span><span class="n">setAppName</span><span class="p">(</span><span class="n">appName</span><span class="p">)</span><span class="o">.</span><span class="n">setMaster</span><span class="p">(</span><span class="n">master</span><span class="p">)</span>
<span class="n">sc</span> <span class="o">=</span> <span class="n">SparkContext</span><span class="p">(</span><span class="n">conf</span><span class="o">=</span><span class="n">conf</span><span class="p">)</span>

<span class="c1">#or if you just don&#39;t care (by default, the master will be local)</span>
<span class="n">sc</span> <span class="o">=</span> <span class="n">SparkContext</span><span class="p">()</span>

<span class="c1">#For closing it (don&#39;t forget, if you want to create a new one later)</span>
<span class="n">sc</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
</pre></div>
</div>
<p>Using version 2.X, we can use SparkSession:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">pyspark</span> <span class="kn">import</span> <span class="n">SparkSession</span>
<span class="n">spark</span> <span class="o">=</span> <span class="n">SparkSession</span> \
  <span class="o">.</span><span class="n">builder</span> \
  <span class="o">.</span><span class="n">appName</span><span class="p">(</span><span class="s2">&quot;Protob Conversion to Parquet&quot;</span><span class="p">)</span> \
  <span class="o">.</span><span class="n">config</span><span class="p">(</span><span class="s2">&quot;spark.some.config.option&quot;</span><span class="p">,</span> <span class="s2">&quot;some-value&quot;</span><span class="p">)</span> \
  <span class="o">.</span><span class="n">getOrCreate</span><span class="p">()</span>
</pre></div>
</div>
<p>To change the spark configuration (for example to tune the numbers of workers available), we can define it</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1">#from pyspark import SparkSession,SQLContext #if not present in the notebook (in the &quot;pyspark3Jupyter&quot; command)</span>
<span class="n">spark</span><span class="o">.</span><span class="n">stop</span><span class="p">()</span> <span class="c1">#If some default spark running</span>
<span class="n">spark</span> <span class="o">=</span> <span class="n">SparkSession</span> \
  <span class="o">.</span><span class="n">builder</span> \
  <span class="o">.</span><span class="n">appName</span><span class="p">(</span><span class="s2">&quot;3839_spark&quot;</span><span class="p">)</span> \
  <span class="o">.</span><span class="n">config</span><span class="p">(</span><span class="s2">&quot;spark.executor.cores&quot;</span><span class="p">,</span> <span class="s2">&quot;3&quot;</span><span class="p">)</span> \
  <span class="o">.</span><span class="n">config</span><span class="p">(</span><span class="s2">&quot;spark.executor.memory&quot;</span><span class="p">,</span><span class="s2">&quot;15g&quot;</span><span class="p">)</span> \
  <span class="o">.</span><span class="n">config</span><span class="p">(</span><span class="s2">&quot;spark.dynamicAllocation.maxExecutors&quot;</span><span class="p">,</span><span class="s2">&quot;20&quot;</span><span class="p">)</span> \
  <span class="o">.</span><span class="n">config</span><span class="p">(</span><span class="s2">&quot;spark.dynamicAllocation.cachedExecutorIdleTimeout&quot;</span><span class="p">,</span><span class="s2">&quot;30m&quot;</span><span class="p">)</span> \
  <span class="o">.</span><span class="n">config</span><span class="p">(</span><span class="s2">&quot;spark.sql.parquet.writeLegacyFormat&quot;</span><span class="p">,</span><span class="s2">&quot;true&quot;</span><span class="p">)</span> \
  <span class="o">.</span><span class="n">enableHiveSupport</span><span class="p">()</span> \
  <span class="o">.</span><span class="n">getOrCreate</span><span class="p">()</span>
<span class="n">sqlCtx</span> <span class="o">=</span> <span class="n">SQLContext</span><span class="p">(</span><span class="n">sc</span><span class="p">)</span>
</pre></div>
</div>
<p>It can also be used that way:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">pyspark.conf</span> <span class="kn">import</span> <span class="n">SparkContext</span><span class="p">,</span> <span class="n">SparkConf</span>
<span class="kn">from</span> <span class="nn">pyspark.sql</span> <span class="kn">import</span> <span class="n">SparkSession</span>
<span class="n">conf</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">sparkContext</span><span class="o">.</span><span class="n">_conf</span><span class="o">.</span><span class="n">setAll</span><span class="p">([(</span><span class="s1">&#39;spark.executor.memory&#39;</span><span class="p">,</span> <span class="s1">&#39;4g&#39;</span><span class="p">),</span> <span class="p">(</span><span class="s1">&#39;spark.app.name&#39;</span><span class="p">,</span> <span class="s1">&#39;Spark Updated Conf&#39;</span><span class="p">),</span> <span class="p">(</span><span class="s1">&#39;spark.executor.cores&#39;</span><span class="p">,</span> <span class="s1">&#39;4&#39;</span><span class="p">),</span> <span class="p">(</span><span class="s1">&#39;spark.cores.max&#39;</span><span class="p">,</span> <span class="s1">&#39;4&#39;</span><span class="p">),</span> <span class="p">(</span><span class="s1">&#39;spark.driver.memory&#39;</span><span class="p">,</span><span class="s1">&#39;4g&#39;</span><span class="p">)])</span>
<span class="n">spark</span> <span class="o">=</span> <span class="n">SparkSession</span><span class="o">.</span><span class="n">builder</span><span class="o">.</span><span class="n">config</span><span class="p">(</span><span class="n">conf</span><span class="o">=</span><span class="n">conf</span><span class="p">)</span><span class="o">.</span><span class="n">getOrCreate</span><span class="p">()</span>
</pre></div>
</div>
<p>In order to check which are the configuration parameters of the notebook:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">spark</span><span class="o">.</span><span class="n">sparkContext</span><span class="o">.</span><span class="n">getConf</span><span class="p">()</span><span class="o">.</span><span class="n">getAll</span><span class="p">()</span>
<span class="c1">#or, when using sc:</span>
<span class="n">sc</span><span class="o">.</span><span class="n">_conf</span><span class="o">.</span><span class="n">getAll</span><span class="p">()</span>
</pre></div>
</div>
<p>If the Spark context is created to read SQL data (i.e. if we have sqlCtx), then we can simply use:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">sql</span> <span class="o">=</span> <span class="s2">&quot;&quot;&quot;</span>
<span class="s2">select * from risk_work.PBAFF_TestTrans</span>
<span class="s2">&quot;&quot;&quot;</span>
<span class="c1"># Create a cashed version of data</span>
<span class="n">Data</span> <span class="o">=</span> <span class="n">sqlCtx</span><span class="o">.</span><span class="n">sql</span><span class="p">(</span><span class="n">sql</span><span class="p">)</span>
<span class="n">Data</span> <span class="o">=</span> <span class="n">Data</span><span class="o">.</span><span class="n">cache</span><span class="p">()</span> <span class="c1">#this is to cache the object, makes it faster to reload/reuse it later</span>
</pre></div>
</div>
<p>Here a comparison of 2 ways of opening a table:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">spark</span> <span class="o">=</span> <span class="n">SparkSession</span><span class="p">(</span><span class="n">sc</span><span class="p">)</span>
<span class="n">sqlCtx</span> <span class="o">=</span> <span class="n">SQLContext</span><span class="p">(</span><span class="n">sc</span><span class="p">)</span>
<span class="n">table1</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">sql</span><span class="p">(</span><span class="s1">&#39;&#39;&#39;select * from 3839_project_pbaff.trx_201805_mcc_pcat_gir_tra&#39;&#39;&#39;</span><span class="p">)</span>
<span class="n">table2</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">table</span><span class="p">(</span><span class="s1">&#39;3839_project_pbaff.trx_201805_mcc_pcat_gir_tra&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="rdds">
<h3>RDDs<a class="headerlink" href="#rdds" title="Permalink to this headline">¶</a></h3>
<p>RDDs are the main data structure type in Pyspark until version 2.X. When possible, let’s work with the Dataframe approach rather than RDDs (they will become more and more deprecated, and are planed to disappear in 3.X)</p>
<p>sc.parallelize allows to convert python list to RDDs</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">rdd</span> <span class="o">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">parallelize</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">10</span><span class="p">))</span>
</pre></div>
</div>
<p>Here is the DataCamp Cheatsheet for RDDs:</p>
<div class="figure align-default" id="id2">
<a class="reference internal image-reference" href="docs/Cheatsheets/PySpark_Cheat_Sheet_Python.png"><img alt="map to buried treasure" src="docs/Cheatsheets/PySpark_Cheat_Sheet_Python.png" /></a>
<p class="caption"><span class="caption-text">This Cheatsheet is taken from DataCamp.</span><a class="headerlink" href="#id2" title="Permalink to this image">¶</a></p>
</div>
</div>
<div class="section" id="dataframes">
<h3>Dataframes<a class="headerlink" href="#dataframes" title="Permalink to this headline">¶</a></h3>
<p>Starting from Pyspark 1.5, Dataframes are built ontop of RDDs and allow to deal easier with data, in a more Pandas-like way. Since version 2.0, they become the main data type.</p>
<p>Here is the DataCamp Cheatsheet for RDDs:</p>
<div class="figure align-default" id="id3">
<a class="reference internal image-reference" href="../_images/PySpark_SQL_Cheat_Sheet_Python1.png"><img alt="map to buried treasure" src="../_images/PySpark_SQL_Cheat_Sheet_Python1.png" style="width: 1397.0px; height: 992.0px;" /></a>
<p class="caption"><span class="caption-text">This Cheatsheet is taken from DataCamp.</span><a class="headerlink" href="#id3" title="Permalink to this image">¶</a></p>
</div>
<p>From Pandas to Pyspark dataframe:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1">#Loading a Pandas dataframe:</span>
<span class="n">df_pd</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s2">&quot;/home/BC4350/Desktop/Iris.csv&quot;</span><span class="p">)</span>
<span class="c1">#Conversion to a Pyspark dataframe:</span>
<span class="n">df_sp</span> <span class="o">=</span> <span class="n">sqlContext</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">(</span><span class="n">df_pd</span><span class="p">)</span> <span class="c1">#or sc.createDataFrame(df_pd)</span>
<span class="c1">#If needs to go back to Pandas:</span>
<span class="n">df_pd</span> <span class="o">=</span> <span class="n">df_sp</span><span class="o">.</span><span class="n">toPandas</span><span class="p">()</span>
</pre></div>
</div>
<p>From RDD to dataframe:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">df</span> <span class="o">=</span> <span class="n">rdd</span><span class="o">.</span><span class="n">toDF</span><span class="p">()</span>
</pre></div>
</div>
<p>Creating a df from scatch: sometimes you have to specify the datatype:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">pyspark.sql.types</span> <span class="kn">import</span> <span class="n">FloatType</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">([</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">],</span> <span class="n">FloatType</span><span class="p">())</span>

<span class="n">df</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="o">+-----+</span>
<span class="o">|</span><span class="n">value</span><span class="o">|</span>
<span class="o">+-----+</span>
<span class="o">|</span>  <span class="mf">1.0</span><span class="o">|</span>
<span class="o">|</span>  <span class="mf">2.0</span><span class="o">|</span>
<span class="o">|</span>  <span class="mf">3.0</span><span class="o">|</span>
<span class="o">+-----+</span>
</pre></div>
</div>
</div>
<div class="section" id="partitions-in-pyspark">
<h3>Partitions in pyspark<a class="headerlink" href="#partitions-in-pyspark" title="Permalink to this headline">¶</a></h3>
<p>How many partitions should we have? Rule of thumb is to have 128Mb per partition.</p>
<p>The default number of partitions in Spark is 200. For big dataframes, this low number of partitions leads to high shuffle block size (i.e. when shuffling, high block size to be shuffled). 2 Things to keep in mind against this:</p>
<ul class="simple">
<li><p>increase the number of partitions (therefore reducing the number of partion size)</p></li>
<li><p>Get rid of skew in the data</p></li>
</ul>
<p>It is important not to have too big partitions, since the job might fail due to the 2Gb limit (no Spark shuffle block can be greater than 2Gb)</p>
<p>Rule of thumb: if number of partitions lower than 2000 but close to it, better to bump it above 2000, safer.</p>
<p>You can check the number of partitions:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">df</span><span class="o">.</span><span class="n">rdd</span><span class="o">.</span><span class="n">partitions</span><span class="o">.</span><span class="n">size</span>
<span class="c1">#or</span>
<span class="n">df</span><span class="o">.</span><span class="n">rdd</span><span class="o">.</span><span class="n">getNumPartitions</span><span class="p">()</span>

<span class="c1">#To change the number of partitions:</span>
<span class="n">df2</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">repartition</span><span class="p">(</span><span class="mi">15</span><span class="p">)</span>

<span class="c1">#re-check the number of partitions:</span>
<span class="n">df2</span><span class="o">.</span><span class="n">rdd</span><span class="o">.</span><span class="n">partitions</span><span class="o">.</span><span class="n">size</span>
<span class="c1">#or</span>
<span class="n">df2</span><span class="o">.</span><span class="n">rdd</span><span class="o">.</span><span class="n">getNumPartitions</span><span class="p">()</span>
</pre></div>
</div>
<p>Beware of data shuffle when repartitioning as this is expensive. Take a look at coalesce if needed. “coalesce” just decreases the size of partitions (while “partition” allows to increase), but “coalesce” does not shuffle the data.</p>
<p>Note: it is possible to change the default number of partitions: <a class="reference external" href="https://stackoverflow.com/questions/46510881/how-to-set-spark-sql-shuffle-partitions-when-using-the-lastest-spark-version">https://stackoverflow.com/questions/46510881/how-to-set-spark-sql-shuffle-partitions-when-using-the-lastest-spark-version</a> : spark.conf.set(“spark.sql.shuffle.partitions”, 1000)</p>
<p>It is also possible to partition dataframe when loading them: <a class="reference external" href="https://deepsense.ai/optimize-spark-with-distribute-by-and-cluster-by/">https://deepsense.ai/optimize-spark-with-distribute-by-and-cluster-by/</a></p>
<p>A nice function to read the number of partitions as well as the size of each partitions:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">check_partition</span><span class="p">(</span><span class="n">df</span><span class="p">):</span>

  <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Num partition: </span><span class="si">{0}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">rdd</span><span class="o">.</span><span class="n">getNumPartitions</span><span class="p">()))</span>

  <span class="k">def</span> <span class="nf">count_partition</span><span class="p">(</span><span class="n">index</span><span class="p">,</span> <span class="n">iterator</span><span class="p">):</span>
      <span class="k">yield</span> <span class="p">(</span><span class="n">index</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">iterator</span><span class="p">)))</span>

  <span class="n">data</span> <span class="o">=</span> <span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">rdd</span><span class="o">.</span><span class="n">mapPartitionsWithIndex</span><span class="p">(</span><span class="n">count_partition</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">collect</span><span class="p">())</span>

  <span class="k">for</span> <span class="n">index</span><span class="p">,</span> <span class="n">count</span> <span class="ow">in</span> <span class="n">data</span><span class="p">:</span>
      <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;partition </span><span class="si">{0:2d}</span><span class="s2">: </span><span class="si">{1}</span><span class="s2"> bytes&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">index</span><span class="p">,</span> <span class="n">count</span><span class="p">))</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">table</span><span class="p">(</span><span class="s1">&#39;database.table&#39;</span><span class="p">)</span>
<span class="n">check_partition</span><span class="p">(</span><span class="n">df</span><span class="p">)</span>
</pre></div>
</div>
<p>Example of output:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">Num</span> <span class="n">partition</span><span class="p">:</span> <span class="mi">29</span>
<span class="n">partition</span>  <span class="mi">0</span><span class="p">:</span> <span class="mi">93780</span> <span class="nb">bytes</span>
<span class="n">partition</span>  <span class="mi">1</span><span class="p">:</span> <span class="mi">93363</span> <span class="nb">bytes</span>
<span class="n">partition</span>  <span class="mi">2</span><span class="p">:</span> <span class="mi">93153</span> <span class="nb">bytes</span>
</pre></div>
</div>
</div>
<div class="section" id="concerning-partition-skewness-problem">
<h3>Concerning partition skewness problem<a class="headerlink" href="#concerning-partition-skewness-problem" title="Permalink to this headline">¶</a></h3>
<p>Great link on avoiding data skewness: <a class="reference external" href="https://medium.com/simpl-under-the-hood/spark-protip-joining-on-skewed-dataframes-7bfa610be704">https://medium.com/simpl-under-the-hood/spark-protip-joining-on-skewed-dataframes-7bfa610be704</a></p>
<p>Very good presentations takling skewness:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://www.youtube.com/watch?v=6zg7NTw-kTQ&amp;list=PLuitsavBRqtNM0XACsWSAHRzwdLIaHmq-&amp;index=4&amp;t=1391s">https://www.youtube.com/watch?v=6zg7NTw-kTQ&amp;list=PLuitsavBRqtNM0XACsWSAHRzwdLIaHmq-&amp;index=4&amp;t=1391s</a></p></li>
<li><p><a class="reference external" href="https://www.youtube.com/watch?v=daXEp4HmS-E&amp;list=PLuitsavBRqtNM0XACsWSAHRzwdLIaHmq-&amp;index=6&amp;t=912s">https://www.youtube.com/watch?v=daXEp4HmS-E&amp;list=PLuitsavBRqtNM0XACsWSAHRzwdLIaHmq-&amp;index=6&amp;t=912s</a></p></li>
</ul>
<p>Ideally we would like to have partitions like this:</p>
<div class="figure align-default">
<a class="reference internal image-reference" href="docs/Images/Distribution_system_good.png"><img alt="Memory management in yarn and spark" src="docs/Images/Distribution_system_good.png" /></a>
</div>
<p>But sometimes things like this can happen:</p>
<div class="figure align-default">
<a class="reference internal image-reference" href="docs/Images/Distribution_system_bad.png"><img alt="Memory management in yarn and spark" src="docs/Images/Distribution_system_bad.png" /></a>
</div>
<p>Let’s say we have 2 tables skewed:</p>
<div class="figure align-default">
<a class="reference internal image-reference" href="../_images/skewed_tables1.png"><img alt="skewed tables" src="../_images/skewed_tables1.png" style="width: 283.2px; height: 162.0px;" /></a>
</div>
<p>If we want to do a join,</p>
<div class="figure align-default">
<a class="reference internal image-reference" href="../_images/skewed_tables_joining1.png"><img alt="skewed tables being joined" src="../_images/skewed_tables_joining1.png" style="width: 391.8px; height: 239.39999999999998px;" /></a>
</div>
<p>Solution using a broadcast join:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">pyspark.sql.functions</span> <span class="kn">import</span> <span class="n">broadcast</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">broadcast</span><span class="p">(</span><span class="n">A</span><span class="p">)</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">B</span><span class="p">,[</span><span class="s2">&quot;join_col&quot;</span><span class="p">],</span><span class="s2">&quot;left&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>Solution using a SALT key (applied for a groupby operation, but would be similar for a join):</p>
<div class="figure align-default">
<a class="reference internal image-reference" href="docs/Images/Data_skew_solution_using_salt.png"><img alt="Data_skew_solution_using_salt" src="docs/Images/Data_skew_solution_using_salt.png" /></a>
</div>
</div>
<div class="section" id="spark-executor-cores-and-memory-management-resources-allocation-in-spark">
<h3>Spark executor/cores and memory management: Resources allocation in Spark<a class="headerlink" href="#spark-executor-cores-and-memory-management-resources-allocation-in-spark" title="Permalink to this headline">¶</a></h3>
<p><a class="reference external" href="https://spoddutur.github.io/spark-notes/distribution_of_executors_cores_and_memory_for_spark_application.html">https://spoddutur.github.io/spark-notes/distribution_of_executors_cores_and_memory_for_spark_application.html</a></p>
<p>Here a good intro:</p>
<p><a class="reference external" href="https://blog.cloudera.com/blog/2015/03/how-to-tune-your-apache-spark-jobs-part-2/">https://blog.cloudera.com/blog/2015/03/how-to-tune-your-apache-spark-jobs-part-2/</a></p>
<p>The two main resources that Spark (and YARN) think about are CPU and memory. Disk and network I/O, of course, play a part in Spark performance as well, but neither Spark nor YARN currently do anything to actively manage them.</p>
<p>Every Spark executor in an application has the same fixed number of cores and same fixed heap size. The number of cores can be specified with the –executor-cores flag when invoking spark-submit, spark-shell, and pyspark from the command line, or by setting the spark.executor.cores property in the spark-defaults.conf file or on a SparkConf object. Similarly, the heap size can be controlled with the –executor-memory flag or the spark.executor.memory property. The cores property controls the number of concurrent tasks an executor can run. –executor-cores 5 means that each executor can run a maximum of five tasks at the same time. The memory property impacts the amount of data Spark can cache, as well as the maximum sizes of the shuffle data structures used for grouping, aggregations, and joins.</p>
<p>The –num-executors command-line flag or spark.executor.instances configuration property control the number of executors requested. Starting in CDH 5.4/Spark 1.3, you will be able to avoid setting this property by turning on dynamic allocation with the spark.dynamicAllocation.enabled property. Dynamic allocation enables a Spark application to request executors when there is a backlog of pending tasks and free up executors when idle.</p>
<p>It’s also important to think about how the resources requested by Spark will fit into what YARN has available. The relevant YARN properties are:</p>
<ul class="simple">
<li><p>yarn.nodemanager.resource.memory-mb controls the maximum sum of memory used by the containers on each node.</p></li>
<li><p>yarn.nodemanager.resource.cpu-vcores controls the maximum sum of cores used by the containers on each node.</p></li>
</ul>
<p>Asking for five executor cores will result in a request to YARN for five virtual cores. The memory requested from YARN is a little more complex for a couple reasons:</p>
<ul class="simple">
<li><p>–executor-memory/spark.executor.memory controls the executor heap size, but JVMs can also use some memory off heap, for example for interned Strings and direct byte buffers. The value of the spark.yarn.executor.memoryOverhead property is added to the executor memory to determine the full memory request to YARN for each executor. It defaults to max(384, .07 * spark.executor.memory).</p></li>
<li><p>YARN may round the requested memory up a little. YARN’s yarn.scheduler.minimum-allocation-mb and yarn.scheduler.increment-allocation-mb properties control the minimum and increment request values respectively.</p></li>
</ul>
<p>The following (not to scale with defaults) shows the hierarchy of memory properties in Spark and YARN:</p>
<div class="figure align-default">
<a class="reference internal image-reference" href="docs/Images/Memory_spark_yarn.png"><img alt="Memory management in yarn and spark" src="docs/Images/Memory_spark_yarn.png" /></a>
</div>
<p>And if that weren’t enough to think about, a few final concerns when sizing Spark executors:</p>
<ul class="simple">
<li><p>The application master (AM), which is a non-executor container with the special capability of requesting containers from YARN, takes up resources of its own that must be budgeted in. In yarn-client mode, it defaults to a 1024MB and one vcore. In yarn-cluster mode, the application master runs the driver, so it’s often useful to bolster its resources with the –driver-memory and –driver-cores properties.</p></li>
<li><p>Running executors with too much memory often results in excessive garbage collection delays. 64GB is a rough guess at a good upper limit for a single executor.</p></li>
<li><p>the HDFS client has trouble with tons of concurrent threads. A rough guess is that at most five tasks per executor can achieve full write throughput, so it’s good to keep the number of cores per executor below that number.</p></li>
<li><p>Running tiny executors (with a single core and just enough memory needed to run a single task, for example) throws away the benefits that come from running multiple tasks in a single JVM. For example, broadcast variables need to be replicated once on each executor, so many small executors will result in many more copies of the data.</p></li>
</ul>
<p>EXAMPLE: Let’s say we have a cluster with the following physical specifications:</p>
<ul class="simple">
<li><p>6 physical nodes</p></li>
<li><p>each node has 16 cores</p></li>
<li><p>each node has 64Gb of memory</p></li>
</ul>
<p>What are the spark parameters, to use as much resources as possible from the cluster?</p>
<p>Note that each of the node runs NodeManagers. The NodeManager capacities, yarn.nodemanager.resource.memory-mb and yarn.nodemanager.resource.cpu-vcores, should probably be set to 63 * 1024 = 64512 (megabytes) and 15 cores respectively. We avoid allocating 100% of the resources to YARN containers because the node needs some resources to run the OS and Hadoop daemons.</p>
<p>The likely first impulse would be to use –num-executors 6 –executor-cores 15 –executor-memory 63G. However, this is the wrong approach because:</p>
<ul class="simple">
<li><p>63GB + the executor memory overhead won’t fit within the 63GB capacity of the NodeManagers.</p></li>
<li><p>The application master will take up a core on one of the nodes, meaning that there won’t be room for a 15-core executor on that node.</p></li>
<li><p>15 cores per executor can lead to bad HDFS I/O throughput.</p></li>
</ul>
<p>A better option would be to use –num-executors 17 –executor-cores 5 –executor-memory 19G. Why?</p>
<ul class="simple">
<li><p>This config results in three executors on all nodes except for the one with the AM, which will have two executors.</p></li>
<li><p>–executor-memory was derived as (63/3 executors per node) = 21. The memory overhead should take 7% (or in more recent cases 10%) of the allocated memory: 21 * 0.07 = 1.47 Gb. So the total memory allocated should be no large than 21 – 1.47 ~ 19.</p></li>
</ul>
</div>
<div class="section" id="basic-commands">
<h3>Basic commands<a class="headerlink" href="#basic-commands" title="Permalink to this headline">¶</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1">#Counting how many rows in dataframe:</span>
<span class="n">df</span><span class="o">.</span><span class="n">count</span><span class="p">()</span>

<span class="c1">#Displaying first 20 rows:</span>
<span class="n">df</span><span class="o">.</span><span class="n">show</span><span class="p">(</span><span class="mi">20</span><span class="p">)</span>

<span class="c1">#Count how many distinct values for a column:</span>
<span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="s2">&quot;column&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">distinct</span><span class="p">()</span><span class="o">.</span><span class="n">count</span><span class="p">()</span>

<span class="c1">#Count how many Null in a column:</span>
<span class="n">df</span><span class="o">.</span><span class="n">filter</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">columName</span><span class="o">.</span><span class="n">isNull</span><span class="p">())</span><span class="o">.</span><span class="n">count</span><span class="p">()</span>

<span class="c1">#Convert the type of a column to float. In fact you can add a new column, columnFloat:</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">withColumn</span><span class="p">(</span><span class="s2">&quot;columnFloat&quot;</span><span class="p">,</span> <span class="n">df</span><span class="p">[</span><span class="s2">&quot;column&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="s2">&quot;float&quot;</span><span class="p">))</span>
<span class="c1">#Or simply replace the old column by the new one:</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">withColumn</span><span class="p">(</span><span class="s2">&quot;column&quot;</span><span class="p">,</span> <span class="n">df</span><span class="p">[</span><span class="s2">&quot;column&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="s2">&quot;float&quot;</span><span class="p">))</span>

<span class="c1">#Sorting:</span>
<span class="n">df</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="s2">&quot;KNID&quot;</span><span class="p">,</span><span class="s2">&quot;sum(BLPS)&quot;</span><span class="p">,</span><span class="n">ascending</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span><span class="o">.</span><span class="n">show</span><span class="p">(</span><span class="mi">100</span><span class="p">)</span>

<span class="c1">#Moving to a Pandas dataframe:</span>
<span class="n">df_pd</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">toPandas</span><span class="p">()</span>

<span class="c1">#Add a new column (dayofmonth) to a dataframe:</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">withColumn</span><span class="p">(</span><span class="s1">&#39;dayofmonth&#39;</span><span class="p">,</span><span class="n">df</span><span class="o">.</span><span class="n">bgdt</span><span class="p">[</span><span class="mi">7</span><span class="p">:</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">DoubleType</span><span class="p">())</span><span class="o">/</span><span class="mf">31.</span><span class="p">)</span>

<span class="c1">#Add a new column with a constant value:</span>
<span class="kn">from</span> <span class="nn">pyspark.sql.functions</span> <span class="kn">import</span> <span class="n">lit</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">withColumn</span><span class="p">(</span><span class="s1">&#39;NewColumn&#39;</span><span class="p">,</span> <span class="n">lit</span><span class="p">(</span><span class="n">constant</span><span class="p">))</span>

<span class="c1">#Changing the type of a column:</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">withColumn</span><span class="p">(</span><span class="s2">&quot;pjkd&quot;</span><span class="p">,</span> <span class="n">df</span><span class="p">[</span><span class="s2">&quot;pjkd&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="s2">&quot;int&quot;</span><span class="p">))</span>

<span class="c1">#Renaming a column:</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">withColumnRenamed</span><span class="p">(</span><span class="s1">&#39;value&#39;</span><span class="p">,</span> <span class="s1">&#39;value2&#39;</span><span class="p">)</span>

<span class="c1">#Trimming of whitespace in strings</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">withColumn</span><span class="p">(</span><span class="s1">&#39;columnName&#39;</span><span class="p">,</span> <span class="n">trim</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">columnName</span><span class="p">))</span>

<span class="c1">#Filtering (with where clause):</span>
<span class="n">df_filtered</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="s1">&#39;init&#39;</span><span class="p">,</span><span class="s1">&#39;initFeatures&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;init&#39;</span><span class="p">]</span><span class="o">==</span><span class="s1">&#39;0006&#39;</span><span class="p">)</span>

<span class="c1">#Modify only SOME values of a column: we can use a when clause for that:</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">withColumn</span><span class="p">(</span><span class="s1">&#39;column&#39;</span><span class="p">,</span> <span class="n">when</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;otherColumn&#39;</span><span class="p">]</span><span class="o">==</span><span class="n">something</span><span class="p">,</span> <span class="n">constant</span><span class="p">)</span><span class="o">.</span><span class="n">otherwise</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;column&#39;</span><span class="p">]))</span>

<span class="c1">#Vertical concatenation of 2 dataframes</span>
<span class="n">df_result</span> <span class="o">=</span> <span class="n">df_1</span><span class="o">.</span><span class="n">unionAll</span><span class="p">(</span><span class="n">df_2</span><span class="p">)</span>

<span class="c1">#Find common columns in 2 different dataframes:</span>
<span class="nb">list</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="n">df1</span><span class="o">.</span><span class="n">columns</span><span class="p">)</span><span class="o">.</span><span class="n">intersection</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="n">df2</span><span class="o">.</span><span class="n">columns</span><span class="p">)))</span>

<span class="c1">#Add a column of monotonically increasing ID:</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">withColumn</span><span class="p">(</span><span class="s2">&quot;id&quot;</span><span class="p">,</span> <span class="n">monotonically_increasing_id</span><span class="p">())</span>

<span class="c1">#Add a column made of a uniform random number</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">withColumn</span><span class="p">(</span><span class="s1">&#39;random_number&#39;</span><span class="p">,</span> <span class="n">rand</span><span class="p">()</span> <span class="p">)</span>
</pre></div>
</div>
<p>Type definition for several variables at once (“recasting”):</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># recast variable</span>
<span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="n">c</span><span class="p">],</span><span class="n">df</span><span class="p">[</span><span class="n">c</span><span class="p">]</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="s1">&#39;int&#39;</span><span class="p">))</span>
<span class="n">dtype_dict</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;Player&#39;</span> <span class="p">:</span> <span class="n">StringType</span><span class="p">,</span> <span class="s1">&#39;Pos&#39;</span> <span class="p">:</span> <span class="n">StringType</span><span class="p">,</span> <span class="s1">&#39;Tm&#39;</span> <span class="p">:</span> <span class="n">StringType</span><span class="p">,</span> <span class="s1">&#39;Age&#39;</span> <span class="p">:</span> <span class="n">IntegerType</span><span class="p">,</span> <span class="s1">&#39;G&#39;</span> <span class="p">:</span> <span class="n">IntegerType</span><span class="p">,</span> <span class="s1">&#39;GS&#39;</span> <span class="p">:</span> <span class="n">IntegerType</span><span class="p">,</span> <span class="s1">&#39;yr&#39;</span> <span class="p">:</span> <span class="n">IntegerType</span><span class="p">}</span>

<span class="n">df2</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">fillna</span><span class="p">(</span><span class="s1">&#39;0&#39;</span><span class="p">)</span>

<span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="n">df2</span><span class="o">.</span><span class="n">schema</span><span class="o">.</span><span class="n">names</span><span class="p">[</span><span class="mi">6</span><span class="p">:]:</span>
  <span class="n">dtype</span> <span class="o">=</span> <span class="n">DoubleType</span> <span class="k">if</span> <span class="n">c</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">dtype_dict</span><span class="o">.</span><span class="n">keys</span><span class="p">()</span> <span class="k">else</span> <span class="n">dtype_dict</span><span class="p">[</span><span class="n">c</span><span class="p">]</span>
<span class="n">df2</span> <span class="o">=</span> <span class="n">df2</span><span class="o">.</span><span class="n">withColumn</span><span class="p">(</span><span class="n">c</span><span class="p">,</span><span class="n">df2</span><span class="p">[</span><span class="n">c</span><span class="p">]</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">dtype</span><span class="p">()))</span>
</pre></div>
</div>
<p>Dropping duplicate rows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">df</span> <span class="o">=</span> <span class="n">sqlContext</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">([(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)],</span> <span class="p">[</span><span class="s2">&quot;A&quot;</span><span class="p">,</span> <span class="s2">&quot;B&quot;</span><span class="p">,</span> <span class="s2">&quot;C&quot;</span><span class="p">])</span>

<span class="n">df</span><span class="o">.</span><span class="n">dropDuplicates</span><span class="p">()</span> <span class="c1"># drops all rows which have all same columns</span>

<span class="n">df</span><span class="o">.</span><span class="n">DropColumns</span><span class="p">([</span><span class="s1">&#39;A&#39;</span><span class="p">,</span><span class="s1">&#39;B&#39;</span><span class="p">])</span> <span class="c1"># drops all rows which have all same elements in A and B</span>
</pre></div>
</div>
</div>
<div class="section" id="random-sampling">
<h3>Random sampling<a class="headerlink" href="#random-sampling" title="Permalink to this headline">¶</a></h3>
<p>The trick is to sort using a random number and the take the N first rows.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">df_sampled</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">orderBy</span><span class="p">(</span><span class="n">rand</span><span class="p">())</span><span class="o">.</span><span class="n">limit</span><span class="p">(</span><span class="mi">5000</span><span class="p">)</span>
</pre></div>
</div>
<p>In Hive the equivalent is</p>
<p>select * from my_table order by rand() limit 10000;</p>
<p>BUT! If your input table has an distribution key, the order by rand might not work as expected, in that case you need to use something like this:</p>
<p>select * from my_table distribute by rand() sort by rand() limit 10000;</p>
<p>To do this in Spark, we could use a temp table, like this. Let’s say we have a dataframe containing many time series, one for each customer (millions of customers). And we want a sample of 100K customers and their time series.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">customer_list</span> <span class="o">=</span> <span class="n">time_series</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="s1">&#39;primaryaccountholder&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">distinct</span><span class="p">()</span>

<span class="n">customer_list</span><span class="o">.</span><span class="n">createOrReplaceTempView</span><span class="p">(</span><span class="s1">&#39;customer_list_temp&#39;</span><span class="p">)</span>

<span class="n">customer_list_sample</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">sql</span><span class="p">(</span><span class="s1">&#39;select * from customer_list_temp distribute by rand() sort by rand() limit 100000&#39;</span><span class="p">)</span>

<span class="n">customer_list_sample</span><span class="o">.</span><span class="n">count</span><span class="p">()</span>
</pre></div>
</div>
<p>By this we extracted the list of 100K customers. Then we can extract the associated data (time series) selecting for only these customers (using a join).</p>
</div>
<div class="section" id="aggregating-in-pyspark">
<h3>Aggregating in Pyspark<a class="headerlink" href="#aggregating-in-pyspark" title="Permalink to this headline">¶</a></h3>
<p>The main aggregation functions:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">approxCountDistinct</span><span class="p">,</span> <span class="n">avg</span><span class="p">,</span> <span class="n">count</span><span class="p">,</span> <span class="n">countDistinct</span><span class="p">,</span> <span class="n">first</span><span class="p">,</span> <span class="n">last</span><span class="p">,</span> <span class="nb">max</span><span class="p">,</span> <span class="n">mean</span><span class="p">,</span> <span class="nb">min</span><span class="p">,</span> <span class="nb">sum</span><span class="p">,</span> <span class="n">sumDistinct</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1">#Grouping and aggregating:</span>
<span class="n">df</span><span class="o">.</span><span class="n">groupBy</span><span class="p">(</span><span class="s2">&quot;KNID&quot;</span><span class="p">,</span><span class="s2">&quot;IDKT&quot;</span><span class="p">,</span><span class="s2">&quot;counter_account&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">agg</span><span class="p">({</span><span class="s2">&quot;BLPS&quot;</span><span class="p">:</span> <span class="s2">&quot;sum&quot;</span><span class="p">,</span> <span class="s2">&quot;KNID&quot;</span><span class="p">:</span> <span class="s2">&quot;count&quot;</span><span class="p">})</span>
<span class="c1">#Other example with aggregation on distinct knid:</span>
<span class="n">df</span><span class="o">.</span><span class="n">groupBy</span><span class="p">(</span><span class="s1">&#39;txft&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">agg</span><span class="p">(</span><span class="n">countDistinct</span><span class="p">(</span><span class="s1">&#39;knid&#39;</span><span class="p">))</span><span class="o">.</span><span class="n">orderBy</span><span class="p">(</span><span class="s1">&#39;count(knid)&#39;</span><span class="p">,</span><span class="n">ascending</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">show</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span><span class="kc">False</span><span class="p">)</span>

<span class="c1">#Here for one column only:</span>
<span class="n">part_party</span><span class="o">.</span><span class="n">groupBy</span><span class="p">(</span><span class="s1">&#39;bankid&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">count</span><span class="p">()</span><span class="o">.</span><span class="n">orderBy</span><span class="p">(</span><span class="s1">&#39;count&#39;</span><span class="p">,</span><span class="n">ascending</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">show</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span><span class="kc">False</span><span class="p">)</span>

<span class="c1">#Example: we have a given dataframe like</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">sqlContext</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">([(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">8</span><span class="p">),</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">6</span><span class="p">),</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">)],</span> <span class="p">[</span><span class="s2">&quot;A&quot;</span><span class="p">,</span> <span class="s2">&quot;B&quot;</span><span class="p">])</span>
<span class="n">df</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="o">+---+---+</span>
<span class="o">|</span>  <span class="n">A</span><span class="o">|</span>  <span class="n">B</span><span class="o">|</span>
<span class="o">+---+---+</span>
<span class="o">|</span>  <span class="mi">1</span><span class="o">|</span>  <span class="mi">4</span><span class="o">|</span>
<span class="o">|</span>  <span class="mi">2</span><span class="o">|</span>  <span class="mi">5</span><span class="o">|</span>
<span class="o">|</span>  <span class="mi">2</span><span class="o">|</span>  <span class="mi">8</span><span class="o">|</span>
<span class="o">|</span>  <span class="mi">3</span><span class="o">|</span>  <span class="mi">6</span><span class="o">|</span>
<span class="o">|</span>  <span class="mi">3</span><span class="o">|</span>  <span class="mi">2</span><span class="o">|</span>
<span class="o">+---+---+</span>

<span class="c1">#Then we can build the aggregates for each values of A using:</span>
<span class="kn">from</span> <span class="nn">pyspark.sql</span> <span class="kn">import</span> <span class="n">functions</span> <span class="k">as</span> <span class="n">F</span>
<span class="n">df</span><span class="o">.</span><span class="n">groupBy</span><span class="p">(</span><span class="s2">&quot;A&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">agg</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">avg</span><span class="p">(</span><span class="s2">&quot;B&quot;</span><span class="p">),</span> <span class="n">F</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="s2">&quot;B&quot;</span><span class="p">),</span> <span class="n">F</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="s2">&quot;B&quot;</span><span class="p">))</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="o">+---+------+------+------+</span>
<span class="o">|</span>  <span class="n">A</span><span class="o">|</span><span class="n">avg</span><span class="p">(</span><span class="n">B</span><span class="p">)</span><span class="o">|</span><span class="nb">min</span><span class="p">(</span><span class="n">B</span><span class="p">)</span><span class="o">|</span><span class="nb">max</span><span class="p">(</span><span class="n">B</span><span class="p">)</span><span class="o">|</span>
<span class="o">+---+------+------+------+</span>
<span class="o">|</span>  <span class="mi">1</span><span class="o">|</span>   <span class="mf">4.0</span><span class="o">|</span>     <span class="mi">4</span><span class="o">|</span>     <span class="mi">4</span><span class="o">|</span>
<span class="o">|</span>  <span class="mi">3</span><span class="o">|</span>   <span class="mf">4.0</span><span class="o">|</span>     <span class="mi">2</span><span class="o">|</span>     <span class="mi">6</span><span class="o">|</span>
<span class="o">|</span>  <span class="mi">2</span><span class="o">|</span>   <span class="mf">6.5</span><span class="o">|</span>     <span class="mi">5</span><span class="o">|</span>     <span class="mi">8</span><span class="o">|</span>
<span class="o">+---+------+------+------+</span>

<span class="c1">#We can also build aggregates using aliases:</span>

<span class="n">df</span><span class="o">.</span><span class="n">groupBy</span><span class="p">(</span><span class="s2">&quot;A&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">agg</span><span class="p">(</span>
  <span class="n">F</span><span class="o">.</span><span class="n">first</span><span class="p">(</span><span class="s2">&quot;B&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s2">&quot;my first&quot;</span><span class="p">),</span>
  <span class="n">F</span><span class="o">.</span><span class="n">last</span><span class="p">(</span><span class="s2">&quot;B&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s2">&quot;my last&quot;</span><span class="p">),</span>
  <span class="n">F</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="s2">&quot;B&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s2">&quot;my everything&quot;</span><span class="p">)</span>
<span class="p">)</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="o">+---+--------+-------+-------------+</span>
<span class="o">|</span>  <span class="n">A</span><span class="o">|</span><span class="n">my</span> <span class="n">first</span><span class="o">|</span><span class="n">my</span> <span class="n">last</span><span class="o">|</span><span class="n">my</span> <span class="n">everything</span><span class="o">|</span>
<span class="o">+---+--------+-------+-------------+</span>
<span class="o">|</span>  <span class="mi">1</span><span class="o">|</span>       <span class="mi">4</span><span class="o">|</span>      <span class="mi">4</span><span class="o">|</span>            <span class="mi">4</span><span class="o">|</span>
<span class="o">|</span>  <span class="mi">3</span><span class="o">|</span>       <span class="mi">6</span><span class="o">|</span>      <span class="mi">2</span><span class="o">|</span>            <span class="mi">8</span><span class="o">|</span>
<span class="o">|</span>  <span class="mi">2</span><span class="o">|</span>       <span class="mi">8</span><span class="o">|</span>      <span class="mi">5</span><span class="o">|</span>           <span class="mi">13</span><span class="o">|</span>
<span class="o">+---+--------+-------+-------------+</span>
</pre></div>
</div>
<p>Group data and give how many counts per group (similar to .value_counts() in pandas):</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">df</span><span class="o">.</span><span class="n">groupBy</span><span class="p">(</span><span class="s1">&#39;colum&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">count</span><span class="p">()</span><span class="o">.</span><span class="n">orderBy</span><span class="p">(</span><span class="s1">&#39;count&#39;</span><span class="p">,</span><span class="n">ascending</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">show</span><span class="p">()</span> <span class="c1"># will show biggest groups first</span>

<span class="o">+---+------+</span>
<span class="o">|</span>  <span class="n">A</span><span class="o">|</span> <span class="n">count</span><span class="o">|</span>
<span class="o">+---+------+</span>
<span class="o">|</span> <span class="n">AB</span><span class="o">|</span>   <span class="mi">250</span><span class="o">|</span>
<span class="o">|</span> <span class="n">CD</span><span class="o">|</span>    <span class="mi">32</span><span class="o">|</span>
<span class="o">|</span><span class="n">EFG</span><span class="o">|</span>     <span class="mi">8</span><span class="o">|</span>
<span class="o">+---+------+</span>
</pre></div>
</div>
<p>Group by data and count (distinct) number of elements for one column:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># simple count</span>
<span class="n">df</span><span class="o">.</span><span class="n">groupBy</span><span class="p">(</span><span class="s1">&#39;columnToGroupOn&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">agg</span><span class="p">(</span><span class="n">count</span><span class="p">(</span><span class="s1">&#39;columnToCount&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s1">&#39;count&#39;</span><span class="p">))</span><span class="o">.</span><span class="n">orderBy</span><span class="p">(</span><span class="s1">&#39;count&#39;</span><span class="p">,</span><span class="n">ascending</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="c1"># distinct count</span>
<span class="n">df</span><span class="o">.</span><span class="n">groupBy</span><span class="p">(</span><span class="s1">&#39;columnToGroupOn&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">agg</span><span class="p">(</span><span class="n">countDistinct</span><span class="p">(</span><span class="s1">&#39;columnToCount&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s1">&#39;count&#39;</span><span class="p">))</span><span class="o">.</span><span class="n">orderBy</span><span class="p">(</span><span class="s1">&#39;count&#39;</span><span class="p">,</span><span class="n">ascending</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="section" id="joins">
<h3>Joins<a class="headerlink" href="#joins" title="Permalink to this headline">¶</a></h3>
<p><a class="reference external" href="https://docs.databricks.com/spark/latest/faq/join-two-dataframes-duplicated-column.html">https://docs.databricks.com/spark/latest/faq/join-two-dataframes-duplicated-column.html</a></p>
<p>Here is a simple example of inner join where we keep all left columns and SOME of the right columns:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">pyspark.sql.functions</span> <span class="kn">import</span> <span class="o">*</span>

<span class="n">df1</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s1">&#39;df1&#39;</span><span class="p">)</span>
<span class="n">df2</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s1">&#39;df2&#39;</span><span class="p">)</span>
<span class="n">df1</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">df2</span><span class="p">,</span> <span class="n">df1</span><span class="o">.</span><span class="n">id</span> <span class="o">==</span> <span class="n">df2</span><span class="o">.</span><span class="n">id</span><span class="p">)</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="s1">&#39;df1.*&#39;</span><span class="p">)</span>

<span class="c1">#we can also select everything but one column from right:</span>
<span class="n">df1</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s1">&#39;df1&#39;</span><span class="p">)</span>
<span class="n">df2</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s1">&#39;df2&#39;</span><span class="p">)</span>
<span class="n">df1</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">df2</span><span class="p">,</span> <span class="n">df1</span><span class="o">.</span><span class="n">id</span> <span class="o">==</span> <span class="n">df2</span><span class="o">.</span><span class="n">id</span><span class="p">)</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">df2</span><span class="o">.</span><span class="n">bankid</span><span class="p">)</span>
</pre></div>
</div>
<p>Join on multiple conditions:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">join</span> <span class="o">=</span> <span class="n">txn</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">external</span><span class="p">,</span> <span class="n">on</span><span class="o">=</span><span class="p">[</span><span class="n">txn</span><span class="o">.</span><span class="n">colA</span> <span class="o">==</span> <span class="n">external</span><span class="o">.</span><span class="n">colC</span><span class="p">,</span> <span class="n">txn</span><span class="o">.</span><span class="n">colB</span> <span class="o">==</span> <span class="n">external</span><span class="o">.</span><span class="n">colD</span><span class="p">],</span> <span class="n">how</span><span class="o">=</span><span class="s1">&#39;left&#39;</span><span class="p">)</span>

<span class="c1">#or simply:</span>

<span class="n">join</span> <span class="o">=</span> <span class="n">txn</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">external</span><span class="p">,</span> <span class="p">[</span><span class="n">txn</span><span class="o">.</span><span class="n">colA</span> <span class="o">==</span> <span class="n">external</span><span class="o">.</span><span class="n">colC</span><span class="p">,</span> <span class="n">txn</span><span class="o">.</span><span class="n">colB</span> <span class="o">==</span> <span class="n">external</span><span class="o">.</span><span class="n">colD</span><span class="p">],</span> <span class="s1">&#39;left&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="window-functions">
<h3>Window functions<a class="headerlink" href="#window-functions" title="Permalink to this headline">¶</a></h3>
<p>The main window functions:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">cumeDist</span><span class="p">,</span> <span class="n">denseRank</span><span class="p">,</span> <span class="n">lag</span><span class="p">,</span> <span class="n">lead</span><span class="p">,</span> <span class="n">ntile</span><span class="p">,</span> <span class="n">percentRank</span><span class="p">,</span> <span class="n">rank</span><span class="p">,</span> <span class="n">rowNumber</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1">#Let&#39;s say we have the same dataframe as in the aggregation section:</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">sqlContext</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">([(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">8</span><span class="p">),</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">6</span><span class="p">),</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">)],</span> <span class="p">[</span><span class="s2">&quot;A&quot;</span><span class="p">,</span> <span class="s2">&quot;B&quot;</span><span class="p">])</span>

<span class="c1">#We can build a window function that computes a diff line by line – ordered or not – given a specific key</span>

<span class="kn">from</span> <span class="nn">pyspark.sql.window</span> <span class="kn">import</span> <span class="n">Window</span>
<span class="n">window_over_A</span> <span class="o">=</span> <span class="n">Window</span><span class="o">.</span><span class="n">partitionBy</span><span class="p">(</span><span class="s2">&quot;A&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">orderBy</span><span class="p">(</span><span class="s2">&quot;B&quot;</span><span class="p">)</span>
<span class="n">df</span><span class="o">.</span><span class="n">withColumn</span><span class="p">(</span><span class="s2">&quot;diff&quot;</span><span class="p">,</span> <span class="n">F</span><span class="o">.</span><span class="n">lead</span><span class="p">(</span><span class="s2">&quot;B&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">over</span><span class="p">(</span><span class="n">window_over_A</span><span class="p">)</span> <span class="o">-</span> <span class="n">df</span><span class="o">.</span><span class="n">B</span><span class="p">)</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="o">+---+---+----+</span>
<span class="o">|</span>  <span class="n">A</span><span class="o">|</span>  <span class="n">B</span><span class="o">|</span><span class="n">diff</span><span class="o">|</span>
<span class="o">+---+---+----+</span>
<span class="o">|</span>  <span class="mi">1</span><span class="o">|</span>  <span class="mi">4</span><span class="o">|</span><span class="n">null</span><span class="o">|</span>
<span class="o">|</span>  <span class="mi">3</span><span class="o">|</span>  <span class="mi">2</span><span class="o">|</span>   <span class="mi">4</span><span class="o">|</span>
<span class="o">|</span>  <span class="mi">3</span><span class="o">|</span>  <span class="mi">6</span><span class="o">|</span><span class="n">null</span><span class="o">|</span>
<span class="o">|</span>  <span class="mi">2</span><span class="o">|</span>  <span class="mi">5</span><span class="o">|</span>   <span class="mi">3</span><span class="o">|</span>
<span class="o">|</span>  <span class="mi">2</span><span class="o">|</span>  <span class="mi">8</span><span class="o">|</span><span class="n">null</span><span class="o">|</span>
<span class="o">+---+---+----+</span>
</pre></div>
</div>
<p>Example: a ranking on a window, and selection of first rank:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">window</span> <span class="o">=</span> <span class="n">Window</span><span class="o">.</span><span class="n">partitionBy</span><span class="p">(</span><span class="s1">&#39;mtts&#39;</span><span class="p">,</span><span class="s1">&#39;pcatkey1&#39;</span><span class="p">,</span><span class="s1">&#39;pcatkey2&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">orderBy</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">desc</span><span class="p">(</span><span class="s1">&#39;pcat_opts&#39;</span><span class="p">))</span>
<span class="n">trx_2018</span><span class="o">=</span><span class="n">trx_2018</span><span class="o">.</span><span class="n">withColumn</span><span class="p">(</span><span class="s2">&quot;rank_pcatids&quot;</span><span class="p">,</span> <span class="n">F</span><span class="o">.</span><span class="n">rank</span><span class="p">()</span><span class="o">.</span><span class="n">over</span><span class="p">(</span><span class="n">window</span><span class="p">))</span>
<span class="n">trx_2018</span><span class="o">=</span><span class="n">trx_2018</span><span class="o">.</span><span class="n">filter</span><span class="p">(</span><span class="s2">&quot;rank_pcatids==1&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>Another thing: when we want to count the number of rows PER GROUP:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1">#some data:</span>
<span class="n">data</span> <span class="o">=</span> <span class="p">[</span>
  <span class="p">(</span><span class="s1">&#39;a&#39;</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span>
  <span class="p">(</span><span class="s1">&#39;a&#39;</span><span class="p">,</span> <span class="mi">8</span><span class="p">),</span>
  <span class="p">(</span><span class="s1">&#39;a&#39;</span><span class="p">,</span> <span class="mi">7</span><span class="p">),</span>
  <span class="p">(</span><span class="s1">&#39;b&#39;</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
<span class="p">]</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">sqlCtx</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="p">[</span><span class="s2">&quot;x&quot;</span><span class="p">,</span> <span class="s2">&quot;y&quot;</span><span class="p">])</span>
<span class="n">df</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="o">+---+---+</span>
<span class="o">|</span>  <span class="n">x</span><span class="o">|</span>  <span class="n">y</span><span class="o">|</span>
<span class="o">+---+---+</span>
<span class="o">|</span>  <span class="n">a</span><span class="o">|</span>  <span class="mi">5</span><span class="o">|</span>
<span class="o">|</span>  <span class="n">a</span><span class="o">|</span>  <span class="mi">8</span><span class="o">|</span>
<span class="o">|</span>  <span class="n">a</span><span class="o">|</span>  <span class="mi">7</span><span class="o">|</span>
<span class="o">|</span>  <span class="n">b</span><span class="o">|</span>  <span class="mi">1</span><span class="o">|</span>
<span class="o">+---+---+</span>

<span class="n">w</span> <span class="o">=</span> <span class="n">Window</span><span class="o">.</span><span class="n">partitionBy</span><span class="p">(</span><span class="s1">&#39;x&#39;</span><span class="p">)</span>
<span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="s1">&#39;y&#39;</span><span class="p">,</span> <span class="n">count</span><span class="p">(</span><span class="s1">&#39;x&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">over</span><span class="p">(</span><span class="n">w</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s1">&#39;count&#39;</span><span class="p">))</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="s1">&#39;y&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="o">+---+---+-----+</span>
<span class="o">|</span>  <span class="n">x</span><span class="o">|</span>  <span class="n">y</span><span class="o">|</span><span class="n">count</span><span class="o">|</span>
<span class="o">+---+---+-----+</span>
<span class="o">|</span>  <span class="n">a</span><span class="o">|</span>  <span class="mi">5</span><span class="o">|</span>    <span class="mi">3</span><span class="o">|</span>
<span class="o">|</span>  <span class="n">a</span><span class="o">|</span>  <span class="mi">7</span><span class="o">|</span>    <span class="mi">3</span><span class="o">|</span>
<span class="o">|</span>  <span class="n">a</span><span class="o">|</span>  <span class="mi">8</span><span class="o">|</span>    <span class="mi">3</span><span class="o">|</span>
<span class="o">|</span>  <span class="n">b</span><span class="o">|</span>  <span class="mi">1</span><span class="o">|</span>    <span class="mi">1</span><span class="o">|</span>
<span class="o">+---+---+-----+</span>

<span class="c1">#We can get exactly the same using pure SQL:</span>
<span class="n">df</span><span class="o">.</span><span class="n">registerTempTable</span><span class="p">(</span><span class="s1">&#39;table&#39;</span><span class="p">)</span>
<span class="n">sqlCtx</span><span class="o">.</span><span class="n">sql</span><span class="p">(</span>
  <span class="s1">&#39;SELECT x, y, COUNT(x) OVER (PARTITION BY x) AS n FROM table ORDER BY x, y&#39;</span>
<span class="p">)</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="c1">#Slightly different: we want to have the new column included into the dataframe. Very similar.</span>
<span class="c1">#the advantage of this is when there are many columns, not practical with select.</span>
<span class="n">w</span> <span class="o">=</span> <span class="n">Window</span><span class="o">.</span><span class="n">partitionBy</span><span class="p">(</span><span class="s1">&#39;x&#39;</span><span class="p">)</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">withColumn</span><span class="p">(</span><span class="s2">&quot;count&quot;</span><span class="p">,</span> <span class="n">count</span><span class="p">(</span><span class="s1">&#39;x&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">over</span><span class="p">(</span><span class="n">w</span><span class="p">))</span> <span class="c1">#not necessary to sort in fact</span>
<span class="c1">#if we want to sort:</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">withColumn</span><span class="p">(</span><span class="s2">&quot;count&quot;</span><span class="p">,</span> <span class="n">count</span><span class="p">(</span><span class="s1">&#39;x&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">over</span><span class="p">(</span><span class="n">w</span><span class="p">))</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="s1">&#39;y&#39;</span><span class="p">)</span>
</pre></div>
</div>
<p>See also a comparison of cumulative sum made on groups in pandas and in pyspark (see the pandas section).</p>
</div>
<div class="section" id="generate-a-column-with-dates-between-2-dates">
<h3>Generate a column with dates between 2 dates<a class="headerlink" href="#generate-a-column-with-dates-between-2-dates" title="Permalink to this headline">¶</a></h3>
<p>I could not find a native way, so I generated it from Pandas and converted to Spark:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Create a Pandas dataframe with the column &quot;time&quot; containing the dates between start_date and end_date</span>
<span class="n">time</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">date_range</span><span class="p">(</span><span class="n">start_date</span><span class="p">,</span> <span class="n">end_date</span><span class="p">,</span> <span class="n">freq</span><span class="o">=</span><span class="s1">&#39;D&#39;</span><span class="p">)</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;time&#39;</span><span class="p">])</span>
<span class="n">df</span><span class="p">[</span><span class="s1">&#39;time&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">time</span>
<span class="n">df</span><span class="p">[</span><span class="s1">&#39;time&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">to_datetime</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;time&#39;</span><span class="p">])</span>

<span class="c1"># Converting to Pyspark</span>
<span class="n">df_sp</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">(</span><span class="n">dff</span><span class="p">)</span>
<span class="n">df_sp</span> <span class="o">=</span> <span class="n">df_sp</span><span class="o">.</span><span class="n">withColumn</span><span class="p">(</span><span class="s1">&#39;transactiondate&#39;</span><span class="p">,</span><span class="n">psf</span><span class="o">.</span><span class="n">to_date</span><span class="p">(</span><span class="n">df_sp</span><span class="o">.</span><span class="n">time</span><span class="p">))</span>
<span class="n">df_sp</span><span class="o">.</span><span class="n">show</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span>

<span class="o">+-------------------+---------------+</span>
<span class="o">|</span>               <span class="n">time</span><span class="o">|</span><span class="n">transactiondate</span><span class="o">|</span>
<span class="o">+-------------------+---------------+</span>
<span class="o">|</span><span class="mi">2017</span><span class="o">-</span><span class="mi">01</span><span class="o">-</span><span class="mi">01</span> <span class="mi">00</span><span class="p">:</span><span class="mi">00</span><span class="p">:</span><span class="mi">00</span><span class="o">|</span>     <span class="mi">2017</span><span class="o">-</span><span class="mi">01</span><span class="o">-</span><span class="mi">01</span><span class="o">|</span>
<span class="o">|</span><span class="mi">2017</span><span class="o">-</span><span class="mi">01</span><span class="o">-</span><span class="mi">02</span> <span class="mi">00</span><span class="p">:</span><span class="mi">00</span><span class="p">:</span><span class="mi">00</span><span class="o">|</span>     <span class="mi">2017</span><span class="o">-</span><span class="mi">01</span><span class="o">-</span><span class="mi">02</span><span class="o">|</span>
<span class="o">|</span><span class="mi">2017</span><span class="o">-</span><span class="mi">01</span><span class="o">-</span><span class="mi">03</span> <span class="mi">00</span><span class="p">:</span><span class="mi">00</span><span class="p">:</span><span class="mi">00</span><span class="o">|</span>     <span class="mi">2017</span><span class="o">-</span><span class="mi">01</span><span class="o">-</span><span class="mi">03</span><span class="o">|</span>
<span class="o">|</span><span class="mi">2017</span><span class="o">-</span><span class="mi">01</span><span class="o">-</span><span class="mi">04</span> <span class="mi">00</span><span class="p">:</span><span class="mi">00</span><span class="p">:</span><span class="mi">00</span><span class="o">|</span>     <span class="mi">2017</span><span class="o">-</span><span class="mi">01</span><span class="o">-</span><span class="mi">04</span><span class="o">|</span>
<span class="o">|</span><span class="mi">2017</span><span class="o">-</span><span class="mi">01</span><span class="o">-</span><span class="mi">05</span> <span class="mi">00</span><span class="p">:</span><span class="mi">00</span><span class="p">:</span><span class="mi">00</span><span class="o">|</span>     <span class="mi">2017</span><span class="o">-</span><span class="mi">01</span><span class="o">-</span><span class="mi">05</span><span class="o">|</span>
<span class="o">+-------------------+---------------+</span>
</pre></div>
</div>
</div>
<div class="section" id="generate-an-array-of-dates-between-2-dates">
<h3>Generate an array of dates between 2 dates<a class="headerlink" href="#generate-an-array-of-dates-between-2-dates" title="Permalink to this headline">¶</a></h3>
<p>Inspired by some of the answers here: <a class="reference external" href="https://stackoverflow.com/questions/43141671/sparksql-on-pyspark-how-to-generate-time-series">https://stackoverflow.com/questions/43141671/sparksql-on-pyspark-how-to-generate-time-series</a></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">pyspark.sql.functions</span> <span class="kn">import</span> <span class="n">sequence</span><span class="p">,</span> <span class="n">to_date</span><span class="p">,</span> <span class="n">explode</span><span class="p">,</span> <span class="n">col</span>
<span class="n">spark</span><span class="o">.</span><span class="n">sql</span><span class="p">(</span><span class="s2">&quot;SELECT sequence(to_date(&#39;2018-01-01&#39;), to_date(&#39;2018-03-01&#39;), interval 1 month) as date&quot;</span><span class="p">)</span>

<span class="o">+------------------------------------------+</span>
<span class="o">|</span>                  <span class="n">date</span>                    <span class="o">|</span>
<span class="o">+------------------------------------------+</span>
<span class="o">|</span> <span class="p">[</span><span class="s2">&quot;2018-01-01&quot;</span><span class="p">,</span><span class="s2">&quot;2018-02-01&quot;</span><span class="p">,</span><span class="s2">&quot;2018-03-01&quot;</span><span class="p">]</span> <span class="o">|</span>
<span class="o">+------------------------------------------+</span>

<span class="c1">#or in case of start_date, end_date already defined:</span>
<span class="n">spark</span><span class="o">.</span><span class="n">sql</span><span class="p">(</span><span class="s2">&quot;SELECT sequence(to_date(&#39;</span><span class="si">{0}</span><span class="s2">&#39;), to_date(&#39;</span><span class="si">{1}</span><span class="s2">&#39;), interval 1 month) as transactiondate&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">start_date</span><span class="p">,</span> <span class="n">end_date</span><span class="p">))</span>
</pre></div>
</div>
<p>You can use the explode function to “pivot” this array into rows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">spark</span><span class="o">.</span><span class="n">sql</span><span class="p">(</span><span class="s2">&quot;SELECT sequence(to_date(&#39;2018-01-01&#39;), to_date(&#39;2018-03-01&#39;), interval 1 month) as date&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">withColumn</span><span class="p">(</span><span class="s2">&quot;date&quot;</span><span class="p">,</span> <span class="n">explode</span><span class="p">(</span><span class="n">col</span><span class="p">(</span><span class="s2">&quot;date&quot;</span><span class="p">))</span>

<span class="o">+------------+</span>
<span class="o">|</span>    <span class="n">date</span>    <span class="o">|</span>
<span class="o">+------------+</span>
<span class="o">|</span> <span class="mi">2018</span><span class="o">-</span><span class="mi">01</span><span class="o">-</span><span class="mi">01</span> <span class="o">|</span>
<span class="o">|</span> <span class="mi">2018</span><span class="o">-</span><span class="mi">02</span><span class="o">-</span><span class="mi">01</span> <span class="o">|</span>
<span class="o">|</span> <span class="mi">2018</span><span class="o">-</span><span class="mi">03</span><span class="o">-</span><span class="mi">01</span> <span class="o">|</span>
<span class="o">+------------+</span>
</pre></div>
</div>
</div>
<div class="section" id="fill-forward-or-backward-in-spark">
<h3>Fill forward or backward in spark<a class="headerlink" href="#fill-forward-or-backward-in-spark" title="Permalink to this headline">¶</a></h3>
<p>Taken from <a class="reference external" href="https://johnpaton.net/posts/forward-fill-spark/">https://johnpaton.net/posts/forward-fill-spark/</a> . This is also based on window functions.</p>
<p>Forward fill: filling null values with the last known non-null value, leaving only leading nulls unchanged.</p>
<p>Note: in Pandas this is easy. We just do a groupby without aggregation, and to each group apply the .fillna method, specifying specifying method=’ffill’, also known as method=’pad’:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">df_filled</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">groupby</span><span class="p">(</span><span class="s1">&#39;location&#39;</span><span class="p">)</span>\
            <span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">group</span><span class="p">:</span> <span class="n">group</span><span class="o">.</span><span class="n">fillna</span><span class="p">(</span><span class="n">method</span><span class="o">=</span><span class="s1">&#39;ffill&#39;</span><span class="p">))</span>
</pre></div>
</div>
<p>In Pyspark we need a window function as well as the ‘last’ function of pyspark.sql. ‘last’ returns the last value in the window (implying that the window must have a meaningful ordering).  It takes an optional argument ignorenulls which, when set to True, causes last to return the last non-null value in the window, if such a value exists.</p>
<p>The strategy to forward fill in Spark is as follows. First we define a window, which is ordered in time, and which includes all the rows from the beginning of time up until the current row. We achieve this here simply by selecting the rows in the window as being the rowsBetween -sys.maxint (the largest negative value possible), and 0 (the current row). Specifying too large of a value for the rows doesn’t cause any errors, so we can just use a very large number to be sure our window reaches until the very beginning of the dataframe. If you need to optimize memory usage, you can make your job much more efficient by finding the maximal number of consecutive nulls in your dataframe and only taking a large enough window to include all of those plus one non-null value.</p>
<p>We act with last over the window we have defined, specifying ignorenulls=True. If the current row is non-null, then the output will just be the value of current row. However, if the current row is null, then the function will return the most recent (last) non-null value in the window.</p>
<p>Let’s say we have some array:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">values</span> <span class="o">=</span> <span class="p">[</span>
  <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="s2">&quot;2015-12-01&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">),</span>
  <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="s2">&quot;2015-12-02&quot;</span><span class="p">,</span> <span class="mi">25</span><span class="p">),</span>
  <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="s2">&quot;2015-12-03&quot;</span><span class="p">,</span> <span class="mi">30</span><span class="p">),</span>
  <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="s2">&quot;2015-12-04&quot;</span><span class="p">,</span> <span class="mi">55</span><span class="p">),</span>
  <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="s2">&quot;2015-12-05&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">),</span>
  <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="s2">&quot;2015-12-06&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">),</span>
  <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="s2">&quot;2015-12-07&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">),</span>
  <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="s2">&quot;2015-12-08&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">),</span>
  <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="s2">&quot;2015-12-09&quot;</span><span class="p">,</span> <span class="mi">49</span><span class="p">),</span>
  <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="s2">&quot;2015-12-10&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">),</span>
<span class="p">]</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">(</span><span class="n">values</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;customer&#39;</span><span class="p">,</span> <span class="s1">&#39;date&#39;</span><span class="p">,</span> <span class="s1">&#39;value&#39;</span><span class="p">])</span>
<span class="n">df</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="o">+--------+----------+-----+</span>
<span class="o">|</span><span class="n">customer</span><span class="o">|</span>      <span class="n">date</span><span class="o">|</span><span class="n">value</span><span class="o">|</span>
<span class="o">+--------+----------+-----+</span>
<span class="o">|</span>       <span class="mi">1</span><span class="o">|</span><span class="mi">2015</span><span class="o">-</span><span class="mi">12</span><span class="o">-</span><span class="mi">01</span><span class="o">|</span> <span class="n">null</span><span class="o">|</span>
<span class="o">|</span>       <span class="mi">1</span><span class="o">|</span><span class="mi">2015</span><span class="o">-</span><span class="mi">12</span><span class="o">-</span><span class="mi">02</span><span class="o">|</span>   <span class="mi">25</span><span class="o">|</span>
<span class="o">|</span>       <span class="mi">1</span><span class="o">|</span><span class="mi">2015</span><span class="o">-</span><span class="mi">12</span><span class="o">-</span><span class="mi">03</span><span class="o">|</span>   <span class="mi">30</span><span class="o">|</span>
<span class="o">|</span>       <span class="mi">1</span><span class="o">|</span><span class="mi">2015</span><span class="o">-</span><span class="mi">12</span><span class="o">-</span><span class="mi">04</span><span class="o">|</span>   <span class="mi">55</span><span class="o">|</span>
<span class="o">|</span>       <span class="mi">1</span><span class="o">|</span><span class="mi">2015</span><span class="o">-</span><span class="mi">12</span><span class="o">-</span><span class="mi">05</span><span class="o">|</span> <span class="n">null</span><span class="o">|</span>
<span class="o">|</span>       <span class="mi">1</span><span class="o">|</span><span class="mi">2015</span><span class="o">-</span><span class="mi">12</span><span class="o">-</span><span class="mi">06</span><span class="o">|</span> <span class="n">null</span><span class="o">|</span>
<span class="o">|</span>       <span class="mi">2</span><span class="o">|</span><span class="mi">2015</span><span class="o">-</span><span class="mi">12</span><span class="o">-</span><span class="mi">07</span><span class="o">|</span> <span class="n">null</span><span class="o">|</span>
<span class="o">|</span>       <span class="mi">2</span><span class="o">|</span><span class="mi">2015</span><span class="o">-</span><span class="mi">12</span><span class="o">-</span><span class="mi">08</span><span class="o">|</span> <span class="n">null</span><span class="o">|</span>
<span class="o">|</span>       <span class="mi">2</span><span class="o">|</span><span class="mi">2015</span><span class="o">-</span><span class="mi">12</span><span class="o">-</span><span class="mi">09</span><span class="o">|</span>   <span class="mi">49</span><span class="o">|</span>
<span class="o">|</span>       <span class="mi">2</span><span class="o">|</span><span class="mi">2015</span><span class="o">-</span><span class="mi">12</span><span class="o">-</span><span class="mi">10</span><span class="o">|</span> <span class="n">null</span><span class="o">|</span>
<span class="o">+--------+----------+-----+</span>

<span class="kn">from</span> <span class="nn">pyspark.sql</span> <span class="kn">import</span> <span class="n">Window</span>
<span class="kn">from</span> <span class="nn">pyspark.sql.functions</span> <span class="kn">import</span> <span class="n">last</span>
<span class="kn">import</span> <span class="nn">sys</span>

<span class="n">window</span> <span class="o">=</span> <span class="n">Window</span><span class="o">.</span><span class="n">partitionBy</span><span class="p">(</span><span class="s1">&#39;customer&#39;</span><span class="p">)</span>\
             <span class="o">.</span><span class="n">orderBy</span><span class="p">(</span><span class="s1">&#39;date&#39;</span><span class="p">)</span>\
             <span class="o">.</span><span class="n">rowsBetween</span><span class="p">(</span><span class="o">-</span><span class="n">sys</span><span class="o">.</span><span class="n">maxsize</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>

<span class="n">spark_df_filled</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">withColumn</span><span class="p">(</span><span class="s1">&#39;value_ffill&#39;</span><span class="p">,</span> <span class="n">last</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;value&#39;</span><span class="p">],</span> <span class="n">ignorenulls</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">over</span><span class="p">(</span><span class="n">window</span><span class="p">)</span> <span class="p">)</span>
<span class="n">spark_df_filled</span> <span class="o">=</span> <span class="n">spark_df_filled</span><span class="o">.</span><span class="n">orderBy</span><span class="p">(</span><span class="s1">&#39;customer&#39;</span><span class="p">,</span><span class="s1">&#39;date&#39;</span><span class="p">)</span>
<span class="n">spark_df_filled</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="o">+--------+----------+-----+-----------+</span>
<span class="o">|</span><span class="n">customer</span><span class="o">|</span>      <span class="n">date</span><span class="o">|</span><span class="n">value</span><span class="o">|</span><span class="n">value_ffill</span><span class="o">|</span>
<span class="o">+--------+----------+-----+-----------+</span>
<span class="o">|</span>       <span class="mi">1</span><span class="o">|</span><span class="mi">2015</span><span class="o">-</span><span class="mi">12</span><span class="o">-</span><span class="mi">01</span><span class="o">|</span> <span class="n">null</span><span class="o">|</span>       <span class="n">null</span><span class="o">|</span>
<span class="o">|</span>       <span class="mi">1</span><span class="o">|</span><span class="mi">2015</span><span class="o">-</span><span class="mi">12</span><span class="o">-</span><span class="mi">02</span><span class="o">|</span>   <span class="mi">25</span><span class="o">|</span>         <span class="mi">25</span><span class="o">|</span>
<span class="o">|</span>       <span class="mi">1</span><span class="o">|</span><span class="mi">2015</span><span class="o">-</span><span class="mi">12</span><span class="o">-</span><span class="mi">03</span><span class="o">|</span>   <span class="mi">30</span><span class="o">|</span>         <span class="mi">30</span><span class="o">|</span>
<span class="o">|</span>       <span class="mi">1</span><span class="o">|</span><span class="mi">2015</span><span class="o">-</span><span class="mi">12</span><span class="o">-</span><span class="mi">04</span><span class="o">|</span>   <span class="mi">55</span><span class="o">|</span>         <span class="mi">55</span><span class="o">|</span>
<span class="o">|</span>       <span class="mi">1</span><span class="o">|</span><span class="mi">2015</span><span class="o">-</span><span class="mi">12</span><span class="o">-</span><span class="mi">05</span><span class="o">|</span> <span class="n">null</span><span class="o">|</span>         <span class="mi">55</span><span class="o">|</span>
<span class="o">|</span>       <span class="mi">1</span><span class="o">|</span><span class="mi">2015</span><span class="o">-</span><span class="mi">12</span><span class="o">-</span><span class="mi">06</span><span class="o">|</span> <span class="n">null</span><span class="o">|</span>         <span class="mi">55</span><span class="o">|</span>
<span class="o">|</span>       <span class="mi">2</span><span class="o">|</span><span class="mi">2015</span><span class="o">-</span><span class="mi">12</span><span class="o">-</span><span class="mi">07</span><span class="o">|</span> <span class="n">null</span><span class="o">|</span>       <span class="n">null</span><span class="o">|</span>
<span class="o">|</span>       <span class="mi">2</span><span class="o">|</span><span class="mi">2015</span><span class="o">-</span><span class="mi">12</span><span class="o">-</span><span class="mi">08</span><span class="o">|</span> <span class="n">null</span><span class="o">|</span>       <span class="n">null</span><span class="o">|</span>
<span class="o">|</span>       <span class="mi">2</span><span class="o">|</span><span class="mi">2015</span><span class="o">-</span><span class="mi">12</span><span class="o">-</span><span class="mi">09</span><span class="o">|</span>   <span class="mi">49</span><span class="o">|</span>         <span class="mi">49</span><span class="o">|</span>
<span class="o">|</span>       <span class="mi">2</span><span class="o">|</span><span class="mi">2015</span><span class="o">-</span><span class="mi">12</span><span class="o">-</span><span class="mi">10</span><span class="o">|</span> <span class="n">null</span><span class="o">|</span>         <span class="mi">49</span><span class="o">|</span>
<span class="o">+--------+----------+-----+-----------+</span>
</pre></div>
</div>
</div>
<div class="section" id="arrays-create-time-series-format-from-row-time-series-arraytype-format">
<h3>Arrays: Create time series format from row time series (ArrayType format)<a class="headerlink" href="#arrays-create-time-series-format-from-row-time-series-arraytype-format" title="Permalink to this headline">¶</a></h3>
<p>List of general array operations in Spark: <a class="reference external" href="https://mungingdata.com/apache-spark/arraytype-columns/">https://mungingdata.com/apache-spark/arraytype-columns/</a></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1">#https://stackoverflow.com/questions/38080748/convert-pyspark-string-to-date-format</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">sqlContext</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">([(</span><span class="s2">&quot;1991-11-15&quot;</span><span class="p">,</span><span class="s1">&#39;a&#39;</span><span class="p">,</span><span class="mi">23</span><span class="p">),</span>
                               <span class="p">(</span><span class="s2">&quot;1991-11-16&quot;</span><span class="p">,</span><span class="s1">&#39;a&#39;</span><span class="p">,</span><span class="mi">24</span><span class="p">),</span>
                               <span class="p">(</span><span class="s2">&quot;1991-11-17&quot;</span><span class="p">,</span><span class="s1">&#39;a&#39;</span><span class="p">,</span><span class="mi">32</span><span class="p">),</span>
                               <span class="p">(</span><span class="s2">&quot;1991-11-25&quot;</span><span class="p">,</span><span class="s1">&#39;b&#39;</span><span class="p">,</span><span class="mi">13</span><span class="p">),</span>
                               <span class="p">(</span><span class="s2">&quot;1991-11-26&quot;</span><span class="p">,</span><span class="s1">&#39;b&#39;</span><span class="p">,</span><span class="mi">14</span><span class="p">)],</span> <span class="n">schema</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;date&#39;</span><span class="p">,</span> <span class="s1">&#39;customer&#39;</span><span class="p">,</span> <span class="s1">&#39;balance_day&#39;</span><span class="p">])</span>

<span class="n">df</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="o">+----------+--------+-----------+</span>
<span class="o">|</span>      <span class="n">date</span><span class="o">|</span><span class="n">customer</span><span class="o">|</span><span class="n">balance_day</span><span class="o">|</span>
<span class="o">+----------+--------+-----------+</span>
<span class="o">|</span><span class="mi">1991</span><span class="o">-</span><span class="mi">11</span><span class="o">-</span><span class="mi">15</span><span class="o">|</span>       <span class="n">a</span><span class="o">|</span>         <span class="mi">23</span><span class="o">|</span>
<span class="o">|</span><span class="mi">1991</span><span class="o">-</span><span class="mi">11</span><span class="o">-</span><span class="mi">16</span><span class="o">|</span>       <span class="n">a</span><span class="o">|</span>         <span class="mi">24</span><span class="o">|</span>
<span class="o">|</span><span class="mi">1991</span><span class="o">-</span><span class="mi">11</span><span class="o">-</span><span class="mi">17</span><span class="o">|</span>       <span class="n">a</span><span class="o">|</span>         <span class="mi">32</span><span class="o">|</span>
<span class="o">|</span><span class="mi">1991</span><span class="o">-</span><span class="mi">11</span><span class="o">-</span><span class="mi">25</span><span class="o">|</span>       <span class="n">b</span><span class="o">|</span>         <span class="mi">13</span><span class="o">|</span>
<span class="o">|</span><span class="mi">1991</span><span class="o">-</span><span class="mi">11</span><span class="o">-</span><span class="mi">26</span><span class="o">|</span>       <span class="n">b</span><span class="o">|</span>         <span class="mi">14</span><span class="o">|</span>
<span class="o">+----------+--------+-----------+</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">groupby</span><span class="p">(</span><span class="s2">&quot;customer&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">agg</span><span class="p">(</span><span class="n">psf</span><span class="o">.</span><span class="n">collect_list</span><span class="p">(</span><span class="s1">&#39;date&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s1">&#39;time_series_dates&#39;</span><span class="p">),</span>
                              <span class="n">psf</span><span class="o">.</span><span class="n">collect_list</span><span class="p">(</span><span class="s1">&#39;balance_day&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s1">&#39;time_series_values&#39;</span><span class="p">),</span>
                              <span class="n">psf</span><span class="o">.</span><span class="n">collect_list</span><span class="p">(</span><span class="n">psf</span><span class="o">.</span><span class="n">struct</span><span class="p">(</span><span class="s1">&#39;date&#39;</span><span class="p">,</span><span class="s1">&#39;balance_day&#39;</span><span class="p">))</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s1">&#39;time_series_tuples&#39;</span><span class="p">))</span>


<span class="n">df</span><span class="o">.</span><span class="n">show</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span><span class="kc">False</span><span class="p">)</span>

<span class="o">+--------+------------------------------------+------------------+---------------------------------------------------+</span>
<span class="o">|</span><span class="n">customer</span><span class="o">|</span><span class="n">time_series_dates</span>                   <span class="o">|</span><span class="n">time_series_values</span><span class="o">|</span><span class="n">time_series_tuples</span>                                 <span class="o">|</span>
<span class="o">+--------+------------------------------------+------------------+---------------------------------------------------+</span>
<span class="o">|</span><span class="n">b</span>       <span class="o">|</span><span class="p">[</span><span class="mi">1991</span><span class="o">-</span><span class="mi">11</span><span class="o">-</span><span class="mi">25</span><span class="p">,</span> <span class="mi">1991</span><span class="o">-</span><span class="mi">11</span><span class="o">-</span><span class="mi">26</span><span class="p">]</span>            <span class="o">|</span><span class="p">[</span><span class="mi">13</span><span class="p">,</span> <span class="mi">14</span><span class="p">]</span>          <span class="o">|</span><span class="p">[[</span><span class="mi">1991</span><span class="o">-</span><span class="mi">11</span><span class="o">-</span><span class="mi">25</span><span class="p">,</span><span class="mi">13</span><span class="p">],</span> <span class="p">[</span><span class="mi">1991</span><span class="o">-</span><span class="mi">11</span><span class="o">-</span><span class="mi">26</span><span class="p">,</span><span class="mi">14</span><span class="p">]]</span>                 <span class="o">|</span>
<span class="o">|</span><span class="n">a</span>       <span class="o">|</span><span class="p">[</span><span class="mi">1991</span><span class="o">-</span><span class="mi">11</span><span class="o">-</span><span class="mi">15</span><span class="p">,</span> <span class="mi">1991</span><span class="o">-</span><span class="mi">11</span><span class="o">-</span><span class="mi">16</span><span class="p">,</span> <span class="mi">1991</span><span class="o">-</span><span class="mi">11</span><span class="o">-</span><span class="mi">17</span><span class="p">]</span><span class="o">|</span><span class="p">[</span><span class="mi">23</span><span class="p">,</span> <span class="mi">24</span><span class="p">,</span> <span class="mi">32</span><span class="p">]</span>      <span class="o">|</span><span class="p">[[</span><span class="mi">1991</span><span class="o">-</span><span class="mi">11</span><span class="o">-</span><span class="mi">15</span><span class="p">,</span><span class="mi">23</span><span class="p">],</span> <span class="p">[</span><span class="mi">1991</span><span class="o">-</span><span class="mi">11</span><span class="o">-</span><span class="mi">16</span><span class="p">,</span><span class="mi">24</span><span class="p">],</span> <span class="p">[</span><span class="mi">1991</span><span class="o">-</span><span class="mi">11</span><span class="o">-</span><span class="mi">17</span><span class="p">,</span><span class="mi">32</span><span class="p">]]</span><span class="o">|</span>
<span class="o">+--------+------------------------------------+------------------+---------------------------------------------------+</span>
</pre></div>
</div>
</div>
<div class="section" id="revert-from-time-series-list-format-to-traditional-exploded-format">
<h3>Revert from time series (list) format to traditional (exploded) format<a class="headerlink" href="#revert-from-time-series-list-format-to-traditional-exploded-format" title="Permalink to this headline">¶</a></h3>
<p>Taken from <a class="reference external" href="https://stackoverflow.com/questions/41027315/pyspark-split-multiple-array-columns-into-rows">https://stackoverflow.com/questions/41027315/pyspark-split-multiple-array-columns-into-rows</a></p>
<p>Let’s say we have 2 customers 1 and 2</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">pyspark.sql</span> <span class="kn">import</span> <span class="n">Row</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">sqlContext</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">([</span><span class="n">Row</span><span class="p">(</span><span class="n">customer</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">time</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">],</span><span class="n">value</span><span class="o">=</span><span class="p">[</span><span class="mi">7</span><span class="p">,</span><span class="mi">8</span><span class="p">,</span><span class="mi">9</span><span class="p">]),</span> <span class="n">Row</span><span class="p">(</span><span class="n">customer</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">time</span><span class="o">=</span><span class="p">[</span><span class="mi">4</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">6</span><span class="p">],</span><span class="n">value</span><span class="o">=</span><span class="p">[</span><span class="mi">10</span><span class="p">,</span><span class="mi">11</span><span class="p">,</span><span class="mi">12</span><span class="p">])])</span>
<span class="n">df</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="o">+--------+---------+------------+</span>
<span class="o">|</span><span class="n">customer</span><span class="o">|</span>     <span class="n">time</span><span class="o">|</span>       <span class="n">value</span><span class="o">|</span>
<span class="o">+--------+---------+------------+</span>
<span class="o">|</span>       <span class="mi">1</span><span class="o">|</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]</span><span class="o">|</span>   <span class="p">[</span><span class="mi">7</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">9</span><span class="p">]</span><span class="o">|</span>
<span class="o">|</span>       <span class="mi">2</span><span class="o">|</span><span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">]</span><span class="o">|</span><span class="p">[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">11</span><span class="p">,</span> <span class="mi">12</span><span class="p">]</span><span class="o">|</span>
<span class="o">+--------+---------+------------+</span>

<span class="n">df_exploded</span> <span class="o">=</span> <span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">rdd</span>
               <span class="o">.</span><span class="n">flatMap</span><span class="p">(</span><span class="k">lambda</span> <span class="n">row</span><span class="p">:</span> <span class="p">[(</span><span class="n">row</span><span class="o">.</span><span class="n">key</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">)</span> <span class="k">for</span> <span class="n">b</span><span class="p">,</span> <span class="n">c</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">row</span><span class="o">.</span><span class="n">time</span><span class="p">,</span> <span class="n">row</span><span class="o">.</span><span class="n">value</span><span class="p">)])</span>
               <span class="o">.</span><span class="n">toDF</span><span class="p">([</span><span class="s1">&#39;key&#39;</span><span class="p">,</span> <span class="s1">&#39;time&#39;</span><span class="p">,</span> <span class="s1">&#39;value&#39;</span><span class="p">]))</span>

<span class="o">+--------+--------+---------+</span>
<span class="o">|</span><span class="n">customer</span><span class="o">|</span><span class="n">time_row</span><span class="o">|</span><span class="n">value_row</span><span class="o">|</span>
<span class="o">+--------+--------+---------+</span>
<span class="o">|</span>       <span class="mi">1</span><span class="o">|</span>       <span class="mi">1</span><span class="o">|</span>        <span class="mi">7</span><span class="o">|</span>
<span class="o">|</span>       <span class="mi">1</span><span class="o">|</span>       <span class="mi">2</span><span class="o">|</span>        <span class="mi">8</span><span class="o">|</span>
<span class="o">|</span>       <span class="mi">1</span><span class="o">|</span>       <span class="mi">3</span><span class="o">|</span>        <span class="mi">9</span><span class="o">|</span>
<span class="o">|</span>       <span class="mi">2</span><span class="o">|</span>       <span class="mi">4</span><span class="o">|</span>       <span class="mi">10</span><span class="o">|</span>
<span class="o">|</span>       <span class="mi">2</span><span class="o">|</span>       <span class="mi">5</span><span class="o">|</span>       <span class="mi">11</span><span class="o">|</span>
<span class="o">|</span>       <span class="mi">2</span><span class="o">|</span>       <span class="mi">6</span><span class="o">|</span>       <span class="mi">12</span><span class="o">|</span>
<span class="o">+--------+--------+---------+</span>
</pre></div>
</div>
<p>Based on this, here is a function that does the same:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">explode_time_series</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">time</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
  <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">  This function explodes the time series format to classical format</span>

<span class="sd">  Input  : - df          : the dataframe containin the time series</span>
<span class="sd">           - key         : the name of the key column (ex: &quot;customer&quot;)</span>
<span class="sd">           - time        : the name of the time column</span>
<span class="sd">           - value       : the name of the value column</span>

<span class="sd">  Output : - df_exploded : the same dataframe as input but with exploded time and value</span>

<span class="sd">  example: a simple dataframe with time series:</span>

<span class="sd">  from pyspark.sql import Row</span>
<span class="sd">  df = sqlContext.createDataFrame([Row(customer=1, time=[1,2,3],value=[7,8,9]), Row(customer=2, time=[4,5,6],value=[10,11,12])])</span>
<span class="sd">  df.show()</span>

<span class="sd">  +--------+---------+------------+</span>
<span class="sd">  |customer|     time|       value|</span>
<span class="sd">  +--------+---------+------------+</span>
<span class="sd">  |       1|[1, 2, 3]|   [7, 8, 9]|</span>
<span class="sd">  |       2|[4, 5, 6]|[10, 11, 12]|</span>
<span class="sd">  +--------+---------+------------+</span>

<span class="sd">  will become:</span>

<span class="sd">  df_exploded = explode_time_series(df,&#39;customer&#39;,&#39;time&#39;,&#39;value&#39;)</span>
<span class="sd">  df_exploded.show()</span>

<span class="sd">  +--------+--------+---------+</span>
<span class="sd">  |customer|time_row|value_row|</span>
<span class="sd">  +--------+--------+---------+</span>
<span class="sd">  |       1|       1|        7|</span>
<span class="sd">  |       1|       2|        8|</span>
<span class="sd">  |       1|       3|        9|</span>
<span class="sd">  |       2|       4|       10|</span>
<span class="sd">  |       2|       5|       11|</span>
<span class="sd">  |       2|       6|       12|</span>
<span class="sd">  +--------+--------+---------+</span>
<span class="sd">  &#39;&#39;&#39;</span>

  <span class="n">df_exploded</span> <span class="o">=</span> <span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">rdd</span>
                 <span class="o">.</span><span class="n">flatMap</span><span class="p">(</span><span class="k">lambda</span> <span class="n">row</span><span class="p">:</span> <span class="p">[(</span><span class="n">row</span><span class="p">[</span><span class="n">key</span><span class="p">],</span> <span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">)</span> <span class="k">for</span> <span class="n">b</span><span class="p">,</span> <span class="n">c</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">row</span><span class="p">[</span><span class="n">time</span><span class="p">],</span> <span class="n">row</span><span class="p">[</span><span class="n">value</span><span class="p">])])</span>
                 <span class="o">.</span><span class="n">toDF</span><span class="p">([</span><span class="n">key</span><span class="p">,</span> <span class="n">time</span><span class="p">,</span> <span class="n">value</span><span class="p">]))</span>

  <span class="k">return</span> <span class="n">df_exploded</span>

<span class="n">df_exploded</span> <span class="o">=</span> <span class="n">explode_time_series</span><span class="p">(</span><span class="n">df</span><span class="p">,</span><span class="s1">&#39;customer&#39;</span><span class="p">,</span><span class="s1">&#39;time&#39;</span><span class="p">,</span><span class="s1">&#39;value&#39;</span><span class="p">)</span>
<span class="n">df_exploded</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="section" id="converting-dates-in-pyspark">
<h3>Converting dates in Pyspark<a class="headerlink" href="#converting-dates-in-pyspark" title="Permalink to this headline">¶</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1">#Converting date from yyyy mm dd to year, month, day</span>
<span class="kn">from</span> <span class="nn">pyspark.sql.functions</span> <span class="kn">import</span> <span class="n">year</span><span class="p">,</span> <span class="n">month</span><span class="p">,</span> <span class="n">dayofmonth</span>
<span class="n">d</span> <span class="o">=</span> <span class="p">[{</span><span class="s1">&#39;date&#39;</span><span class="p">:</span> <span class="s1">&#39;20170412&#39;</span><span class="p">}]</span>
<span class="n">dp_data</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">d</span><span class="p">)</span>

<span class="n">df_date</span> <span class="o">=</span> <span class="n">sqlContext</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">(</span><span class="n">dp_data</span><span class="p">)</span>
<span class="n">df_date</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="o">+--------+</span>
<span class="o">|</span>    <span class="n">date</span><span class="o">|</span>
<span class="o">+--------+</span>
<span class="o">|</span><span class="mi">20170412</span><span class="o">|</span>
<span class="o">+--------+</span>

<span class="n">df_date</span> <span class="o">=</span> <span class="n">df_date</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">from_unixtime</span><span class="p">(</span><span class="n">unix_timestamp</span><span class="p">(</span><span class="s1">&#39;date&#39;</span><span class="p">,</span> <span class="s1">&#39;yyyyMMdd&#39;</span><span class="p">))</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s1">&#39;date&#39;</span><span class="p">))</span> <span class="c1">#date should first be converted to unixtime</span>
<span class="n">df_date</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="s2">&quot;date&quot;</span><span class="p">,</span><span class="n">year</span><span class="p">(</span><span class="s2">&quot;date&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s1">&#39;year&#39;</span><span class="p">),</span> <span class="n">month</span><span class="p">(</span><span class="s2">&quot;date&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s1">&#39;month&#39;</span><span class="p">),</span> <span class="n">dayofmonth</span><span class="p">(</span><span class="s2">&quot;date&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s1">&#39;day&#39;</span><span class="p">))</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="o">+-------------------+----+-----+---+</span>
<span class="o">|</span>               <span class="n">date</span><span class="o">|</span><span class="n">year</span><span class="o">|</span><span class="n">month</span><span class="o">|</span><span class="n">day</span><span class="o">|</span>
<span class="o">+-------------------+----+-----+---+</span>
<span class="o">|</span><span class="mi">2017</span><span class="o">-</span><span class="mi">04</span><span class="o">-</span><span class="mi">12</span> <span class="mi">00</span><span class="p">:</span><span class="mi">00</span><span class="p">:</span><span class="mi">00</span><span class="o">|</span><span class="mi">2017</span><span class="o">|</span>    <span class="mi">4</span><span class="o">|</span> <span class="mi">12</span><span class="o">|</span>
<span class="o">+-------------------+----+-----+---+</span>
</pre></div>
</div>
<p>Create column by casting a date, using to_date:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">txn</span> <span class="o">=</span> <span class="n">transactions</span><span class="o">.</span><span class="n">withColumn</span><span class="p">(</span><span class="s1">&#39;date&#39;</span><span class="p">,</span><span class="n">to_date</span><span class="p">(</span><span class="n">transactions</span><span class="o">.</span><span class="n">transactiondate</span><span class="p">))</span>
</pre></div>
</div>
<p>Convert a column to date time:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">df</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">withColumn</span><span class="p">(</span><span class="s1">&#39;date&#39;</span><span class="p">,</span> <span class="n">col</span><span class="p">(</span><span class="s1">&#39;date_string&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">DateType</span><span class="p">()))</span>
</pre></div>
</div>
<p>Get the minimum date and maximum date of a column:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">pyspark.sql.functions</span> <span class="kn">import</span> <span class="nb">min</span><span class="p">,</span> <span class="nb">max</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">([</span>
  <span class="s2">&quot;2017-01-01&quot;</span><span class="p">,</span> <span class="s2">&quot;2018-02-08&quot;</span><span class="p">,</span> <span class="s2">&quot;2019-01-03&quot;</span><span class="p">],</span> <span class="s2">&quot;string&quot;</span>
<span class="p">)</span><span class="o">.</span><span class="n">selectExpr</span><span class="p">(</span><span class="s2">&quot;CAST(value AS date) AS date&quot;</span><span class="p">)</span>

<span class="n">min_date</span><span class="p">,</span> <span class="n">max_date</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="nb">min</span><span class="p">(</span><span class="s2">&quot;date&quot;</span><span class="p">),</span> <span class="nb">max</span><span class="p">(</span><span class="s2">&quot;date&quot;</span><span class="p">))</span><span class="o">.</span><span class="n">first</span><span class="p">()</span>
<span class="n">min_date</span><span class="p">,</span> <span class="n">max_date</span>
</pre></div>
</div>
<p>Select rows (filter) between 2 dates (or datetime):</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">txn</span> <span class="o">=</span> <span class="n">txn</span><span class="o">.</span><span class="n">filter</span><span class="p">(</span><span class="n">col</span><span class="p">(</span><span class="s2">&quot;date&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">between</span><span class="p">(</span><span class="s1">&#39;2017-01-01&#39;</span><span class="p">,</span><span class="s1">&#39;2018-12-31&#39;</span><span class="p">))</span> <span class="c1">#for dates only</span>

<span class="n">txn</span> <span class="o">=</span> <span class="n">txn</span><span class="o">.</span><span class="n">filter</span><span class="p">(</span><span class="n">col</span><span class="p">(</span><span class="s2">&quot;date&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">between</span><span class="p">(</span><span class="n">pd</span><span class="o">.</span><span class="n">to_datetime</span><span class="p">(</span><span class="s1">&#39;2017-04-13&#39;</span><span class="p">),</span><span class="n">pd</span><span class="o">.</span><span class="n">to_datetime</span><span class="p">(</span><span class="s1">&#39;2017-04-14&#39;</span><span class="p">))</span> <span class="c1">#for dates only; works also with pandas dates</span>

<span class="n">txn</span> <span class="o">=</span> <span class="n">txn</span><span class="o">.</span><span class="n">filter</span><span class="p">(</span><span class="n">col</span><span class="p">(</span><span class="s2">&quot;datetime&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">between</span><span class="p">(</span><span class="s1">&#39;2017-04-13 12:00:00&#39;</span><span class="p">,</span><span class="s1">&#39;2017-04-14 00:00:00&#39;</span><span class="p">))</span> <span class="c1">#for datetime</span>
</pre></div>
</div>
<p>Create a df with a date range:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">date_range</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">()</span>
<span class="n">date_range</span><span class="p">[</span><span class="s1">&#39;date&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">date_range</span><span class="p">(</span><span class="n">start</span><span class="o">=</span><span class="s1">&#39;2017-01-01&#39;</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="s1">&#39;2018-09-01&#39;</span><span class="p">)</span>
<span class="n">date_range_sp</span> <span class="o">=</span> <span class="n">sqlContext</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">(</span><span class="n">date_range</span><span class="p">)</span>
<span class="n">date_range_sp</span> <span class="o">=</span> <span class="n">date_range_sp</span><span class="o">.</span><span class="n">withColumn</span><span class="p">(</span><span class="s1">&#39;date&#39;</span><span class="p">,</span><span class="n">to_date</span><span class="p">(</span><span class="s1">&#39;date&#39;</span><span class="p">,</span> <span class="s2">&quot;yyyy-MM-dd&quot;</span><span class="p">))</span>
<span class="n">date_range_sp</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<p>Casting to timestamp from string with format 2015-01-01 23:59:59:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span> <span class="n">df</span><span class="o">.</span><span class="n">start_time</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="s2">&quot;timestamp&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s2">&quot;start_time&quot;</span><span class="p">)</span> <span class="p">)</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># datetime function</span>
<span class="n">current_date</span><span class="p">,</span> <span class="n">current_timestamp</span><span class="p">,</span> <span class="n">trunc</span><span class="p">,</span> <span class="n">date_format</span>
<span class="n">datediff</span><span class="p">,</span> <span class="n">date_add</span><span class="p">,</span> <span class="n">date_sub</span><span class="p">,</span> <span class="n">add_months</span><span class="p">,</span> <span class="n">last_day</span><span class="p">,</span> <span class="n">next_day</span><span class="p">,</span> <span class="n">months_between</span>
<span class="n">year</span><span class="p">,</span> <span class="n">month</span><span class="p">,</span> <span class="n">dayofmonth</span><span class="p">,</span> <span class="n">hour</span><span class="p">,</span> <span class="n">minute</span><span class="p">,</span> <span class="n">second</span>
<span class="n">unix_timestamp</span><span class="p">,</span> <span class="n">from_unixtime</span><span class="p">,</span> <span class="n">to_date</span><span class="p">,</span> <span class="n">quarter</span><span class="p">,</span> <span class="n">day</span><span class="p">,</span> <span class="n">dayofyear</span><span class="p">,</span> <span class="n">weekofyear</span><span class="p">,</span> <span class="n">from_utc_timestamp</span><span class="p">,</span> <span class="n">to_utc_timestamp</span>
</pre></div>
</div>
</div>
<div class="section" id="nan-null-none-handling">
<h3>NaN/Null/None handling<a class="headerlink" href="#nan-null-none-handling" title="Permalink to this headline">¶</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1">#dropping NaN in whole dataframe:</span>
<span class="n">df</span><span class="o">.</span><span class="n">na</span><span class="o">.</span><span class="n">drop</span><span class="p">()</span>

<span class="c1">#dropping NaN in one column (it will remove all rows of the df where that column contains a NaN):</span>
<span class="c1">#df.select(&quot;column&quot;).na.drop()  this does not work!</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s2">&quot;column&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">isNull</span><span class="p">())</span> <span class="c1">#or df = df.where(df.column.isNull())</span>

<span class="c1">#Filling with NaN or with whatever value, let&#39;s say 50:</span>
<span class="n">df</span><span class="o">.</span><span class="n">na</span><span class="o">.</span><span class="n">fill</span><span class="p">(</span><span class="mi">50</span><span class="p">)</span>

<span class="c1">#Count how many NaN/Null/None in a column:</span>
<span class="n">df</span><span class="o">.</span><span class="n">filter</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">columnName</span><span class="o">.</span><span class="n">isNull</span><span class="p">())</span><span class="o">.</span><span class="n">count</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="section" id="saving-a-table-in-hadoop">
<h3>Saving a table in Hadoop<a class="headerlink" href="#saving-a-table-in-hadoop" title="Permalink to this headline">¶</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1">#mode: one of append, overwrite, error, ignore (default: error)</span>
<span class="c1">#partitionBy: names of partitioning columns</span>
<span class="n">p2</span><span class="o">.</span><span class="n">saveAsTable</span><span class="p">(</span><span class="s1">&#39;risk_work.TULE_savetest&#39;</span><span class="p">,</span><span class="n">partitionBy</span><span class="o">=</span><span class="s1">&#39;KNID&#39;</span><span class="p">,</span><span class="n">mode</span><span class="o">=</span><span class="s1">&#39;overwrite&#39;</span><span class="p">)</span>
</pre></div>
</div>
<p>PySpark does not save the table in an ORC format - therefore, we cannot query the saved tables via Ambari or SQL Developer. So if you want to be able to use these programs to investigate your created tables, you should save the tables like this:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1">#Classical saving</span>
<span class="n">df</span><span class="o">.</span><span class="n">saveAsTable</span><span class="p">(</span><span class="s1">&#39;risk_temp.table_name, mode=&#39;</span><span class="n">overwrite</span><span class="s1">&#39;)</span>
<span class="c1">#Specifying the format</span>
<span class="n">df</span><span class="o">.</span><span class="n">write</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="s2">&quot;ORC&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">saveAsTable</span><span class="p">(</span><span class="s1">&#39;risk_temp.table_name, mode=&#39;</span><span class="n">overwrite</span><span class="s1">&#39;)</span>
</pre></div>
</div>
</div>
<div class="section" id="filtering-data-in-pyspark">
<h3>Filtering data in Pyspark<a class="headerlink" href="#filtering-data-in-pyspark" title="Permalink to this headline">¶</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1">#example 1:</span>
<span class="n">p2</span> <span class="o">=</span> <span class="n">p1</span><span class="o">.</span><span class="n">filter</span><span class="p">(</span><span class="n">p1</span><span class="o">.</span><span class="n">BLPS</span> <span class="o">&gt;</span><span class="mi">0</span><span class="p">)</span>

<span class="c1">#example 2:</span>
<span class="n">p3</span> <span class="o">=</span> <span class="n">p2</span><span class="o">.</span><span class="n">filter</span><span class="p">(</span><span class="n">trim</span><span class="p">(</span><span class="n">p2</span><span class="o">.</span><span class="n">KNID</span><span class="p">)</span> <span class="o">==</span> <span class="s1">&#39;0011106277&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">cache</span><span class="p">()</span>

<span class="c1"># we can also use the &quot;between&quot; function:</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">filter</span><span class="p">(</span><span class="n">col</span><span class="p">(</span><span class="s2">&quot;age&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">between</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span><span class="mi">30</span><span class="p">))</span>
</pre></div>
</div>
<p>Here is a comparison of the filtering of a dataframe done in Pandas and the same operation done in Pyspark (taken from <a class="reference external" href="https://lab.getbase.com/pandarize-spark-dataframes/">https://lab.getbase.com/pandarize-spark-dataframes/</a>):</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1">#Pandas:</span>
<span class="n">sliced</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="n">data</span><span class="o">.</span><span class="n">workclass</span><span class="o">.</span><span class="n">isin</span><span class="p">([</span><span class="s1">&#39; Local-gov&#39;</span><span class="p">,</span> <span class="s1">&#39; State-gov&#39;</span><span class="p">])</span> \
               <span class="o">&amp;</span> <span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">education_num</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">)][[</span><span class="s1">&#39;age&#39;</span><span class="p">,</span> <span class="s1">&#39;workclass&#39;</span><span class="p">]]</span>

<span class="n">sliced</span><span class="o">.</span><span class="n">head</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

   <span class="n">age</span>   <span class="n">workclass</span>
<span class="mi">0</span>   <span class="mi">39</span>   <span class="n">State</span><span class="o">-</span><span class="n">gov</span>

<span class="c1">#Pyspark:</span>
<span class="n">slicedSpark</span> <span class="o">=</span> <span class="n">dataSpark</span><span class="p">[</span><span class="n">dataSpark</span><span class="o">.</span><span class="n">workclass</span><span class="o">.</span><span class="n">inSet</span><span class="p">([</span><span class="s1">&#39; Local-gov&#39;</span><span class="p">,</span> <span class="s1">&#39; State-gov&#39;</span><span class="p">])</span>
                         <span class="o">&amp;</span> <span class="p">(</span><span class="n">dataSpark</span><span class="o">.</span><span class="n">education_num</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">)][[</span><span class="s1">&#39;age&#39;</span><span class="p">,</span> <span class="s1">&#39;workclass&#39;</span><span class="p">]]</span>

<span class="n">slicedSpark</span><span class="o">.</span><span class="n">take</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

<span class="p">[</span><span class="n">Row</span><span class="p">(</span><span class="n">age</span><span class="o">=</span><span class="mf">48.0</span><span class="p">,</span> <span class="n">workclass</span><span class="o">=</span><span class="sa">u</span><span class="s1">&#39; State-gov&#39;</span><span class="p">)]</span>
</pre></div>
</div>
<p>There is one important difference. In Pandas, boolean slicing expects just a boolean series, which means you can apply filter from another DataFrame if they match in length. In Pyspark you can only filter data based on columns from DataFrame you want to filter.</p>
</div>
<div class="section" id="opening-tables-from-data-warehouse">
<h3>Opening tables from Data Warehouse<a class="headerlink" href="#opening-tables-from-data-warehouse" title="Permalink to this headline">¶</a></h3>
<p>(strongly outdated)</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">sys</span>
<span class="n">TOOLS_PATH</span> <span class="o">=</span> <span class="s1">&#39;/home/BC3589/Git/tools&#39;</span>
<span class="k">if</span> <span class="n">TOOLS_PATH</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">sys</span><span class="o">.</span><span class="n">path</span><span class="p">:</span>
  <span class="n">sys</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">TOOLS_PATH</span><span class="p">)</span>

<span class="kn">from</span> <span class="nn">connection.SQLConnector</span> <span class="kn">import</span> <span class="n">SQLConnector</span>

<span class="c1"># testing connection to Exploration Warehouse</span>
<span class="n">etpew_connector</span> <span class="o">=</span> <span class="n">SQLConnector</span><span class="p">(</span><span class="s1">&#39;ETPEW&#39;</span><span class="p">)</span>
<span class="n">sql</span> <span class="o">=</span> <span class="s2">&quot;select top 1 * from [ETZ3EW].[dbo].[ZW_KUNDE_MST_HV];&quot;</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">etpew_connector</span><span class="o">.</span><span class="n">query_to_pandas</span><span class="p">(</span><span class="n">sql</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Loaded from [ETZ3EW].[dbo].[ZW_KUNDE_MST_HV]&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

<span class="c1"># testing connection to MCS</span>
<span class="n">mcs_connector</span> <span class="o">=</span> <span class="n">SQLConnector</span><span class="p">(</span><span class="s1">&#39;MCS&#39;</span><span class="p">)</span>
<span class="n">sql</span> <span class="o">=</span> <span class="s2">&quot;SELECT top 1 * FROM sys.databases&quot;</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">mcs_connector</span><span class="o">.</span><span class="n">query_to_pandas</span><span class="p">(</span><span class="n">sql</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Loaded from sys.databases&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="section" id="user-defined-functions-udf">
<h3>User-defined functions (UDF)<a class="headerlink" href="#user-defined-functions-udf" title="Permalink to this headline">¶</a></h3>
<p>Here is a very good and simple introduction: <a class="reference external" href="https://changhsinlee.com/pyspark-udf/">https://changhsinlee.com/pyspark-udf/</a></p>
<p>Simple example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">pyspark.sql.types</span> <span class="kn">import</span> <span class="n">StringType</span>
<span class="kn">from</span> <span class="nn">pyspark.sql.functions</span> <span class="kn">import</span> <span class="n">udf</span>

<span class="n">maturity_udf</span> <span class="o">=</span> <span class="n">udf</span><span class="p">(</span><span class="k">lambda</span> <span class="n">age</span><span class="p">:</span> <span class="s2">&quot;adult&quot;</span> <span class="k">if</span> <span class="n">age</span> <span class="o">&gt;=</span><span class="mi">18</span> <span class="k">else</span> <span class="s2">&quot;child&quot;</span><span class="p">,</span> <span class="n">StringType</span><span class="p">())</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">sqlContext</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">([{</span><span class="s1">&#39;name&#39;</span><span class="p">:</span> <span class="s1">&#39;Alice&#39;</span><span class="p">,</span> <span class="s1">&#39;age&#39;</span><span class="p">:</span> <span class="mi">1</span><span class="p">}])</span>
<span class="n">df</span><span class="o">.</span><span class="n">withColumn</span><span class="p">(</span><span class="s2">&quot;maturity&quot;</span><span class="p">,</span> <span class="n">maturity_udf</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">age</span><span class="p">))</span>
</pre></div>
</div>
<p>Here is an example of removal of whitespaces in a string column of a dataframe:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">pyspark.sql.types</span> <span class="kn">import</span> <span class="n">StringType</span>

<span class="n">spaceDeleteUDF</span> <span class="o">=</span> <span class="n">udf</span><span class="p">(</span><span class="k">lambda</span> <span class="n">s</span><span class="p">:</span> <span class="n">s</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">&quot; &quot;</span><span class="p">,</span> <span class="s2">&quot;&quot;</span><span class="p">),</span> <span class="n">StringType</span><span class="p">())</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">sqlContext</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">([(</span><span class="s2">&quot;aaa 111&quot;</span><span class="p">,),</span> <span class="p">(</span><span class="s2">&quot;bbb 222&quot;</span><span class="p">,),</span> <span class="p">(</span><span class="s2">&quot;ccc 333&quot;</span><span class="p">,)],</span> <span class="p">[</span><span class="s2">&quot;names&quot;</span><span class="p">])</span>

<span class="n">df</span><span class="o">.</span><span class="n">withColumn</span><span class="p">(</span><span class="s2">&quot;names&quot;</span><span class="p">,</span> <span class="n">spaceDeleteUDF</span><span class="p">(</span><span class="s2">&quot;names&quot;</span><span class="p">))</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="o">+------+</span>
<span class="o">|</span> <span class="n">names</span><span class="o">|</span>
<span class="o">+------+</span>
<span class="o">|</span><span class="n">aaa111</span><span class="o">|</span>
<span class="o">|</span><span class="n">bbb222</span><span class="o">|</span>
<span class="o">|</span><span class="n">ccc333</span><span class="o">|</span>
<span class="o">+------+</span>
</pre></div>
</div>
<p>Here is a great example of a UDF with MULTIPLE COLUMNS AS INPUT:</p>
<p>(Taken from <a class="reference external" href="https://stackoverflow.com/questions/47824841/pyspark-passing-multiple-dataframe-fields-to-udf?rq=1">https://stackoverflow.com/questions/47824841/pyspark-passing-multiple-dataframe-fields-to-udf?rq=1</a>)</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">math</span>

<span class="k">def</span> <span class="nf">distance</span><span class="p">(</span><span class="n">origin</span><span class="p">,</span> <span class="n">destination</span><span class="p">):</span>
  <span class="n">lat1</span><span class="p">,</span> <span class="n">lon1</span> <span class="o">=</span> <span class="n">origin</span>
  <span class="n">lat2</span><span class="p">,</span> <span class="n">lon2</span> <span class="o">=</span> <span class="n">destination</span>
  <span class="n">radius</span> <span class="o">=</span> <span class="mi">6371</span> <span class="c1"># km</span>
  <span class="n">dlat</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">radians</span><span class="p">(</span><span class="n">lat2</span><span class="o">-</span><span class="n">lat1</span><span class="p">)</span>
  <span class="n">dlon</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">radians</span><span class="p">(</span><span class="n">lon2</span><span class="o">-</span><span class="n">lon1</span><span class="p">)</span>
  <span class="n">a</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">dlat</span><span class="o">/</span><span class="mi">2</span><span class="p">)</span> <span class="o">*</span> <span class="n">math</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">dlat</span><span class="o">/</span><span class="mi">2</span><span class="p">)</span> <span class="o">+</span> <span class="n">math</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">math</span><span class="o">.</span><span class="n">radians</span><span class="p">(</span><span class="n">lat1</span><span class="p">))</span> \
  <span class="o">*</span> <span class="n">math</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">math</span><span class="o">.</span><span class="n">radians</span><span class="p">(</span><span class="n">lat2</span><span class="p">))</span> <span class="o">*</span> <span class="n">math</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">dlon</span><span class="o">/</span><span class="mi">2</span><span class="p">)</span> <span class="o">*</span> <span class="n">math</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">dlon</span><span class="o">/</span><span class="mi">2</span><span class="p">)</span>
  <span class="n">c</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">math</span><span class="o">.</span><span class="n">atan2</span><span class="p">(</span><span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">a</span><span class="p">),</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">a</span><span class="p">))</span>
  <span class="n">d</span> <span class="o">=</span> <span class="n">radius</span> <span class="o">*</span> <span class="n">c</span>
  <span class="k">return</span> <span class="n">d</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">([([</span><span class="mi">101</span><span class="p">,</span> <span class="mi">121</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">121</span><span class="p">,</span> <span class="o">-</span><span class="mi">212</span><span class="p">])],</span> <span class="p">[</span><span class="s2">&quot;origin&quot;</span><span class="p">,</span> <span class="s2">&quot;destination&quot;</span><span class="p">])</span>
<span class="n">filter_udf</span> <span class="o">=</span> <span class="n">psf</span><span class="o">.</span><span class="n">udf</span><span class="p">(</span><span class="n">distance</span><span class="p">,</span> <span class="n">pst</span><span class="o">.</span><span class="n">DoubleType</span><span class="p">())</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">withColumn</span><span class="p">(</span><span class="s2">&quot;distance&quot;</span><span class="p">,</span> <span class="n">filter_udf</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">origin</span><span class="p">,</span> <span class="n">df</span><span class="o">.</span><span class="n">destination</span><span class="p">))</span>
<span class="n">df</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="o">+----------+------------+------------------+</span>
<span class="o">|</span>    <span class="n">origin</span><span class="o">|</span> <span class="n">destination</span><span class="o">|</span>          <span class="n">distance</span><span class="o">|</span>
<span class="o">+----------+------------+------------------+</span>
<span class="o">|</span><span class="p">[</span><span class="mi">101</span><span class="p">,</span> <span class="mi">121</span><span class="p">]</span><span class="o">|</span><span class="p">[</span><span class="o">-</span><span class="mi">121</span><span class="p">,</span> <span class="o">-</span><span class="mi">212</span><span class="p">]</span><span class="o">|</span><span class="mf">15447.812243421227</span><span class="o">|</span>
<span class="o">+----------+------------+------------------+</span>
</pre></div>
</div>
<p>Here is an example of a UDF with MULTIPLE COLUMNS AS OUTPUT:</p>
<p>(Taken from <a class="reference external" href="https://stackoverflow.com/questions/47669895/how-to-add-multiple-columns-using-udf?rq=1">https://stackoverflow.com/questions/47669895/how-to-add-multiple-columns-using-udf?rq=1</a>)</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pyspark.sql.types</span> <span class="k">as</span> <span class="nn">pst</span>
<span class="kn">from</span> <span class="nn">pyspark.sql</span> <span class="kn">import</span> <span class="n">Row</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">([(</span><span class="s2">&quot;Alive&quot;</span><span class="p">,</span> <span class="mi">4</span><span class="p">)],</span> <span class="p">[</span><span class="s2">&quot;Name&quot;</span><span class="p">,</span> <span class="s2">&quot;Number&quot;</span><span class="p">])</span>

<span class="k">def</span> <span class="nf">example</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">Row</span><span class="p">(</span><span class="s1">&#39;Signal_type&#39;</span><span class="p">,</span> <span class="s1">&#39;array&#39;</span><span class="p">)(</span><span class="n">n</span> <span class="o">+</span> <span class="mi">2</span><span class="p">,</span> <span class="p">[</span><span class="n">n</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span><span class="n">n</span><span class="o">+</span><span class="mi">2</span><span class="p">])</span>


<span class="n">schema</span> <span class="o">=</span> <span class="n">pst</span><span class="o">.</span><span class="n">StructType</span><span class="p">([</span>
    <span class="n">pst</span><span class="o">.</span><span class="n">StructField</span><span class="p">(</span><span class="s2">&quot;Signal_type&quot;</span><span class="p">,</span> <span class="n">pst</span><span class="o">.</span><span class="n">IntegerType</span><span class="p">(),</span> <span class="kc">False</span><span class="p">),</span>
    <span class="n">pst</span><span class="o">.</span><span class="n">StructField</span><span class="p">(</span><span class="s2">&quot;array&quot;</span><span class="p">,</span> <span class="n">pst</span><span class="o">.</span><span class="n">ArrayType</span><span class="p">(</span><span class="n">pst</span><span class="o">.</span><span class="n">IntegerType</span><span class="p">()),</span> <span class="kc">False</span><span class="p">)])</span>

<span class="n">example_udf</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">udf</span><span class="p">(</span><span class="n">example</span><span class="p">,</span> <span class="n">schema</span><span class="p">)</span>

<span class="n">newDF</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">withColumn</span><span class="p">(</span><span class="s2">&quot;Output&quot;</span><span class="p">,</span> <span class="n">example_udf</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s2">&quot;Number&quot;</span><span class="p">]))</span>
<span class="n">newDF</span> <span class="o">=</span> <span class="n">newDF</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="s2">&quot;Name&quot;</span><span class="p">,</span> <span class="s2">&quot;Number&quot;</span><span class="p">,</span> <span class="s2">&quot;Output.*&quot;</span><span class="p">)</span>

<span class="n">newDF</span><span class="o">.</span><span class="n">show</span><span class="p">(</span><span class="n">truncate</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="o">+-----+------+-----------+------+</span>
<span class="o">|</span><span class="n">Name</span> <span class="o">|</span><span class="n">Number</span><span class="o">|</span><span class="n">signal_type</span><span class="o">|</span><span class="n">array</span> <span class="o">|</span>
<span class="o">+-----+------+-----------+------+</span>
<span class="o">|</span><span class="n">Alive</span><span class="o">|</span><span class="mi">4</span>     <span class="o">|</span><span class="mi">6</span>          <span class="o">|</span><span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">6</span><span class="p">]</span><span class="o">|</span>
<span class="o">+-----+------+-----------+------+</span>
</pre></div>
</div>
</div>
<div class="section" id="pandas-udf">
<h3>Pandas UDF<a class="headerlink" href="#pandas-udf" title="Permalink to this headline">¶</a></h3>
<p>UDF’s are slow… But there are now pandas_udf pyspark function, that is said to work faster, and convert a simple pandas function to pyspark:
<a class="reference external" href="https://databricks.com/blog/2017/10/30/introducing-vectorized-udfs-for-pyspark.html">https://databricks.com/blog/2017/10/30/introducing-vectorized-udfs-for-pyspark.html</a></p>
<p>Some example on arrays: <a class="reference external" href="https://stackoverflow.com/questions/54432794/pandas-udf-that-operates-on-arrays">https://stackoverflow.com/questions/54432794/pandas-udf-that-operates-on-arrays</a></p>
<p>Great and deep intro: <a class="reference external" href="https://florianwilhelm.info/2019/04/more_efficient_udfs_with_pyspark/">https://florianwilhelm.info/2019/04/more_efficient_udfs_with_pyspark/</a> (good explanation of the 3 output modes of the pandas UDF)</p>
<div class="figure align-default">
<a class="reference internal image-reference" href="../_images/Pandas_UDF_explanation1.PNG"><img alt="Pandas_UDF_explanation.PNG" src="../_images/Pandas_UDF_explanation1.PNG" style="width: 832.0px; height: 288.0px;" /></a>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">pyspark.sql.functions</span> <span class="kn">import</span> <span class="n">pandas_udf</span><span class="p">,</span><span class="n">PandasUDFType</span>
<span class="kn">from</span> <span class="nn">pyspark.sql.types</span> <span class="kn">import</span> <span class="o">*</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">([([</span><span class="mf">1.4343</span><span class="p">,</span><span class="mf">2.3434</span><span class="p">,</span><span class="mf">3.4545</span><span class="p">],</span><span class="s1">&#39;val1&#39;</span><span class="p">),([</span><span class="mf">4.5656</span><span class="p">,</span><span class="mf">5.1215</span><span class="p">,</span><span class="mf">6.5656</span><span class="p">],</span><span class="s1">&#39;val2&#39;</span><span class="p">)],[</span><span class="s1">&#39;col1&#39;</span><span class="p">,</span><span class="s1">&#39;col2&#39;</span><span class="p">])</span>
<span class="n">df</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="kn">from</span> <span class="nn">pyspark.sql.functions</span> <span class="kn">import</span> <span class="n">pandas_udf</span><span class="p">,</span><span class="n">PandasUDFType</span>
<span class="kn">from</span> <span class="nn">pyspark.sql.types</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>

<span class="nd">@pandas_udf</span><span class="p">(</span><span class="n">ArrayType</span><span class="p">(</span><span class="n">FloatType</span><span class="p">()),</span><span class="n">PandasUDFType</span><span class="o">.</span><span class="n">SCALAR</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">round_func</span><span class="p">(</span><span class="n">v</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">v</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span><span class="n">np</span><span class="o">.</span><span class="n">around</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">decimals</span><span class="o">=</span><span class="mi">2</span><span class="p">))</span>

<span class="n">df</span><span class="o">.</span><span class="n">withColumn</span><span class="p">(</span><span class="s1">&#39;col3&#39;</span><span class="p">,</span><span class="n">round_func</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">col1</span><span class="p">))</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="o">+--------------------+----+------------------+</span>
<span class="o">|</span>                <span class="n">col1</span><span class="o">|</span><span class="n">col2</span><span class="o">|</span>              <span class="n">col3</span><span class="o">|</span>
<span class="o">+--------------------+----+------------------+</span>
<span class="o">|</span><span class="p">[</span><span class="mf">1.4343</span><span class="p">,</span> <span class="mf">2.3434</span><span class="p">,</span> <span class="o">...|</span><span class="n">val1</span><span class="o">|</span><span class="p">[</span><span class="mf">1.43</span><span class="p">,</span> <span class="mf">2.34</span><span class="p">,</span> <span class="mf">3.45</span><span class="p">]</span><span class="o">|</span>
<span class="o">|</span><span class="p">[</span><span class="mf">4.5656</span><span class="p">,</span> <span class="mf">5.1215</span><span class="p">,</span> <span class="o">...|</span><span class="n">val2</span><span class="o">|</span><span class="p">[</span><span class="mf">4.57</span><span class="p">,</span> <span class="mf">5.12</span><span class="p">,</span> <span class="mf">6.57</span><span class="p">]</span><span class="o">|</span>
<span class="o">+--------------------+----+------------------+</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="machine-learning-using-the-mllib-package">
<h2>Machine Learning using the MLlib package<a class="headerlink" href="#machine-learning-using-the-mllib-package" title="Permalink to this headline">¶</a></h2>
<p>There are 2 main packages for Machine Learning in Pyspark. MLlib, which is based on RDDs, and ML, which is based on Dataframes. The distinction is very important! After version 2.0, RDDs are deprecated (removed in Spark 3.0) in profit of Pyspark dataframes, which are much more Pandas-friendly.</p>
<div class="section" id="the-random-forest">
<h3>The Random Forest<a class="headerlink" href="#the-random-forest" title="Permalink to this headline">¶</a></h3>
<p>The MLlib’s version of Random Forest is described in details here: <a class="reference external" href="https://spark.apache.org/docs/1.6.1/mllib-ensembles.html">https://spark.apache.org/docs/1.6.1/mllib-ensembles.html</a> .
Here is a very simple working code:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">pyspark.mllib.regression</span> <span class="kn">import</span> <span class="n">LabeledPoint</span>
<span class="kn">from</span> <span class="nn">pyspark.mllib.tree</span> <span class="kn">import</span> <span class="n">RandomForest</span>
<span class="kn">from</span> <span class="nn">pyspark.sql.functions</span> <span class="kn">import</span> <span class="o">*</span>

<span class="c1"># Building of some data for supervised ML: first column is label, second is feature</span>
<span class="n">data</span> <span class="o">=</span> <span class="p">[</span>
  <span class="n">LabeledPoint</span><span class="p">(</span><span class="mf">0.0</span><span class="p">,</span> <span class="p">[</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">]),</span>
  <span class="n">LabeledPoint</span><span class="p">(</span><span class="mf">0.0</span><span class="p">,</span> <span class="p">[</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">]),</span>
  <span class="n">LabeledPoint</span><span class="p">(</span><span class="mf">1.0</span><span class="p">,</span> <span class="p">[</span><span class="mf">2.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">]),</span>
  <span class="n">LabeledPoint</span><span class="p">(</span><span class="mf">1.0</span><span class="p">,</span> <span class="p">[</span><span class="mf">3.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">])]</span>

<span class="c1"># Creating RDD from data</span>
<span class="n">trainingData</span><span class="o">=</span><span class="n">sc</span><span class="o">.</span><span class="n">parallelize</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
<span class="n">trainingData</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="nb">print</span> <span class="n">trainingData</span><span class="o">.</span><span class="n">toDF</span><span class="p">()</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="o">+---------+-----+</span>
<span class="o">|</span> <span class="n">features</span><span class="o">|</span><span class="n">label</span><span class="o">|</span>
<span class="o">+---------+-----+</span>
<span class="o">|</span><span class="p">[</span><span class="mf">0.0</span><span class="p">,</span><span class="mf">0.0</span><span class="p">]</span><span class="o">|</span>  <span class="mf">0.0</span><span class="o">|</span>
<span class="o">|</span><span class="p">[</span><span class="mf">1.0</span><span class="p">,</span><span class="mf">1.0</span><span class="p">]</span><span class="o">|</span>  <span class="mf">0.0</span><span class="o">|</span>
<span class="o">|</span><span class="p">[</span><span class="mf">2.0</span><span class="p">,</span><span class="mf">2.0</span><span class="p">]</span><span class="o">|</span>  <span class="mf">1.0</span><span class="o">|</span>
<span class="o">|</span><span class="p">[</span><span class="mf">3.0</span><span class="p">,</span><span class="mf">2.0</span><span class="p">]</span><span class="o">|</span>  <span class="mf">1.0</span><span class="o">|</span>
<span class="o">+---------+-----+</span>

<span class="c1"># Model creation</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">RandomForest</span><span class="o">.</span><span class="n">trainClassifier</span><span class="p">(</span><span class="n">trainingData</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="p">{},</span> <span class="mi">3</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="nb">print</span> <span class="n">model</span><span class="o">.</span><span class="n">numTrees</span><span class="p">()</span>
<span class="nb">print</span> <span class="n">model</span><span class="o">.</span><span class="n">totalNumNodes</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">toDebugString</span><span class="p">())</span>

<span class="c1"># Predicting a new sample</span>
<span class="n">rdd</span> <span class="o">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">parallelize</span><span class="p">([[</span><span class="mf">3.0</span><span class="p">,</span><span class="mf">2.0</span><span class="p">]])</span>
<span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">rdd</span><span class="p">)</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
</pre></div>
</div>
<p>Here is another working example, on the IRIS dataset:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">pyspark.mllib.regression</span> <span class="kn">import</span> <span class="n">LabeledPoint</span>
<span class="kn">from</span> <span class="nn">pyspark.mllib.tree</span> <span class="kn">import</span> <span class="n">RandomForest</span>
<span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_iris</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">accuracy_score</span>
<span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">metrics</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="k">def</span> <span class="nf">Convert_to_LabelPoint_format</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">y</span><span class="p">):</span>
  <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">  This function is intended for the preparation of Supervised ML input data, using the MLlib package of Pyspark.</span>
<span class="sd">  Input:</span>
<span class="sd">  - X: a numpy array containing the features (as many columns as features)</span>
<span class="sd">  - y: a numpy array containing the labels (1 column)</span>
<span class="sd">  Output:</span>
<span class="sd">  - data: a python list containing the data in LabeledPoint format</span>
<span class="sd">  &#39;&#39;&#39;</span>
  <span class="n">data</span> <span class="o">=</span> <span class="p">[]</span>
  <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="nb">len</span><span class="p">(</span><span class="n">y</span><span class="p">)):</span>
      <span class="n">X_list</span> <span class="o">=</span>  <span class="nb">list</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">i</span><span class="p">,:])</span>
      <span class="n">data</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">LabeledPoint</span><span class="p">(</span><span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">],</span><span class="n">X_list</span><span class="p">))</span>
  <span class="k">return</span> <span class="n">data</span>

<span class="c1">#USING IRIS DATASET:</span>
<span class="n">iris</span> <span class="o">=</span> <span class="n">load_iris</span><span class="p">()</span>
<span class="n">idx</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">iris</span><span class="o">.</span><span class="n">target</span><span class="p">)))</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="n">idx</span><span class="p">)</span>              <span class="c1">#We shuffle it (important if we want to split in train and test sets)</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">iris</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">iris</span><span class="o">.</span><span class="n">target</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>

<span class="n">data</span> <span class="o">=</span> <span class="n">Convert_to_LabelPoint_format</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">y</span><span class="p">)</span>

<span class="c1"># Creating RDD from data</span>
<span class="n">data_rdd</span><span class="o">=</span><span class="n">sc</span><span class="o">.</span><span class="n">parallelize</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
<span class="n">data_rdd</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="nb">print</span> <span class="n">data_rdd</span><span class="o">.</span><span class="n">toDF</span><span class="p">()</span><span class="o">.</span><span class="n">show</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span>

<span class="o">+-----------------+-----+</span>
<span class="o">|</span>         <span class="n">features</span><span class="o">|</span><span class="n">label</span><span class="o">|</span>
<span class="o">+-----------------+-----+</span>
<span class="o">|</span><span class="p">[</span><span class="mf">5.1</span><span class="p">,</span><span class="mf">3.4</span><span class="p">,</span><span class="mf">1.5</span><span class="p">,</span><span class="mf">0.2</span><span class="p">]</span><span class="o">|</span>  <span class="mf">0.0</span><span class="o">|</span>
<span class="o">|</span><span class="p">[</span><span class="mf">6.7</span><span class="p">,</span><span class="mf">3.0</span><span class="p">,</span><span class="mf">5.2</span><span class="p">,</span><span class="mf">2.3</span><span class="p">]</span><span class="o">|</span>  <span class="mf">2.0</span><span class="o">|</span>
<span class="o">|</span><span class="p">[</span><span class="mf">5.0</span><span class="p">,</span><span class="mf">3.6</span><span class="p">,</span><span class="mf">1.4</span><span class="p">,</span><span class="mf">0.2</span><span class="p">]</span><span class="o">|</span>  <span class="mf">0.0</span><span class="o">|</span>
<span class="o">|</span><span class="p">[</span><span class="mf">6.8</span><span class="p">,</span><span class="mf">3.2</span><span class="p">,</span><span class="mf">5.9</span><span class="p">,</span><span class="mf">2.3</span><span class="p">]</span><span class="o">|</span>  <span class="mf">2.0</span><span class="o">|</span>
<span class="o">|</span><span class="p">[</span><span class="mf">6.1</span><span class="p">,</span><span class="mf">2.9</span><span class="p">,</span><span class="mf">4.7</span><span class="p">,</span><span class="mf">1.4</span><span class="p">]</span><span class="o">|</span>  <span class="mf">1.0</span><span class="o">|</span>
<span class="o">+-----------------+-----+</span>

<span class="c1">#Splitting the data in training and testing set</span>
<span class="p">(</span><span class="n">trainingData</span><span class="p">,</span> <span class="n">testData</span><span class="p">)</span> <span class="o">=</span> <span class="n">data_rdd</span><span class="o">.</span><span class="n">randomSplit</span><span class="p">([</span><span class="mf">0.7</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">])</span>

<span class="c1"># Model creation</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">RandomForest</span><span class="o">.</span><span class="n">trainClassifier</span><span class="p">(</span><span class="n">trainingData</span><span class="p">,</span> <span class="n">numClasses</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">categoricalFeaturesInfo</span><span class="o">=</span><span class="p">{},</span>
                                   <span class="n">numTrees</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">featureSubsetStrategy</span><span class="o">=</span><span class="s2">&quot;auto&quot;</span><span class="p">,</span>
                                   <span class="n">impurity</span><span class="o">=</span><span class="s1">&#39;gini&#39;</span><span class="p">,</span> <span class="n">maxDepth</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">maxBins</span><span class="o">=</span><span class="mi">32</span><span class="p">)</span>
<span class="nb">print</span> <span class="n">model</span><span class="o">.</span><span class="n">numTrees</span><span class="p">()</span>
<span class="nb">print</span> <span class="n">model</span><span class="o">.</span><span class="n">totalNumNodes</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">toDebugString</span><span class="p">())</span>
<span class="nb">print</span>

<span class="c1"># Predicting for test data</span>
<span class="n">y_pred</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">testData</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="o">.</span><span class="n">features</span><span class="p">))</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>

<span class="c1">#ACCURACY</span>
<span class="n">testData_pd</span> <span class="o">=</span> <span class="n">testData</span><span class="o">.</span><span class="n">toDF</span><span class="p">()</span><span class="o">.</span><span class="n">toPandas</span><span class="p">()</span>
<span class="n">y_test</span> <span class="o">=</span> <span class="n">testData_pd</span><span class="p">[</span><span class="s1">&#39;label&#39;</span><span class="p">]</span>
<span class="n">accuracy</span>    <span class="o">=</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span><span class="n">y_pred</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Accuracy = &quot;</span><span class="p">,</span> <span class="n">accuracy</span><span class="p">)</span>
</pre></div>
</div>
<p>(‘Accuracy = ‘, 0.9772)</p>
</div>
<div class="section" id="kernel-density-estimation-here-1-d-only">
<h3>Kernel Density Estimation (here 1-D only)<a class="headerlink" href="#kernel-density-estimation-here-1-d-only" title="Permalink to this headline">¶</a></h3>
<p>Here we still use the old mllib package. Look for the same using the ml one.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">pyspark.mllib.stat</span> <span class="kn">import</span> <span class="n">KernelDensity</span>
<span class="kn">from</span> <span class="nn">pyspark.mllib.random</span> <span class="kn">import</span> <span class="n">RandomRDDs</span>
<span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>

<span class="c1"># Create the estimator</span>
<span class="n">kd</span> <span class="o">=</span> <span class="n">KernelDensity</span><span class="p">()</span>
<span class="c1"># Choose the range to evaluate density on</span>
<span class="n">ran</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="o">-</span><span class="mi">100</span><span class="p">,</span><span class="mi">100</span><span class="p">,</span><span class="mf">0.1</span><span class="p">);</span>
<span class="c1"># Set kernel bandwidth</span>
<span class="n">kd</span><span class="o">.</span><span class="n">setBandwidth</span><span class="p">(</span><span class="mf">3.0</span><span class="p">)</span>

<span class="c1">#Here with a random normal sample</span>
<span class="n">u</span> <span class="o">=</span> <span class="n">RandomRDDs</span><span class="o">.</span><span class="n">normalRDD</span><span class="p">(</span><span class="n">sc</span><span class="p">,</span> <span class="mi">10000</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
<span class="n">kd</span><span class="o">.</span><span class="n">setSample</span><span class="p">(</span><span class="n">u</span><span class="p">)</span>
<span class="c1">#plot of the histogram</span>
<span class="n">num_bins</span> <span class="o">=</span> <span class="mi">50</span>
<span class="n">n</span><span class="p">,</span> <span class="n">bins</span><span class="p">,</span> <span class="n">patches</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">u</span><span class="o">.</span><span class="n">collect</span><span class="p">(),</span> <span class="n">num_bins</span><span class="p">,</span> <span class="n">normed</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="c1">#plot of the kde</span>
<span class="n">densities</span> <span class="o">=</span> <span class="n">kd</span><span class="o">.</span><span class="n">estimate</span><span class="p">(</span><span class="n">ran</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">ran</span><span class="p">,</span><span class="n">densities</span><span class="p">)</span>

<span class="c1">#Here with a true variable: the age (takes some time to compile)</span>
<span class="n">sql</span> <span class="o">=</span> <span class="s2">&quot;&quot;&quot;</span>
<span class="s2">select * from risk_temp.TULE_TP5</span>
<span class="s2">&quot;&quot;&quot;</span>
<span class="n">p1</span> <span class="o">=</span> <span class="n">sqlContext</span><span class="o">.</span><span class="n">sql</span><span class="p">(</span><span class="n">sql</span><span class="p">)</span>
<span class="n">p2</span> <span class="o">=</span> <span class="n">p1</span><span class="o">.</span><span class="n">filter</span><span class="p">(</span><span class="n">p1</span><span class="o">.</span><span class="n">statusondate</span> <span class="o">==</span> <span class="s1">&#39;2014-01-01&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">cache</span><span class="p">()</span>
<span class="n">p_age</span> <span class="o">=</span> <span class="n">p2</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">y</span><span class="p">:</span> <span class="n">y</span><span class="p">[</span><span class="s1">&#39;ac_age_cust_01&#39;</span><span class="p">])</span>
<span class="n">kd</span><span class="o">.</span><span class="n">setSample</span><span class="p">(</span><span class="n">p_age</span><span class="p">)</span>
<span class="c1"># Choose the range to evaluate density on</span>
<span class="n">ran</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">100</span><span class="p">,</span><span class="mf">0.1</span><span class="p">);</span>  <span class="c1">#We evaluate the KDE in the age range [0,100]</span>
<span class="n">densities</span> <span class="o">=</span> <span class="n">kd</span><span class="o">.</span><span class="n">estimate</span><span class="p">(</span><span class="n">ran</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">ran</span><span class="p">,</span><span class="n">densities</span><span class="p">);</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="machine-learning-using-the-ml-package">
<h2>Machine Learning using the ML package<a class="headerlink" href="#machine-learning-using-the-ml-package" title="Permalink to this headline">¶</a></h2>
<p>For the machine-learning package, look at:</p>
<ul class="simple">
<li><p>Version 1.6: <a class="reference external" href="https://spark.apache.org/docs/1.6.0/ml-guide.html">https://spark.apache.org/docs/1.6.0/ml-guide.html</a></p></li>
<li><p>Version 2.1: <a class="reference external" href="https://spark.apache.org/docs/2.1.0/ml-guide.html">https://spark.apache.org/docs/2.1.0/ml-guide.html</a></p></li>
</ul>
<div class="section" id="convert-a-string-column-to-indexed-labels">
<h3>Convert a string column to indexed labels<a class="headerlink" href="#convert-a-string-column-to-indexed-labels" title="Permalink to this headline">¶</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">pyspark.ml.feature</span> <span class="kn">import</span> <span class="n">StringIndexer</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">sqlContext</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">(</span>
  <span class="p">[(</span><span class="mi">0</span><span class="p">,</span> <span class="s2">&quot;a&quot;</span><span class="p">),</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="s2">&quot;b&quot;</span><span class="p">),</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="s2">&quot;c&quot;</span><span class="p">),</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="s2">&quot;a&quot;</span><span class="p">),</span> <span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="s2">&quot;a&quot;</span><span class="p">),</span> <span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="s2">&quot;c&quot;</span><span class="p">)],</span>
  <span class="p">[</span><span class="s2">&quot;id&quot;</span><span class="p">,</span> <span class="s2">&quot;category&quot;</span><span class="p">])</span>
<span class="n">df</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="o">+---+--------+</span>
<span class="o">|</span> <span class="nb">id</span><span class="o">|</span><span class="n">category</span><span class="o">|</span>
<span class="o">+---+--------+</span>
<span class="o">|</span>  <span class="mi">0</span><span class="o">|</span>       <span class="n">a</span><span class="o">|</span>
<span class="o">|</span>  <span class="mi">1</span><span class="o">|</span>       <span class="n">b</span><span class="o">|</span>
<span class="o">|</span>  <span class="mi">2</span><span class="o">|</span>       <span class="n">c</span><span class="o">|</span>
<span class="o">|</span>  <span class="mi">3</span><span class="o">|</span>       <span class="n">a</span><span class="o">|</span>
<span class="o">|</span>  <span class="mi">4</span><span class="o">|</span>       <span class="n">a</span><span class="o">|</span>
<span class="o">|</span>  <span class="mi">5</span><span class="o">|</span>       <span class="n">c</span><span class="o">|</span>
<span class="o">+---+--------+</span>

<span class="n">indexer</span> <span class="o">=</span> <span class="n">StringIndexer</span><span class="p">(</span><span class="n">inputCol</span><span class="o">=</span><span class="s2">&quot;category&quot;</span><span class="p">,</span> <span class="n">outputCol</span><span class="o">=</span><span class="s2">&quot;categoryIndex&quot;</span><span class="p">)</span>
<span class="n">indexed</span> <span class="o">=</span> <span class="n">indexer</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">df</span><span class="p">)</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">df</span><span class="p">)</span>
<span class="n">indexed</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="o">+---+--------+-------------+</span>
<span class="o">|</span> <span class="nb">id</span><span class="o">|</span><span class="n">category</span><span class="o">|</span><span class="n">categoryIndex</span><span class="o">|</span>
<span class="o">+---+--------+-------------+</span>
<span class="o">|</span>  <span class="mi">0</span><span class="o">|</span>       <span class="n">a</span><span class="o">|</span>          <span class="mf">0.0</span><span class="o">|</span>
<span class="o">|</span>  <span class="mi">1</span><span class="o">|</span>       <span class="n">b</span><span class="o">|</span>          <span class="mf">2.0</span><span class="o">|</span>
<span class="o">|</span>  <span class="mi">2</span><span class="o">|</span>       <span class="n">c</span><span class="o">|</span>          <span class="mf">1.0</span><span class="o">|</span>
<span class="o">|</span>  <span class="mi">3</span><span class="o">|</span>       <span class="n">a</span><span class="o">|</span>          <span class="mf">0.0</span><span class="o">|</span>
<span class="o">|</span>  <span class="mi">4</span><span class="o">|</span>       <span class="n">a</span><span class="o">|</span>          <span class="mf">0.0</span><span class="o">|</span>
<span class="o">|</span>  <span class="mi">5</span><span class="o">|</span>       <span class="n">c</span><span class="o">|</span>          <span class="mf">1.0</span><span class="o">|</span>
<span class="o">+---+--------+-------------+</span>
</pre></div>
</div>
</div>
<div class="section" id="assembling-multiple-columns-in-one-features-columns-using-vectorassembler">
<h3>Assembling multiple columns in one “features” columns using VectorAssembler<a class="headerlink" href="#assembling-multiple-columns-in-one-features-columns-using-vectorassembler" title="Permalink to this headline">¶</a></h3>
<p>VectorAssembler is a transformer that combines a given list of columns into a single vector column. It is useful for supervised ML tools like Random Forest.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">pyspark.ml.linalg</span> <span class="kn">import</span> <span class="n">Vectors</span>
<span class="kn">from</span> <span class="nn">pyspark.ml.feature</span> <span class="kn">import</span> <span class="n">VectorAssembler</span>

<span class="n">dataset</span> <span class="o">=</span> <span class="n">sqlContext</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">(</span>
  <span class="p">[(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">18</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="n">Vectors</span><span class="o">.</span><span class="n">dense</span><span class="p">([</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">10.0</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">]),</span> <span class="mf">1.0</span><span class="p">)],</span>
  <span class="p">[</span><span class="s2">&quot;id&quot;</span><span class="p">,</span> <span class="s2">&quot;hour&quot;</span><span class="p">,</span> <span class="s2">&quot;mobile&quot;</span><span class="p">,</span> <span class="s2">&quot;userFeatures&quot;</span><span class="p">,</span> <span class="s2">&quot;clicked&quot;</span><span class="p">])</span>
<span class="n">dataset</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="o">+---+----+------+--------------+-------+</span>
<span class="o">|</span> <span class="nb">id</span><span class="o">|</span><span class="n">hour</span><span class="o">|</span><span class="n">mobile</span><span class="o">|</span>  <span class="n">userFeatures</span><span class="o">|</span><span class="n">clicked</span><span class="o">|</span>
<span class="o">+---+----+------+--------------+-------+</span>
<span class="o">|</span>  <span class="mi">0</span><span class="o">|</span>  <span class="mi">18</span><span class="o">|</span>   <span class="mf">1.0</span><span class="o">|</span><span class="p">[</span><span class="mf">0.0</span><span class="p">,</span><span class="mf">10.0</span><span class="p">,</span><span class="mf">0.5</span><span class="p">]</span><span class="o">|</span>    <span class="mf">1.0</span><span class="o">|</span>
<span class="o">+---+----+------+--------------+-------+</span>

<span class="n">assembler</span> <span class="o">=</span> <span class="n">VectorAssembler</span><span class="p">(</span>
  <span class="n">inputCols</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;hour&quot;</span><span class="p">,</span> <span class="s2">&quot;mobile&quot;</span><span class="p">,</span> <span class="s2">&quot;userFeatures&quot;</span><span class="p">],</span>
  <span class="n">outputCol</span><span class="o">=</span><span class="s2">&quot;features&quot;</span><span class="p">)</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">assembler</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">dataset</span><span class="p">)</span>
<span class="n">output</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="o">+---+----+------+--------------+-------+-----------------------+</span>
<span class="o">|</span> <span class="nb">id</span><span class="o">|</span><span class="n">hour</span><span class="o">|</span><span class="n">mobile</span><span class="o">|</span>  <span class="n">userFeatures</span><span class="o">|</span><span class="n">clicked</span><span class="o">|</span>               <span class="n">features</span><span class="o">|</span>
<span class="o">+---+----+------+--------------+-------+-----------------------+</span>
<span class="o">|</span>  <span class="mi">0</span><span class="o">|</span>  <span class="mi">18</span><span class="o">|</span>   <span class="mf">1.0</span><span class="o">|</span><span class="p">[</span><span class="mf">0.0</span><span class="p">,</span><span class="mf">10.0</span><span class="p">,</span><span class="mf">0.5</span><span class="p">]</span><span class="o">|</span>    <span class="mf">1.0</span><span class="o">|</span><span class="p">[</span><span class="mf">18.0</span><span class="p">,</span><span class="mf">1.0</span><span class="p">,</span><span class="mf">0.0</span><span class="p">,</span><span class="mf">10.0</span><span class="p">,</span><span class="mf">0.5</span><span class="p">]</span><span class="o">|</span>
<span class="o">+---+----+------+--------------+-------+-----------------------+</span>
</pre></div>
</div>
</div>
<div class="section" id="one-hot-encoding">
<h3>One-Hot encoding<a class="headerlink" href="#one-hot-encoding" title="Permalink to this headline">¶</a></h3>
<p>One-hot encoding (<a class="reference external" href="https://en.wikipedia.org/wiki/One-hot">https://en.wikipedia.org/wiki/One-hot</a>) maps a column of label indices to a column of binary vectors, with at most a single one-value. See also <a class="reference external" href="https://spark.apache.org/docs/2.1.0/ml-features.html#onehotencoder">https://spark.apache.org/docs/2.1.0/ml-features.html#onehotencoder</a>
In this example, we have a dataframe with 5 different values (a-f) and we one-hot encode that column. It is a two-steps task: first need to string-index it, then encode it.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">pyspark.ml.feature</span> <span class="kn">import</span> <span class="n">OneHotEncoder</span><span class="p">,</span> <span class="n">StringIndexer</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">([</span>
  <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="s2">&quot;a&quot;</span><span class="p">),</span>
  <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="s2">&quot;b&quot;</span><span class="p">),</span>
  <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="s2">&quot;c&quot;</span><span class="p">),</span>
  <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="s2">&quot;a&quot;</span><span class="p">),</span>
  <span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="s2">&quot;a&quot;</span><span class="p">),</span>
  <span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="s2">&quot;c&quot;</span><span class="p">),</span>
  <span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="s2">&quot;b&quot;</span><span class="p">),</span>
  <span class="p">(</span><span class="mi">7</span><span class="p">,</span> <span class="s2">&quot;d&quot;</span><span class="p">),</span>
  <span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="s2">&quot;d&quot;</span><span class="p">),</span>
  <span class="p">(</span><span class="mi">9</span><span class="p">,</span> <span class="s2">&quot;e&quot;</span><span class="p">),</span>
  <span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="s2">&quot;e&quot;</span><span class="p">),</span>
  <span class="p">(</span><span class="mi">11</span><span class="p">,</span> <span class="s2">&quot;f&quot;</span><span class="p">),</span>
<span class="p">],</span> <span class="p">[</span><span class="s2">&quot;id&quot;</span><span class="p">,</span> <span class="s2">&quot;category&quot;</span><span class="p">])</span>

<span class="n">stringIndexer</span> <span class="o">=</span> <span class="n">StringIndexer</span><span class="p">(</span><span class="n">inputCol</span><span class="o">=</span><span class="s2">&quot;category&quot;</span><span class="p">,</span> <span class="n">outputCol</span><span class="o">=</span><span class="s2">&quot;categoryIndex&quot;</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">stringIndexer</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">df</span><span class="p">)</span>
<span class="n">indexed</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">df</span><span class="p">)</span>

<span class="n">encoder</span> <span class="o">=</span> <span class="n">OneHotEncoder</span><span class="p">(</span><span class="n">inputCol</span><span class="o">=</span><span class="s2">&quot;categoryIndex&quot;</span><span class="p">,</span> <span class="n">outputCol</span><span class="o">=</span><span class="s2">&quot;categoryVec&quot;</span><span class="p">)</span>
<span class="n">encoded</span> <span class="o">=</span> <span class="n">encoder</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">indexed</span><span class="p">)</span>
<span class="n">encoded</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="o">+---+--------+-------------+-------------+</span>
<span class="o">|</span> <span class="nb">id</span><span class="o">|</span><span class="n">category</span><span class="o">|</span><span class="n">categoryIndex</span><span class="o">|</span>  <span class="n">categoryVec</span><span class="o">|</span>
<span class="o">+---+--------+-------------+-------------+</span>
<span class="o">|</span>  <span class="mi">0</span><span class="o">|</span>       <span class="n">a</span><span class="o">|</span>          <span class="mf">0.0</span><span class="o">|</span><span class="p">(</span><span class="mi">5</span><span class="p">,[</span><span class="mi">0</span><span class="p">],[</span><span class="mf">1.0</span><span class="p">])</span><span class="o">|</span>
<span class="o">|</span>  <span class="mi">1</span><span class="o">|</span>       <span class="n">b</span><span class="o">|</span>          <span class="mf">2.0</span><span class="o">|</span><span class="p">(</span><span class="mi">5</span><span class="p">,[</span><span class="mi">2</span><span class="p">],[</span><span class="mf">1.0</span><span class="p">])</span><span class="o">|</span>
<span class="o">|</span>  <span class="mi">2</span><span class="o">|</span>       <span class="n">c</span><span class="o">|</span>          <span class="mf">3.0</span><span class="o">|</span><span class="p">(</span><span class="mi">5</span><span class="p">,[</span><span class="mi">3</span><span class="p">],[</span><span class="mf">1.0</span><span class="p">])</span><span class="o">|</span>
<span class="o">|</span>  <span class="mi">3</span><span class="o">|</span>       <span class="n">a</span><span class="o">|</span>          <span class="mf">0.0</span><span class="o">|</span><span class="p">(</span><span class="mi">5</span><span class="p">,[</span><span class="mi">0</span><span class="p">],[</span><span class="mf">1.0</span><span class="p">])</span><span class="o">|</span>
<span class="o">|</span>  <span class="mi">4</span><span class="o">|</span>       <span class="n">a</span><span class="o">|</span>          <span class="mf">0.0</span><span class="o">|</span><span class="p">(</span><span class="mi">5</span><span class="p">,[</span><span class="mi">0</span><span class="p">],[</span><span class="mf">1.0</span><span class="p">])</span><span class="o">|</span>
<span class="o">|</span>  <span class="mi">5</span><span class="o">|</span>       <span class="n">c</span><span class="o">|</span>          <span class="mf">3.0</span><span class="o">|</span><span class="p">(</span><span class="mi">5</span><span class="p">,[</span><span class="mi">3</span><span class="p">],[</span><span class="mf">1.0</span><span class="p">])</span><span class="o">|</span>
<span class="o">|</span>  <span class="mi">6</span><span class="o">|</span>       <span class="n">b</span><span class="o">|</span>          <span class="mf">2.0</span><span class="o">|</span><span class="p">(</span><span class="mi">5</span><span class="p">,[</span><span class="mi">2</span><span class="p">],[</span><span class="mf">1.0</span><span class="p">])</span><span class="o">|</span>
<span class="o">|</span>  <span class="mi">7</span><span class="o">|</span>       <span class="n">d</span><span class="o">|</span>          <span class="mf">4.0</span><span class="o">|</span><span class="p">(</span><span class="mi">5</span><span class="p">,[</span><span class="mi">4</span><span class="p">],[</span><span class="mf">1.0</span><span class="p">])</span><span class="o">|</span>
<span class="o">|</span>  <span class="mi">8</span><span class="o">|</span>       <span class="n">d</span><span class="o">|</span>          <span class="mf">4.0</span><span class="o">|</span><span class="p">(</span><span class="mi">5</span><span class="p">,[</span><span class="mi">4</span><span class="p">],[</span><span class="mf">1.0</span><span class="p">])</span><span class="o">|</span>
<span class="o">|</span>  <span class="mi">9</span><span class="o">|</span>       <span class="n">e</span><span class="o">|</span>          <span class="mf">1.0</span><span class="o">|</span><span class="p">(</span><span class="mi">5</span><span class="p">,[</span><span class="mi">1</span><span class="p">],[</span><span class="mf">1.0</span><span class="p">])</span><span class="o">|</span>
<span class="o">|</span> <span class="mi">10</span><span class="o">|</span>       <span class="n">e</span><span class="o">|</span>          <span class="mf">1.0</span><span class="o">|</span><span class="p">(</span><span class="mi">5</span><span class="p">,[</span><span class="mi">1</span><span class="p">],[</span><span class="mf">1.0</span><span class="p">])</span><span class="o">|</span>
<span class="o">|</span> <span class="mi">11</span><span class="o">|</span>       <span class="n">f</span><span class="o">|</span>          <span class="mf">5.0</span><span class="o">|</span>    <span class="p">(</span><span class="mi">5</span><span class="p">,[],[])</span><span class="o">|</span>
<span class="o">+---+--------+-------------+-------------+</span>
</pre></div>
</div>
</div>
<div class="section" id="grouping-data-using-bucketizer">
<h3>Grouping data using Bucketizer<a class="headerlink" href="#grouping-data-using-bucketizer" title="Permalink to this headline">¶</a></h3>
<p>Bucketizer is a transformer that groups a given column using splits defined by the user. In this example, we group a few days of the month in 3 groups: [1-10], [11-20], [21-31]</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">pyspark.ml.feature</span> <span class="kn">import</span> <span class="n">Bucketizer</span>

<span class="c1">#Let&#39;s create a sample with some days of the month</span>
<span class="n">data</span> <span class="o">=</span> <span class="p">[(</span><span class="mi">1</span><span class="p">,),</span> <span class="p">(</span><span class="mi">7</span><span class="p">,),</span> <span class="p">(</span><span class="mi">11</span><span class="p">,),</span> <span class="p">(</span><span class="mi">20</span><span class="p">,),</span> <span class="p">(</span><span class="mi">27</span><span class="p">,),</span> <span class="p">(</span><span class="mi">31</span><span class="p">,)]</span>
<span class="n">dataFrame</span> <span class="o">=</span> <span class="n">sqlContext</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="p">[</span><span class="s2">&quot;day&quot;</span><span class="p">])</span>
<span class="n">dataFrame</span> <span class="o">=</span> <span class="n">dataFrame</span><span class="o">.</span><span class="n">withColumn</span><span class="p">(</span><span class="s2">&quot;day&quot;</span><span class="p">,</span> <span class="n">dataFrame</span><span class="p">[</span><span class="s2">&quot;day&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="s2">&quot;double&quot;</span><span class="p">))</span> <span class="c1">#we need to transform them in double</span>

<span class="c1">#We want to groupe these days in 3 groups [1-10], [11-20], [21-31]</span>
<span class="n">splits</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">11</span><span class="p">,</span> <span class="mi">21</span><span class="p">,</span> <span class="mi">31</span><span class="p">]</span>
<span class="n">bucketizer</span> <span class="o">=</span> <span class="n">Bucketizer</span><span class="p">(</span><span class="n">splits</span><span class="o">=</span><span class="n">splits</span><span class="p">,</span> <span class="n">inputCol</span><span class="o">=</span><span class="s2">&quot;day&quot;</span><span class="p">,</span> <span class="n">outputCol</span><span class="o">=</span><span class="s2">&quot;dayGrouped&quot;</span><span class="p">)</span>

<span class="c1"># Transform original data into its bucket index.</span>
<span class="n">bucketedData</span> <span class="o">=</span> <span class="n">bucketizer</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">dataFrame</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Bucketizer output with </span><span class="si">%d</span><span class="s2"> buckets&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">bucketizer</span><span class="o">.</span><span class="n">getSplits</span><span class="p">())</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span>
<span class="n">bucketedData</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="c1">#Bucketizer output with 3 buckets</span>
<span class="o">+----+----------+</span>
<span class="o">|</span> <span class="n">day</span><span class="o">|</span><span class="n">dayGrouped</span><span class="o">|</span>
<span class="o">+----+----------+</span>
<span class="o">|</span> <span class="mf">1.0</span><span class="o">|</span>       <span class="mf">0.0</span><span class="o">|</span>
<span class="o">|</span> <span class="mf">7.0</span><span class="o">|</span>       <span class="mf">0.0</span><span class="o">|</span>
<span class="o">|</span><span class="mf">11.0</span><span class="o">|</span>       <span class="mf">1.0</span><span class="o">|</span>
<span class="o">|</span><span class="mf">20.0</span><span class="o">|</span>       <span class="mf">1.0</span><span class="o">|</span>
<span class="o">|</span><span class="mf">27.0</span><span class="o">|</span>       <span class="mf">2.0</span><span class="o">|</span>
<span class="o">|</span><span class="mf">31.0</span><span class="o">|</span>       <span class="mf">2.0</span><span class="o">|</span>
<span class="o">+----+----------+</span>
</pre></div>
</div>
</div>
<div class="section" id="id1">
<h3>The Random Forest<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h3>
<p>Here is a working example of the Random Forest using the ML package, applied on the IRIS dataset (so, Multi-class target!):</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">pyspark.ml</span> <span class="kn">import</span> <span class="n">Pipeline</span>
<span class="kn">from</span> <span class="nn">pyspark.ml.classification</span> <span class="kn">import</span> <span class="n">RandomForestClassifier</span>
<span class="kn">from</span> <span class="nn">pyspark.ml.feature</span> <span class="kn">import</span> <span class="n">VectorAssembler</span><span class="p">,</span> <span class="n">StringIndexer</span><span class="p">,</span> <span class="n">VectorIndexer</span>
<span class="kn">from</span> <span class="nn">pyspark.ml.evaluation</span> <span class="kn">import</span> <span class="n">MulticlassClassificationEvaluator</span>
<span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_iris</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="c1">#USING IRIS DATASET:</span>
<span class="n">iris</span> <span class="o">=</span> <span class="n">load_iris</span><span class="p">()</span>                       <span class="c1">#The Iris dataset is available through the scikit-learn API</span>
<span class="n">idx</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">iris</span><span class="o">.</span><span class="n">target</span><span class="p">)))</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="n">idx</span><span class="p">)</span>              <span class="c1">#We shuffle it (important if we want to split in train and test sets)</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">iris</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">iris</span><span class="o">.</span><span class="n">target</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>

<span class="c1"># Load data in Pandas dataFrame and then in a Pyspark dataframe</span>
<span class="n">data_pd</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">column_stack</span><span class="p">((</span><span class="n">X</span><span class="p">,</span><span class="n">y</span><span class="p">)),</span> <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;sepal_length&#39;</span><span class="p">,</span> <span class="s1">&#39;sepal_width&#39;</span><span class="p">,</span> <span class="s1">&#39;petal_length&#39;</span><span class="p">,</span> <span class="s1">&#39;petal_width&#39;</span><span class="p">,</span> <span class="s1">&#39;label&#39;</span><span class="p">])</span>
<span class="n">data_df</span> <span class="o">=</span> <span class="n">sqlContext</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">(</span><span class="n">data_pd</span><span class="p">)</span>

<span class="c1"># This transforms the labels into indexes. See https://spark.apache.org/docs/latest/ml-features.html#stringindexer</span>
<span class="n">labelIndexer</span> <span class="o">=</span> <span class="n">StringIndexer</span><span class="p">(</span><span class="n">inputCol</span><span class="o">=</span><span class="s2">&quot;label&quot;</span><span class="p">,</span> <span class="n">outputCol</span><span class="o">=</span><span class="s2">&quot;indexedLabel&quot;</span><span class="p">)</span>

<span class="c1"># This groups all the features in one pack &quot;features&quot;, needed for the VectorIndexer</span>
<span class="n">vectorizer</span> <span class="o">=</span> <span class="n">VectorAssembler</span><span class="p">(</span><span class="n">inputCols</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;sepal_length&#39;</span><span class="p">,</span> <span class="s1">&#39;sepal_width&#39;</span><span class="p">,</span> <span class="s1">&#39;petal_length&#39;</span><span class="p">,</span> <span class="s1">&#39;petal_width&#39;</span><span class="p">],</span><span class="n">outputCol</span> <span class="o">=</span> <span class="s2">&quot;features&quot;</span><span class="p">)</span>

<span class="c1"># This identifies categorical features, and indexes them. Set maxCategories so features with &gt; 4 distinct values are treated as continuous. #https://spark.apache.org/docs/latest/ml-features.html#stringindexer</span>
<span class="n">featureIndexer</span> <span class="o">=</span> <span class="n">VectorIndexer</span><span class="p">(</span><span class="n">inputCol</span><span class="o">=</span><span class="s2">&quot;features&quot;</span><span class="p">,</span> <span class="n">outputCol</span><span class="o">=</span><span class="s2">&quot;indexedFeatures&quot;</span><span class="p">,</span> <span class="n">maxCategories</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>

<span class="c1"># Split the data into training and test sets (30% held out for testing)</span>
<span class="p">(</span><span class="n">trainingData</span><span class="p">,</span> <span class="n">testData</span><span class="p">)</span> <span class="o">=</span> <span class="n">data_df</span><span class="o">.</span><span class="n">randomSplit</span><span class="p">([</span><span class="mf">0.7</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">])</span>

<span class="c1"># Train a RandomForest model.</span>
<span class="n">rf</span> <span class="o">=</span> <span class="n">RandomForestClassifier</span><span class="p">(</span><span class="n">labelCol</span><span class="o">=</span><span class="s2">&quot;indexedLabel&quot;</span><span class="p">,</span> <span class="n">featuresCol</span><span class="o">=</span><span class="s2">&quot;indexedFeatures&quot;</span><span class="p">,</span> <span class="n">numTrees</span><span class="o">=</span><span class="mi">12</span><span class="p">,</span>  <span class="n">maxDepth</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>

<span class="c1"># Chain indexers and forest in a Pipeline</span>
<span class="n">pipeline</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">(</span><span class="n">stages</span><span class="o">=</span><span class="p">[</span><span class="n">labelIndexer</span><span class="p">,</span> <span class="n">vectorizer</span><span class="p">,</span> <span class="n">featureIndexer</span><span class="p">,</span> <span class="n">rf</span><span class="p">])</span>

<span class="c1"># Train model.  This also runs the indexers.</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">pipeline</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">trainingData</span><span class="p">)</span>

<span class="c1"># Make predictions.</span>
<span class="n">predictions</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">testData</span><span class="p">)</span>

<span class="c1"># Select example rows to display.</span>
<span class="n">predictions</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="s2">&quot;prediction&quot;</span><span class="p">,</span> <span class="s2">&quot;indexedLabel&quot;</span><span class="p">,</span> <span class="s2">&quot;features&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">show</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span>

<span class="c1"># Select (prediction, true label) and compute test error</span>
<span class="n">evaluator</span> <span class="o">=</span> <span class="n">MulticlassClassificationEvaluator</span><span class="p">(</span><span class="n">labelCol</span><span class="o">=</span><span class="s2">&quot;indexedLabel&quot;</span><span class="p">,</span> <span class="n">predictionCol</span><span class="o">=</span><span class="s2">&quot;prediction&quot;</span><span class="p">,</span> <span class="n">metricName</span><span class="o">=</span><span class="s2">&quot;accuracy&quot;</span><span class="p">)</span>
<span class="n">accuracy</span> <span class="o">=</span> <span class="n">evaluator</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">predictions</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Accuracy = </span><span class="si">%g</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">accuracy</span><span class="p">))</span>
</pre></div>
</div>
<p>Accuracy = 0.973684.</p>
</div>
<div class="section" id="evaluation-of-accuracy">
<h3>Evaluation of accuracy<a class="headerlink" href="#evaluation-of-accuracy" title="Permalink to this headline">¶</a></h3>
<p>For binary (2-classes) target, we can use the area under the ROC curve (AUC):</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">pyspark.ml.linalg</span> <span class="kn">import</span> <span class="n">Vectors</span>
<span class="kn">from</span> <span class="nn">pyspark.ml.evaluation</span> <span class="kn">import</span> <span class="n">BinaryClassificationEvaluator</span>
<span class="n">scoreAndLabels</span> <span class="o">=</span> <span class="nb">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="p">(</span><span class="n">Vectors</span><span class="o">.</span><span class="n">dense</span><span class="p">([</span><span class="mf">1.0</span> <span class="o">-</span> <span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]]),</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]),</span> <span class="p">[(</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">),</span> <span class="p">(</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">),</span> <span class="p">(</span><span class="mf">0.4</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">),</span> <span class="p">(</span><span class="mf">0.6</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">),</span> <span class="p">(</span><span class="mf">0.6</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">),</span> <span class="p">(</span><span class="mf">0.6</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">),</span> <span class="p">(</span><span class="mf">0.8</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">)])</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">sqlContext</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">(</span><span class="n">scoreAndLabels</span><span class="p">,</span> <span class="p">[</span><span class="s2">&quot;raw&quot;</span><span class="p">,</span> <span class="s2">&quot;trueLabel&quot;</span><span class="p">])</span>
<span class="n">dataset</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="o">+---------+---------+</span>
<span class="o">|</span>      <span class="n">raw</span><span class="o">|</span><span class="n">trueLabel</span><span class="o">|</span>   <span class="c1">#left column contains probabilities, right contains the true label (not the derived ones)</span>
<span class="o">+---------+---------+</span>
<span class="o">|</span><span class="p">[</span><span class="mf">0.9</span><span class="p">,</span><span class="mf">0.1</span><span class="p">]</span><span class="o">|</span>      <span class="mf">0.0</span><span class="o">|</span>
<span class="o">|</span><span class="p">[</span><span class="mf">0.9</span><span class="p">,</span><span class="mf">0.1</span><span class="p">]</span><span class="o">|</span>      <span class="mf">1.0</span><span class="o">|</span>
<span class="o">|</span><span class="p">[</span><span class="mf">0.6</span><span class="p">,</span><span class="mf">0.4</span><span class="p">]</span><span class="o">|</span>      <span class="mf">0.0</span><span class="o">|</span>
<span class="o">+---------+---------+</span>

<span class="n">evaluator</span> <span class="o">=</span> <span class="n">BinaryClassificationEvaluator</span><span class="p">(</span><span class="n">rawPredictionCol</span><span class="o">=</span><span class="s2">&quot;raw&quot;</span><span class="p">,</span> <span class="n">labelCol</span><span class="o">=</span><span class="s2">&quot;trueLabel&quot;</span><span class="p">,</span> <span class="n">metricName</span><span class="o">=</span><span class="s2">&quot;areaUnderROC&quot;</span><span class="p">)</span>
<span class="nb">print</span> <span class="n">evaluator</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">dataset</span><span class="p">)</span>

<span class="mf">0.708</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="parallelization-of-scikit-learn-into-spark">
<h2>Parallelization of scikit-learn into spark<a class="headerlink" href="#parallelization-of-scikit-learn-into-spark" title="Permalink to this headline">¶</a></h2>
<p>Example using spark_sklearn:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://databricks.com/blog/2016/02/08/auto-scaling-scikit-learn-with-apache-spark.html">https://databricks.com/blog/2016/02/08/auto-scaling-scikit-learn-with-apache-spark.html</a></p></li>
<li><p><a class="reference external" href="https://quickbooks-engineering.intuit.com/operationalizing-scikit-learn-machine-learning-model-under-apache-spark-b009fb6b6c45">https://quickbooks-engineering.intuit.com/operationalizing-scikit-learn-machine-learning-model-under-apache-spark-b009fb6b6c45</a></p></li>
<li><p><a class="reference external" href="https://mapr.com/blog/predicting-airbnb-listing-prices-scikit-learn-and-apache-spark/">https://mapr.com/blog/predicting-airbnb-listing-prices-scikit-learn-and-apache-spark/</a></p></li>
</ul>
<p>Directly using scikit-learn and pyspark’s broadcast function:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://stackoverflow.com/questions/42887621/how-to-do-prediction-with-sklearn-model-inside-spark/42887751">https://stackoverflow.com/questions/42887621/how-to-do-prediction-with-sklearn-model-inside-spark/42887751</a></p></li>
<li><p><a class="reference external" href="https://towardsdatascience.com/deploy-a-python-model-more-efficiently-over-spark-497fc03e0a8d">https://towardsdatascience.com/deploy-a-python-model-more-efficiently-over-spark-497fc03e0a8d</a></p></li>
</ul>
</div>
<div class="section" id="text-analysis-in-pyspark">
<h2>Text analysis in Pyspark<a class="headerlink" href="#text-analysis-in-pyspark" title="Permalink to this headline">¶</a></h2>
<div class="section" id="dealing-with-text-tokenizer-hashing-idf">
<h3>Dealing with text: Tokenizer, Hashing, IDF<a class="headerlink" href="#dealing-with-text-tokenizer-hashing-idf" title="Permalink to this headline">¶</a></h3>
<p>Here from Tue code 1 model in a day, in short</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Split text field in words</span>
<span class="n">tokenizer</span>        <span class="o">=</span> <span class="n">Tokenizer</span><span class="p">(</span><span class="n">inputCol</span><span class="o">=</span><span class="s2">&quot;txft_70&quot;</span><span class="p">,</span> <span class="n">outputCol</span><span class="o">=</span><span class="s2">&quot;words&quot;</span><span class="p">)</span>
<span class="n">wordsData</span>      <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">Data</span><span class="p">)</span>

<span class="c1"># Create Hash table data</span>
<span class="n">NHash</span>          <span class="o">=</span> <span class="mi">256</span> <span class="c1"># number of hashing values used to describe the text</span>
<span class="n">hashing</span>        <span class="o">=</span> <span class="n">HashingTF</span><span class="p">(</span><span class="n">inputCol</span><span class="o">=</span><span class="s2">&quot;words&quot;</span><span class="p">,</span> <span class="n">outputCol</span><span class="o">=</span><span class="s2">&quot;rawTextFeatures&quot;</span><span class="p">,</span> <span class="n">numFeatures</span><span class="o">=</span><span class="n">NHash</span><span class="p">)</span>
<span class="n">featurizedData</span> <span class="o">=</span> <span class="n">hashing</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">wordsData</span><span class="p">)</span>

<span class="c1"># Calculate inverse document frequency</span>
<span class="n">idf</span>            <span class="o">=</span> <span class="n">IDF</span><span class="p">(</span><span class="n">inputCol</span><span class="o">=</span><span class="s2">&quot;rawTextFeatures&quot;</span><span class="p">,</span> <span class="n">outputCol</span><span class="o">=</span><span class="s2">&quot;textFeatures&quot;</span><span class="p">)</span>
<span class="n">idfModel</span>       <span class="o">=</span> <span class="n">idf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">featurizedData</span><span class="p">)</span>
<span class="n">DataText</span>       <span class="o">=</span> <span class="n">idfModel</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">featurizedData</span><span class="p">)</span><span class="o">.</span><span class="n">cache</span><span class="p">()</span>
</pre></div>
</div>
<p>Here a more elaborated example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">pyspark.ml.feature</span> <span class="kn">import</span> <span class="n">HashingTF</span><span class="p">,</span> <span class="n">IDF</span><span class="p">,</span> <span class="n">Tokenizer</span>

<span class="n">sentenceData</span> <span class="o">=</span> <span class="n">sqlContext</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">([</span>
  <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="s2">&quot;Hi I heard about Spark&quot;</span><span class="p">),</span>
  <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="s2">&quot;I wish Java could use case classes&quot;</span><span class="p">),</span>
  <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="s2">&quot;Logistic regression models are neat&quot;</span><span class="p">)],</span> <span class="p">[</span><span class="s2">&quot;label&quot;</span><span class="p">,</span> <span class="s2">&quot;sentence&quot;</span><span class="p">])</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">Tokenizer</span><span class="p">(</span><span class="n">inputCol</span><span class="o">=</span><span class="s2">&quot;sentence&quot;</span><span class="p">,</span> <span class="n">outputCol</span><span class="o">=</span><span class="s2">&quot;words&quot;</span><span class="p">)</span> <span class="c1">#https://spark.apache.org/docs/2.1.0/ml-features.html#tokenizer</span>
<span class="n">wordsData</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">sentenceData</span><span class="p">)</span>
<span class="n">hashingTF</span> <span class="o">=</span> <span class="n">HashingTF</span><span class="p">(</span><span class="n">inputCol</span><span class="o">=</span><span class="s2">&quot;words&quot;</span><span class="p">,</span> <span class="n">outputCol</span><span class="o">=</span><span class="s2">&quot;rawFeatures&quot;</span><span class="p">,</span> <span class="n">numFeatures</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span> <span class="c1">#https://spark.apache.org/docs/2.1.0/ml-features.html#tf-idf</span>
<span class="n">featurizedData</span> <span class="o">=</span> <span class="n">hashingTF</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">wordsData</span><span class="p">)</span>
<span class="n">featurizedData</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="o">+-----+--------------------+--------------------+--------------------+</span>
<span class="o">|</span><span class="n">label</span><span class="o">|</span>            <span class="n">sentence</span><span class="o">|</span>               <span class="n">words</span><span class="o">|</span>         <span class="n">rawFeatures</span><span class="o">|</span>
<span class="o">+-----+--------------------+--------------------+--------------------+</span>
<span class="o">|</span>    <span class="mi">0</span><span class="o">|</span><span class="n">Hi</span> <span class="n">I</span> <span class="n">heard</span> <span class="n">about</span> <span class="o">...|</span><span class="p">[</span><span class="n">hi</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">heard</span><span class="p">,</span> <span class="n">ab</span><span class="o">...|</span><span class="p">(</span><span class="mi">5</span><span class="p">,[</span><span class="mi">0</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">4</span><span class="p">],[</span><span class="mf">2.0</span><span class="p">,</span><span class="mf">2.</span><span class="o">..|</span>
<span class="o">|</span>    <span class="mi">0</span><span class="o">|</span><span class="n">I</span> <span class="n">wish</span> <span class="n">Java</span> <span class="n">could</span><span class="o">...|</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">wish</span><span class="p">,</span> <span class="n">java</span><span class="p">,</span> <span class="n">c</span><span class="o">...|</span><span class="p">(</span><span class="mi">5</span><span class="p">,[</span><span class="mi">0</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">],[</span><span class="mf">1.0</span><span class="o">...|</span>
<span class="o">|</span>    <span class="mi">1</span><span class="o">|</span><span class="n">Logistic</span> <span class="n">regressi</span><span class="o">...|</span><span class="p">[</span><span class="n">logistic</span><span class="p">,</span> <span class="n">regres</span><span class="o">...|</span><span class="p">(</span><span class="mi">5</span><span class="p">,[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">],[</span><span class="mf">1.0</span><span class="o">...|</span>
<span class="o">+-----+--------------------+--------------------+--------------------+</span>

<span class="n">idf</span> <span class="o">=</span> <span class="n">IDF</span><span class="p">(</span><span class="n">inputCol</span><span class="o">=</span><span class="s2">&quot;rawFeatures&quot;</span><span class="p">,</span> <span class="n">outputCol</span><span class="o">=</span><span class="s2">&quot;features&quot;</span><span class="p">)</span>
<span class="n">idfModel</span> <span class="o">=</span> <span class="n">idf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">featurizedData</span><span class="p">)</span>
<span class="n">rescaledData</span> <span class="o">=</span> <span class="n">idfModel</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">featurizedData</span><span class="p">)</span>
<span class="k">for</span> <span class="n">features_label</span> <span class="ow">in</span> <span class="n">rescaledData</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="s2">&quot;features&quot;</span><span class="p">,</span> <span class="s2">&quot;label&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">take</span><span class="p">(</span><span class="mi">3</span><span class="p">):</span>
  <span class="nb">print</span><span class="p">(</span><span class="n">features_label</span><span class="p">)</span>

<span class="c1">#Row(features=SparseVector(5, {0: 0.0, 2: 0.5754, 4: 0.0}), label=0)</span>
<span class="c1">#Row(features=SparseVector(5, {0: 0.0, 2: 0.5754, 3: 0.2877, 4: 0.0}), label=0)</span>
<span class="c1">#Row(features=SparseVector(5, {0: 0.0, 1: 0.6931, 3: 0.5754, 4: 0.0}), label=1)</span>

<span class="n">rescaledData</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="o">+-----+--------------------+--------------------+--------------------+--------------------+</span>
<span class="o">|</span><span class="n">label</span><span class="o">|</span>            <span class="n">sentence</span><span class="o">|</span>               <span class="n">words</span><span class="o">|</span>         <span class="n">rawFeatures</span><span class="o">|</span>            <span class="n">features</span><span class="o">|</span>
<span class="o">+-----+--------------------+--------------------+--------------------+--------------------+</span>
<span class="o">|</span>    <span class="mi">0</span><span class="o">|</span><span class="n">Hi</span> <span class="n">I</span> <span class="n">heard</span> <span class="n">about</span> <span class="o">...|</span><span class="p">[</span><span class="n">hi</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">heard</span><span class="p">,</span> <span class="n">ab</span><span class="o">...|</span><span class="p">(</span><span class="mi">5</span><span class="p">,[</span><span class="mi">0</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">4</span><span class="p">],[</span><span class="mf">2.0</span><span class="p">,</span><span class="mf">2.</span><span class="o">..|</span><span class="p">(</span><span class="mi">5</span><span class="p">,[</span><span class="mi">0</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">4</span><span class="p">],[</span><span class="mf">0.0</span><span class="p">,</span><span class="mf">0.</span><span class="o">..|</span>
<span class="o">|</span>    <span class="mi">0</span><span class="o">|</span><span class="n">I</span> <span class="n">wish</span> <span class="n">Java</span> <span class="n">could</span><span class="o">...|</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">wish</span><span class="p">,</span> <span class="n">java</span><span class="p">,</span> <span class="n">c</span><span class="o">...|</span><span class="p">(</span><span class="mi">5</span><span class="p">,[</span><span class="mi">0</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">],[</span><span class="mf">1.0</span><span class="o">...|</span><span class="p">(</span><span class="mi">5</span><span class="p">,[</span><span class="mi">0</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">],[</span><span class="mf">0.0</span><span class="o">...|</span>
<span class="o">|</span>    <span class="mi">1</span><span class="o">|</span><span class="n">Logistic</span> <span class="n">regressi</span><span class="o">...|</span><span class="p">[</span><span class="n">logistic</span><span class="p">,</span> <span class="n">regres</span><span class="o">...|</span><span class="p">(</span><span class="mi">5</span><span class="p">,[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">],[</span><span class="mf">1.0</span><span class="o">...|</span><span class="p">(</span><span class="mi">5</span><span class="p">,[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">],[</span><span class="mf">0.0</span><span class="o">...|</span>
<span class="o">+-----+--------------------+--------------------+--------------------+--------------------+</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="scala-spark">
<h2>Scala Spark<a class="headerlink" href="#scala-spark" title="Permalink to this headline">¶</a></h2>
<p>From <a class="reference external" href="https://jaceklaskowski.github.io/spark-workshop/exercises/spark-sql-exercise-How-to-add-days-as-values-of-a-column-to-date.html">https://jaceklaskowski.github.io/spark-workshop/exercises/spark-sql-exercise-How-to-add-days-as-values-of-a-column-to-date.html</a></p>
<p>Write a structured query (using spark-shell or Databricks Community Edition) that adds a given number of days (from one column) to a date (from another column) and prints out the rows to the standard output:</p>
<div class="highlight-scala notranslate"><div class="highlight"><pre><span></span><span class="k">val</span> <span class="n">data</span> <span class="k">=</span> <span class="nc">Seq</span><span class="o">(</span>
  <span class="o">(</span><span class="mi">0</span><span class="o">,</span> <span class="s">&quot;2016-01-1&quot;</span><span class="o">),</span>
  <span class="o">(</span><span class="mi">1</span><span class="o">,</span> <span class="s">&quot;2016-02-2&quot;</span><span class="o">),</span>
  <span class="o">(</span><span class="mi">2</span><span class="o">,</span> <span class="s">&quot;2016-03-22&quot;</span><span class="o">),</span>
  <span class="o">(</span><span class="mi">3</span><span class="o">,</span> <span class="s">&quot;2016-04-25&quot;</span><span class="o">),</span>
  <span class="o">(</span><span class="mi">4</span><span class="o">,</span> <span class="s">&quot;2016-05-21&quot;</span><span class="o">),</span>
  <span class="o">(</span><span class="mi">5</span><span class="o">,</span> <span class="s">&quot;2016-06-1&quot;</span><span class="o">),</span>
  <span class="o">(</span><span class="mi">6</span><span class="o">,</span> <span class="s">&quot;2016-03-21&quot;</span><span class="o">)</span>
<span class="o">).</span><span class="n">toDF</span><span class="o">(</span><span class="s">&quot;number_of_days&quot;</span><span class="o">,</span> <span class="s">&quot;date&quot;</span><span class="o">)</span>

<span class="k">import</span> <span class="nn">org.apache.spark.sql.functions.expr</span>
<span class="n">data</span><span class="o">.</span><span class="n">withColumn</span><span class="o">(</span><span class="s">&quot;future&quot;</span><span class="o">,</span><span class="n">expr</span><span class="o">(</span><span class="s">&quot;date_add(date,number_of_days)&quot;</span><span class="o">)).</span><span class="n">show</span><span class="o">()</span>

<span class="o">+--------------+----------+----------+</span>
<span class="o">|</span><span class="n">number_of_days</span><span class="o">|</span>      <span class="n">date</span><span class="o">|</span>    <span class="n">future</span><span class="o">|</span>
<span class="o">+--------------+----------+----------+</span>
<span class="o">|</span>             <span class="mi">0</span><span class="o">|</span> <span class="mi">2016</span><span class="o">-</span><span class="mi">01</span><span class="o">-</span><span class="mi">1</span><span class="o">|</span><span class="mi">2016</span><span class="o">-</span><span class="mi">01</span><span class="o">-</span><span class="mi">01</span><span class="o">|</span>
<span class="o">|</span>             <span class="mi">1</span><span class="o">|</span> <span class="mi">2016</span><span class="o">-</span><span class="mi">02</span><span class="o">-</span><span class="mi">2</span><span class="o">|</span><span class="mi">2016</span><span class="o">-</span><span class="mi">02</span><span class="o">-</span><span class="mi">03</span><span class="o">|</span>
<span class="o">|</span>             <span class="mi">2</span><span class="o">|</span><span class="mi">2016</span><span class="o">-</span><span class="mi">03</span><span class="o">-</span><span class="mi">22</span><span class="o">|</span><span class="mi">2016</span><span class="o">-</span><span class="mi">03</span><span class="o">-</span><span class="mi">24</span><span class="o">|</span>
<span class="o">|</span>             <span class="mi">3</span><span class="o">|</span><span class="mi">2016</span><span class="o">-</span><span class="mi">04</span><span class="o">-</span><span class="mi">25</span><span class="o">|</span><span class="mi">2016</span><span class="o">-</span><span class="mi">04</span><span class="o">-</span><span class="mi">28</span><span class="o">|</span>
<span class="o">|</span>             <span class="mi">4</span><span class="o">|</span><span class="mi">2016</span><span class="o">-</span><span class="mi">05</span><span class="o">-</span><span class="mi">21</span><span class="o">|</span><span class="mi">2016</span><span class="o">-</span><span class="mi">05</span><span class="o">-</span><span class="mi">25</span><span class="o">|</span>
<span class="o">|</span>             <span class="mi">5</span><span class="o">|</span> <span class="mi">2016</span><span class="o">-</span><span class="mi">06</span><span class="o">-</span><span class="mi">1</span><span class="o">|</span><span class="mi">2016</span><span class="o">-</span><span class="mi">06</span><span class="o">-</span><span class="mi">06</span><span class="o">|</span>
<span class="o">|</span>             <span class="mi">6</span><span class="o">|</span><span class="mi">2016</span><span class="o">-</span><span class="mi">03</span><span class="o">-</span><span class="mi">21</span><span class="o">|</span><span class="mi">2016</span><span class="o">-</span><span class="mi">03</span><span class="o">-</span><span class="mi">27</span><span class="o">|</span>
<span class="o">+--------------+----------+----------+</span>
</pre></div>
</div>
</div>
</div>


           </div>
           
          </div>
          <footer>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2017, pmeu

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>