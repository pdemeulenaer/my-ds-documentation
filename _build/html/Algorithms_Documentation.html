

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Algorithms &mdash; Pyspark_Documentation 0.1 documentation</title>
  

  
  
  
  

  
  <script type="text/javascript" src="_static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/doctools.js"></script>
        <script src="_static/language_data.js"></script>
        <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    
    <script type="text/javascript" src="_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Statistics and Probabilities" href="Statistics_probabilities_Documentation.html" />
    <link rel="prev" title="Time Series in Python" href="Time_Series_inPython_Documentation.html" />
    <link href="_static/style.css" rel="stylesheet" type="text/css">

</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="index.html" class="icon icon-home"> Pyspark_Documentation
          

          
          </a>

          
            
            
              <div class="version">
                0.1
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="Pandas_Documentation.html">Python, Jupyter and Pandas TESTTTTTTTTTTTTTTT</a></li>
<li class="toctree-l1"><a class="reference internal" href="Sklearn_Documentation.html">Scikit-learn</a></li>
<li class="toctree-l1"><a class="reference internal" href="Pyspark_Documentation.html">Pyspark</a></li>
<li class="toctree-l1"><a class="reference internal" href="Pyspark_Documentation.html#scala-spark">Scala - Spark</a></li>
<li class="toctree-l1"><a class="reference internal" href="Text_Mining_Documentation.html">Text Mining in Python</a></li>
<li class="toctree-l1"><a class="reference internal" href="DeepLearning_Documentation.html">Deep Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="ReinforcementLearning_Documentation.html">Reinforcement Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="Time_Series_inPython_Documentation.html">Time Series in Python</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Algorithms</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#supervised-machine-learning">Supervised Machine Learning</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#association-rules-apriori-algorithm">Association rules: Apriori algorithm</a></li>
<li class="toctree-l3"><a class="reference internal" href="#implementations">Implementations</a></li>
<li class="toctree-l3"><a class="reference internal" href="#collaborative-filtering">Collaborative filtering</a></li>
<li class="toctree-l3"><a class="reference internal" href="#naive-bayes">Naive Bayes</a></li>
<li class="toctree-l3"><a class="reference internal" href="#decision-tree">Decision Tree</a></li>
<li class="toctree-l3"><a class="reference internal" href="#random-forest">Random Forest</a></li>
<li class="toctree-l3"><a class="reference internal" href="#gradient-boosting-xgboost">Gradient Boosting:, XGBoost</a></li>
<li class="toctree-l3"><a class="reference internal" href="#isolation-forest-if">Isolation Forest (IF)</a></li>
<li class="toctree-l3"><a class="reference internal" href="#algorithm-interpretability">Algorithm interpretability</a></li>
<li class="toctree-l3"><a class="reference internal" href="#performance-metrics-for-sl">Performance metrics (for SL)</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#unsupervised-machine-learning-clustering">Unsupervised Machine Learning - Clustering</a></li>
<li class="toctree-l2"><a class="reference internal" href="#deep-learning">Deep Learning</a></li>
<li class="toctree-l2"><a class="reference internal" href="#machine-learning-ranking">Machine Learning Ranking</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#ranknet">RankNet</a></li>
<li class="toctree-l3"><a class="reference internal" href="#lambdarank">LambdaRank</a></li>
<li class="toctree-l3"><a class="reference internal" href="#lambdamart">LambdaMart</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="Statistics_probabilities_Documentation.html">Statistics and Probabilities</a></li>
<li class="toctree-l1"><a class="reference internal" href="Plot_libraries_Documentation.html">Great plot libraries</a></li>
<li class="toctree-l1"><a class="reference internal" href="SQL_Documentation.html">SQL Server documentation</a></li>
<li class="toctree-l1"><a class="reference internal" href="Bash_Documentation.html">Useful Bash commands (or batch)</a></li>
<li class="toctree-l1"><a class="reference internal" href="Bash_Documentation.html#useful-git-commands">Useful GIT commands</a></li>
<li class="toctree-l1"><a class="reference internal" href="Bash_Documentation.html#useful-vim-commands">Useful VIM commands</a></li>
<li class="toctree-l1"><a class="reference internal" href="Bash_Documentation.html#data-types">Data types</a></li>
<li class="toctree-l1"><a class="reference internal" href="Business_models_Documentation.html">Interesting business models</a></li>
<li class="toctree-l1"><a class="reference internal" href="Sphinx_stuff.html">Sphinx</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">Pyspark_Documentation</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="index.html">Docs</a> &raquo;</li>
        
      <li>Algorithms</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="_sources/Algorithms_Documentation.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="algorithms">
<h1>Algorithms<a class="headerlink" href="#algorithms" title="Permalink to this headline">¶</a></h1>
<p>Glossary of Machine Learning: <a class="reference external" href="https://semanti.ca/blog/?glossary-of-machine-learning-terms">https://semanti.ca/blog/?glossary-of-machine-learning-terms</a></p>
<p>Cheatsheets on Machine Learning: <a class="reference external" href="https://github.com/afshinea/stanford-cs-229-machine-learning/tree/master/en">https://github.com/afshinea/stanford-cs-229-machine-learning/tree/master/en</a></p>
<div class="section" id="supervised-machine-learning">
<h2>Supervised Machine Learning<a class="headerlink" href="#supervised-machine-learning" title="Permalink to this headline">¶</a></h2>
<div class="topic">
<p class="topic-title">Introduction</p>
<p>The objective here is to have everything useful for the projects, not to make a complete documentation of the whole package. Here I will try to document both version 1.6 and &gt;2.0. A special enphase will be done on machine learning module ml (mllib is outdated).
We will not review the full Pyspark documentation. For that, look at <a class="reference external" href="http://spark.apache.org/docs/1.6.0/programming-guide.html">http://spark.apache.org/docs/1.6.0/programming-guide.html</a> for version 1.6, <a class="reference external" href="http://spark.apache.org/docs/2.1.0/programming-guide.html">http://spark.apache.org/docs/2.1.0/programming-guide.html</a> for version 2.1.</p>
</div>
<div class="section" id="association-rules-apriori-algorithm">
<h3>Association rules: Apriori algorithm<a class="headerlink" href="#association-rules-apriori-algorithm" title="Permalink to this headline">¶</a></h3>
<p><a class="reference external" href="https://en.wikipedia.org/wiki/Apriori_algorithm">https://en.wikipedia.org/wiki/Apriori_algorithm</a></p>
<p>Example 1:</p>
<p>Consider the following database, where each row is a transaction and each cell is an individual item of the transaction:</p>
<table class="colwidths-given docutils align-default" id="id1">
<caption><span class="caption-text">Some data</span><a class="headerlink" href="#id1" title="Permalink to this table">¶</a></caption>
<colgroup>
<col style="width: 33%" />
<col style="width: 33%" />
<col style="width: 33%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Data1</p></th>
<th class="head"><p>Data2</p></th>
<th class="head"><p>Data3</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>alpha</p></td>
<td><p>beta</p></td>
<td><p>epsilon</p></td>
</tr>
<tr class="row-odd"><td><p>alpha</p></td>
<td><p>beta</p></td>
<td><p>theta</p></td>
</tr>
<tr class="row-even"><td><p>alpha</p></td>
<td><p>beta</p></td>
<td><p>epsilon</p></td>
</tr>
<tr class="row-odd"><td><p>alpha</p></td>
<td><p>beta</p></td>
<td><p>theta</p></td>
</tr>
</tbody>
</table>
<p>The association rules that can be determined from this database are the following:</p>
<ul class="simple">
<li><p>100% of sets with alpha also contain beta</p></li>
<li><p>50% of sets with alpha, beta also have epsilon</p></li>
<li><p>50% of sets with alpha, beta also have theta</p></li>
</ul>
<p>we can also illustrate this through a variety of examples.</p>
<p>Example 2:</p>
<p>Assume that a large supermarket tracks sales data by stock-keeping unit (SKU) for each item: each item, such as “butter” or “bread”, is identified by a numerical SKU. The supermarket has a database of transactions where each transaction is a set of SKUs that were bought together.</p>
<p>Let the database of transactions consist of following itemsets:</p>
<ul class="simple">
<li><p>{1,2,3,4}</p></li>
<li><p>{1,2,4}</p></li>
<li><p>{1,2}</p></li>
<li><p>{2,3,4}</p></li>
<li><p>{2,3}</p></li>
<li><p>{3,4}</p></li>
<li><p>{2,4}</p></li>
</ul>
<p>We will use Apriori to determine the frequent item sets of this database. To do this, we will say that an item set is frequent if it appears in at least 3 transactions of the database: the value 3 is the support threshold.</p>
<p>The first step of Apriori is to count up the number of occurrences, called the support, of each member item separately. By scanning the database for the first time, we obtain the following result</p>
<table class="colwidths-given docutils align-default" id="id2">
<caption><span class="caption-text">Support table</span><a class="headerlink" href="#id2" title="Permalink to this table">¶</a></caption>
<colgroup>
<col style="width: 50%" />
<col style="width: 50%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Item</p></th>
<th class="head"><p>Support</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>{1}</p></td>
<td><p>3</p></td>
</tr>
<tr class="row-odd"><td><p>{2}</p></td>
<td><p>6</p></td>
</tr>
<tr class="row-even"><td><p>{3}</p></td>
<td><p>4</p></td>
</tr>
<tr class="row-odd"><td><p>{4}</p></td>
<td><p>5</p></td>
</tr>
</tbody>
</table>
<p>All the itemsets of size 1 have a support of at least 3, so they are all frequent.</p>
<p>The next step is to generate a list of all pairs of the frequent items.</p>
<p>For example, regarding the pair {1,2}: the first table of Example 2 shows items 1 and 2 appearing together in three of the itemsets; therefore, we say item {1,2} has support of three.</p>
<table class="colwidths-given docutils align-default" id="id3">
<caption><span class="caption-text">Support table</span><a class="headerlink" href="#id3" title="Permalink to this table">¶</a></caption>
<colgroup>
<col style="width: 50%" />
<col style="width: 50%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Item</p></th>
<th class="head"><p>Support</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>{1,2}</p></td>
<td><p>3</p></td>
</tr>
<tr class="row-odd"><td><p>{1,3}</p></td>
<td><p>1</p></td>
</tr>
<tr class="row-even"><td><p>{1,4}</p></td>
<td><p>2</p></td>
</tr>
<tr class="row-odd"><td><p>{2,3}</p></td>
<td><p>3</p></td>
</tr>
<tr class="row-even"><td><p>{2,4}</p></td>
<td><p>4</p></td>
</tr>
<tr class="row-odd"><td><p>{3,4}</p></td>
<td><p>3</p></td>
</tr>
</tbody>
</table>
<p>The pairs {1,2}, {2,3}, {2,4}, and {3,4} all meet or exceed the minimum support of 3, so they are frequent. The pairs {1,3} and {1,4} are not. Now, because {1,3} and {1,4} are not frequent, any larger set which contains {1,3} or {1,4} cannot be frequent. In this way, we can prune sets: we will now look for frequent triples in the database, but we can already exclude all the triples that contain one of these two pairs:</p>
<table class="colwidths-given docutils align-default" id="id4">
<caption><span class="caption-text">Support table</span><a class="headerlink" href="#id4" title="Permalink to this table">¶</a></caption>
<colgroup>
<col style="width: 50%" />
<col style="width: 50%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Item</p></th>
<th class="head"><p>Support</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>{2,3,4}</p></td>
<td><p>2</p></td>
</tr>
</tbody>
</table>
<p>in the example, there are no frequent triplets. {2,3,4} is below the minimal threshold, and the other triplets were excluded because they were super sets of pairs that were already below the threshold.</p>
<p>We have thus determined the frequent sets of items in the database, and illustrated how some items were not counted because one of their subsets was already known to be below the threshold.</p>
</div>
<div class="section" id="implementations">
<h3>Implementations<a class="headerlink" href="#implementations" title="Permalink to this headline">¶</a></h3>
<p><a class="reference external" href="https://pypi.org/project/efficient-apriori/">https://pypi.org/project/efficient-apriori/</a> (probably the fastest one, and user-friendly, but NOT pandas-friendly. Only pip, not conda)
mlxtend: <a class="reference external" href="https://rasbt.github.io/mlxtend/user_guide/frequent_patterns/apriori/">https://rasbt.github.io/mlxtend/user_guide/frequent_patterns/apriori/</a> (more pandas-friendly … but I don’t find the concept of confidence)</p>
<blockquote>
<div><p>Note: mlxtend is wider ML package (made by Sebastian Raschka!) that can do many stuff, including enemble classification (combination of different classifiers, EnsembleVoteClassifier), see <a class="reference external" href="https://rasbt.github.io/mlxtend/">https://rasbt.github.io/mlxtend/</a></p>
</div></blockquote>
<p>Apriori: <a class="reference external" href="https://github.com/asaini/Apriori">https://github.com/asaini/Apriori</a>
<a class="reference external" href="https://stackabuse.com/association-rule-mining-via-apriori-algorithm-in-python/">https://stackabuse.com/association-rule-mining-via-apriori-algorithm-in-python/</a> with package apyori: <a class="reference external" href="https://github.com/ymoch/apyori">https://github.com/ymoch/apyori</a>
<a class="reference external" href="http://adataanalyst.com/machine-learning/apriori-algorithm-python-3-0/">http://adataanalyst.com/machine-learning/apriori-algorithm-python-3-0/</a></p>
<p>See also <a class="reference external" href="https://www.datacamp.com/community/tutorials/market-basket-analysis-r">https://www.datacamp.com/community/tutorials/market-basket-analysis-r</a> (in R, many different htings)</p>
</div>
<div class="section" id="collaborative-filtering">
<h3>Collaborative filtering<a class="headerlink" href="#collaborative-filtering" title="Permalink to this headline">¶</a></h3>
<p>The main idea behind collaborative filtering is to adopt for a subject same item as for similar other subjects:</p>
<div class="figure align-default">
<a class="reference internal image-reference" href="_images/Collaborative_filtering1.png"><img alt="Collaborative_filtering1" src="_images/Collaborative_filtering1.png" style="width: 773.0px; height: 251.0px;" /></a>
</div>
<p>See also <a class="reference external" href="https://towardsdatascience.com/various-implementations-of-collaborative-filtering-100385c6dfe0">https://towardsdatascience.com/various-implementations-of-collaborative-filtering-100385c6dfe0</a></p>
</div>
<div class="section" id="naive-bayes">
<h3>Naive Bayes<a class="headerlink" href="#naive-bayes" title="Permalink to this headline">¶</a></h3>
<p>First, remember the Bayes theorem:</p>
<div class="figure align-default">
<a class="reference internal image-reference" href="_images/Bayes_theorem.png"><img alt="Bayes theorem" src="_images/Bayes_theorem.png" style="width: 500.0px; height: 238.0px;" /></a>
</div>
</div>
<div class="section" id="decision-tree">
<h3>Decision Tree<a class="headerlink" href="#decision-tree" title="Permalink to this headline">¶</a></h3>
<p><a class="reference external" href="https://medium.com/meta-design-ideas/decision-tree-a-light-intro-to-theory-math-code-10dbb3472ec4">https://medium.com/meta-design-ideas/decision-tree-a-light-intro-to-theory-math-code-10dbb3472ec4</a></p>
<p>Gini impurity: <a class="reference external" href="https://victorzhou.com/blog/gini-impurity/">https://victorzhou.com/blog/gini-impurity/</a></p>
<p>Decision tree/RF: <a class="reference external" href="https://victorzhou.com/blog/intro-to-random-forests/">https://victorzhou.com/blog/intro-to-random-forests/</a></p>
</div>
<div class="section" id="random-forest">
<h3>Random Forest<a class="headerlink" href="#random-forest" title="Permalink to this headline">¶</a></h3>
<p>What is Random Forest?
A random forest is an ensemble of decision trees that will output a prediction value. An ensemble model combines the results from different models. A Random Forest is combination of classification and regression. The result from an ensemble model is usually better than the result from one of the individual models. In Random Forest, each decision tree is constructed by using a random subset of the training data that has predictors with known response. After you have trained your forest, you can then pass each test row through it, in order to output a prediction. The goal is to predict the response when it’s unknown. The response can be categorical(classification) or continuous (regression). In a decision tree, an input is entered at the top and as it traverses down the tree the data gets bucketed into smaller and smaller sets. The random forest takes the notion of decision trees to the next level by combining trees. Thus, in ensemble terms, the trees are weak learners and the random forest is a strong learner.
Pros and Cons of choosing Random Forest?
Random forest combines trees and hence incorporates most of the advantages of trees like handling missing values in variable, suiting for both classification and regression, handling highly non-linear interactions and classification boundaries. In addition, Random Forest gives built-in estimates of accuracy, gives automatic variable selection. variable importance, handles wide data – data with more predictors than observations and works well off the shelf – needs only little tuning, can get results very quickly. The runtimes are quite fast, and they are able to deal with unbalanced and missing data.
Random Forest weaknesses are that when used for regression they cannot predict beyond the range in the training data, and that they may over-fit data sets that are particularly noisy.</p>
<p>For an implementation using Scikit-learn, see <a class="reference internal" href="Sklearn_Documentation.html#rf-sklearn-label"><span class="std std-ref">Random Forest</span></a></p>
<div class="figure align-default" id="id5">
<a class="reference internal image-reference" href="_images/RF_algo.png"><img alt="Random Forest scheme" src="_images/RF_algo.png" style="width: 1096.0px; height: 811.0px;" /></a>
<p class="caption"><span class="caption-text">An old slide…</span><a class="headerlink" href="#id5" title="Permalink to this image">¶</a></p>
</div>
<p>Here are a few posts on the RF algorithm:</p>
<ul class="simple">
<li><p>Simple intro: Decision tree/RF: <a class="reference external" href="https://victorzhou.com/blog/intro-to-random-forests/">https://victorzhou.com/blog/intro-to-random-forests/</a></p></li>
<li><p>Simple intro: <a class="reference external" href="https://towardsdatascience.com/the-random-forest-algorithm-d457d499ffcd">https://towardsdatascience.com/the-random-forest-algorithm-d457d499ffcd</a></p></li>
<li><p>Good intro: <a class="reference external" href="https://medium.com/open-machine-learning-course/open-machine-learning-course-topic-5-ensembles-of-algorithms-and-random-forest-8e05246cbba7">https://medium.com/open-machine-learning-course/open-machine-learning-course-topic-5-ensembles-of-algorithms-and-random-forest-8e05246cbba7</a></p></li>
<li><p>Quick intro: <a class="reference external" href="https://medium.com/&#64;Synced/how-random-forest-algorithm-works-in-machine-learning-3c0fe15b6674">https://medium.com/&#64;Synced/how-random-forest-algorithm-works-in-machine-learning-3c0fe15b6674</a></p></li>
<li><p>Intro : <a class="reference external" href="https://medium.com/&#64;williamkoehrsen/random-forest-simple-explanation-377895a60d2d">https://medium.com/&#64;williamkoehrsen/random-forest-simple-explanation-377895a60d2d</a></p></li>
<li><p>Guide to Decision tree and RF: <a class="reference external" href="https://towardsdatascience.com/enchanted-random-forest-b08d418cb411">https://towardsdatascience.com/enchanted-random-forest-b08d418cb411</a></p></li>
<li><p>Example of application: <a class="reference external" href="https://towardsdatascience.com/random-forest-in-python-24d0893d51c0">https://towardsdatascience.com/random-forest-in-python-24d0893d51c0</a></p></li>
<li><p>Hyperparameter tuning: <a class="reference external" href="https://towardsdatascience.com/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74">https://towardsdatascience.com/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74</a></p></li>
<li><p>missing values in RF: <a class="reference external" href="https://medium.com/airbnb-engineering/overcoming-missing-values-in-a-random-forest-classifier-7b1fc1fc03ba">https://medium.com/airbnb-engineering/overcoming-missing-values-in-a-random-forest-classifier-7b1fc1fc03ba</a></p></li>
<li><p>Example of application (churn): <a class="reference external" href="https://blog.slavv.com/identifying-churn-drivers-with-random-forests-65bad0193e6b">https://blog.slavv.com/identifying-churn-drivers-with-random-forests-65bad0193e6b</a></p></li>
<li><p>Feature importance 1: <a class="reference external" href="https://medium.com/the-artificial-impostor/feature-importance-measures-for-tree-models-part-i-47f187c1a2c3">https://medium.com/the-artificial-impostor/feature-importance-measures-for-tree-models-part-i-47f187c1a2c3</a></p></li>
<li><p>Feature importance 2: <a class="reference external" href="https://becominghuman.ai/feature-importance-measures-for-tree-models-part-ii-20c9ff4329b">https://becominghuman.ai/feature-importance-measures-for-tree-models-part-ii-20c9ff4329b</a></p></li>
<li><p>Interpretation of RF: <a class="reference external" href="http://blog.datadive.net/interpreting-random-forests/">http://blog.datadive.net/interpreting-random-forests/</a></p></li>
<li><p>Interpretation of RF: <a class="reference external" href="https://towardsdatascience.com/intuitive-interpretation-of-random-forest-2238687cae45">https://towardsdatascience.com/intuitive-interpretation-of-random-forest-2238687cae45</a></p></li>
<li><p>Bagging vs Boosting: <a class="reference external" href="https://towardsdatascience.com/how-to-develop-a-robust-algorithm-c38e08f32201">https://towardsdatascience.com/how-to-develop-a-robust-algorithm-c38e08f32201</a></p></li>
<li><p>Categorical variables in tree methods: <a class="reference external" href="https://medium.com/data-design/visiting-categorical-features-and-encoding-in-decision-trees-53400fa65931">https://medium.com/data-design/visiting-categorical-features-and-encoding-in-decision-trees-53400fa65931</a></p></li>
</ul>
<p>Random forest from scratch:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://machinelearningmastery.com/implement-random-forest-scratch-python/">https://machinelearningmastery.com/implement-random-forest-scratch-python/</a> (for classification)</p></li>
<li><p><a class="reference external" href="https://towardsdatascience.com/random-forests-and-decision-trees-from-scratch-in-python-3e4fa5ae4249">https://towardsdatascience.com/random-forests-and-decision-trees-from-scratch-in-python-3e4fa5ae4249</a> (maybe even more complete, but for regression only, could be tuned for classification)</p></li>
</ul>
</div>
<div class="section" id="gradient-boosting-xgboost">
<h3>Gradient Boosting:, XGBoost<a class="headerlink" href="#gradient-boosting-xgboost" title="Permalink to this headline">¶</a></h3>
<p>Gradient Boosting: <a class="reference external" href="https://www.youtube.com/watch?v=sRktKszFmSk&amp;t=370s">https://www.youtube.com/watch?v=sRktKszFmSk&amp;t=370s</a></p>
<p>XGboost: <a class="reference external" href="https://medium.com/&#64;pushkarmandot/how-exactly-xgboost-works-a320d9b8aeef">https://medium.com/&#64;pushkarmandot/how-exactly-xgboost-works-a320d9b8aeef</a></p>
<p>Video XGboost: <a class="reference external" href="https://www.youtube.com/watch?v=Vly8xGnNiWs">https://www.youtube.com/watch?v=Vly8xGnNiWs</a></p>
<ul class="simple">
<li><p>interpretable machine learning: XGBoost: <a class="reference external" href="https://towardsdatascience.com/interpretable-machine-learning-with-xgboost-9ec80d148d27">https://towardsdatascience.com/interpretable-machine-learning-with-xgboost-9ec80d148d27</a></p></li>
</ul>
</div>
<div class="section" id="isolation-forest-if">
<h3>Isolation Forest (IF)<a class="headerlink" href="#isolation-forest-if" title="Permalink to this headline">¶</a></h3>
<p>Isolation Forest is a tree based method for anomaly detection.</p>
<p>Taken from <a class="reference external" href="https://www.youtube.com/watch?v=5p8B2Ikcw-k">https://www.youtube.com/watch?v=5p8B2Ikcw-k</a> :</p>
<div class="figure align-default">
<a class="reference internal image-reference" href="_images/Isolation_forest_1.PNG"><img alt="Isolation_forest_1.PNG" src="_images/Isolation_forest_1.PNG" style="width: 855.0px; height: 324.0px;" /></a>
</div>
<div class="figure align-default">
<a class="reference internal image-reference" href="_images/Isolation_forest_2.PNG"><img alt="Isolation_forest_2.PNG" src="_images/Isolation_forest_2.PNG" style="width: 858.0px; height: 213.0px;" /></a>
</div>
<div class="figure align-default">
<a class="reference internal image-reference" href="_images/Isolation_forest_3.PNG"><img alt="Isolation_forest_3.PNG" src="_images/Isolation_forest_3.PNG" style="width: 867.0px; height: 248.0px;" /></a>
</div>
<p>See also <a class="reference external" href="https://towardsdatascience.com/outlier-detection-with-isolation-forest-3d190448d45e">https://towardsdatascience.com/outlier-detection-with-isolation-forest-3d190448d45e</a></p>
</div>
<div class="section" id="algorithm-interpretability">
<h3>Algorithm interpretability<a class="headerlink" href="#algorithm-interpretability" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>Tree-specific: Interpretation of RF: <a class="reference external" href="http://blog.datadive.net/interpreting-random-forests/">http://blog.datadive.net/interpreting-random-forests/</a></p></li>
<li><p>Tree-specific: Interpretation of RF: <a class="reference external" href="https://towardsdatascience.com/intuitive-interpretation-of-random-forest-2238687cae45">https://towardsdatascience.com/intuitive-interpretation-of-random-forest-2238687cae45</a></p></li>
<li><p>Tree-specific (treeinterpreter for classification): <a class="reference external" href="http://engineering.pivotal.io/post/interpreting-decision-trees-and-random-forests/">http://engineering.pivotal.io/post/interpreting-decision-trees-and-random-forests/</a></p></li>
<li><p>XGBoost-specific: interpretable machine learning: XGBoost: <a class="reference external" href="https://towardsdatascience.com/interpretable-machine-learning-with-xgboost-9ec80d148d27">https://towardsdatascience.com/interpretable-machine-learning-with-xgboost-9ec80d148d27</a></p></li>
</ul>
</div>
<div class="section" id="performance-metrics-for-sl">
<h3>Performance metrics (for SL)<a class="headerlink" href="#performance-metrics-for-sl" title="Permalink to this headline">¶</a></h3>
<p>Excellent post on different metrics usually used: <a class="reference external" href="https://towardsdatascience.com/beyond-accuracy-precision-and-recall-3da06bea9f6c">https://towardsdatascience.com/beyond-accuracy-precision-and-recall-3da06bea9f6c</a></p>
<p>Discussion of ROC, GAIN, LIFT and other important quantities: <a class="reference external" href="http://www.saedsayad.com/model_evaluation_c.htm">http://www.saedsayad.com/model_evaluation_c.htm</a> , <a class="reference external" href="http://www2.cs.uregina.ca/~dbd/cs831/notes/lift_chart/lift_chart.html">http://www2.cs.uregina.ca/~dbd/cs831/notes/lift_chart/lift_chart.html</a></p>
<p><strong>Gain chart</strong>: Gain or lift is a measure of the effectiveness of a classification model calculated as the ratio between the results obtained with and without the model. Gain and lift charts are visual aids for evaluating performance of classification models. However, in contrast to the confusion matrix that evaluates models on the whole population gain or lift chart evaluates model performance in a portion of the population. (Here the wizard curve is the perfect model! Different than in ROC)</p>
<p>In Y-axis we have the % of positive response, in X-axis we have the % of customers contacted. The principle in the blue curve (the model) is to start by the customers with best scores/rank, then as the curves evolves to the right we add worse ones.</p>
<div class="figure align-default">
<a class="reference internal image-reference" href="_images/Gain_chart.png"><img alt="A gain chart" src="_images/Gain_chart.png" style="width: 652.0px; height: 887.0px;" /></a>
</div>
<p>So we see that if we pick the 10% best customers, we jump very high, much more than when we add from 30% to 40% for example.</p>
<p><strong>Lift chart</strong>: The lift chart shows how much more likely we are to receive positive responses than if we contact a random sample of customers. For example, by contacting only 10% of customers based on the predictive model we will reach 3 times as many respondents, as if we use no model.</p>
<p>The horizontal 0-line is the baseline, the random case.</p>
<div class="figure align-default">
<a class="reference internal image-reference" href="_images/Lift.png"><img alt="A lift chart" src="_images/Lift.png" style="width: 425.0px; height: 270.0px;" /></a>
</div>
<p>A variant of the lift is to put in Y-axis the probability derived for the customers (for example the probability to take a loan) vs the number of customers (of course, customers are ranked by the proba, as in normal lift):</p>
<div class="figure align-default">
<a class="reference internal image-reference" href="_images/Lift_variant.png"><img alt="A lift variant" src="_images/Lift_variant.png" style="width: 614.0px; height: 234.0px;" /></a>
</div>
<p>Simple on lift: <a class="reference external" href="https://www.kdnuggets.com/2016/03/lift-analysis-data-scientist-secret-weapon.html">https://www.kdnuggets.com/2016/03/lift-analysis-data-scientist-secret-weapon.html</a></p>
<p><strong>The Z-score</strong>: Simply put, a z-score is the number of standard deviations from the mean a data point is. But more technically it’s a measure of how many standard deviations below or above the population mean a raw score is. A z-score is also known as a standard score and it can be placed on a normal distribution curve. Z-scores range from -3 standard deviations (which would fall to the far left of the normal distribution curve) up to +3 standard deviations (which would fall to the far right of the normal distribution curve). In order to use a z-score, you need to know the mean μ and also the population standard deviation σ.</p>
<p>Z-scores are a way to compare results from a test to a “normal” population. Results from tests or surveys have thousands of possible results and units. However, those results can often seem meaningless. For example, knowing that someone’s weight is 150 pounds might be good information, but if you want to compare it to the “average” person’s weight, looking at a vast table of data can be overwhelming (especially if some weights are recorded in kilograms). A z-score can tell you where that person’s weight is compared to the average population’s mean weight.</p>
<p>The Z Score Formula: For One Sample:</p>
<p>The basic z score formula for a sample is:</p>
<p>z = (x – μ) / σ</p>
<p>For example, let’s say you have a test score of 190. The test has a mean (μ) of 150 and a standard deviation (σ) of 25. Assuming a normal distribution, your z score would be:</p>
<p>z = (x – μ) / σ = 190 – 150 / 25 = 1.6.</p>
<p>The z score tells you how many standard deviations from the mean your score is. In this example, your score is 1.6 standard deviations above the mean.</p>
<p>The Z score formula: for multiple samples (i.e. multiple data points):</p>
<p>When you have multiple samples and want to describe the standard deviation of those sample means (the standard error), you would use this z score formula:</p>
<p>z = (x – μ) / (σ / √n)</p>
<p>This z-score will tell you how many standard errors there are between the sample mean and the population mean.</p>
<p>Sample problem: In general, the mean height of women is 65″ with a standard deviation of 3.5″. What is the probability of finding a random sample of 50 women with a mean height of 70″, assuming the heights are normally distributed?</p>
<p>z = (x – μ) / (σ / √n) = (70 – 65) / (3.5/√50) = 5 / 0.495 = 10.1</p>
<p>The key here is that we’re dealing with a sampling distribution of means, so we know we have to include the standard error in the formula. We also know that 99% of values fall within 3 standard deviations from the mean in a normal probability distribution (see 68 95 99.7 rule). Therefore, there’s less than 1% probability that any sample of women will have a mean height of 70″.</p>
<p>See <a class="reference external" href="http://www.statisticshowto.com/probability-and-statistics/z-score/">http://www.statisticshowto.com/probability-and-statistics/z-score/</a> , <a class="reference external" href="https://en.wikipedia.org/wiki/Standard_score">https://en.wikipedia.org/wiki/Standard_score</a> for more</p>
<p>Z-score normalization is another term for standardization, or scaling… You just remove the mean and divide by the standard deviation:</p>
<div class="figure align-default">
<a class="reference internal image-reference" href="_images/Standardization.png"><img alt="Standardization" src="_images/Standardization.png" style="width: 1150.0px; height: 331.0px;" /></a>
</div>
<p>In scikit-learn:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">StandardScaler</span>

<span class="n">scaler</span> <span class="o">=</span> <span class="n">StandardScaler</span><span class="p">()</span>

<span class="c1"># Fit only to the training data</span>
<span class="n">scaler</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>

<span class="c1"># Now apply the transformations to the data:</span>
<span class="n">X_train</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
<span class="n">X_test</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="unsupervised-machine-learning-clustering">
<h2>Unsupervised Machine Learning - Clustering<a class="headerlink" href="#unsupervised-machine-learning-clustering" title="Permalink to this headline">¶</a></h2>
</div>
<div class="section" id="deep-learning">
<h2>Deep Learning<a class="headerlink" href="#deep-learning" title="Permalink to this headline">¶</a></h2>
</div>
<div class="section" id="machine-learning-ranking">
<h2>Machine Learning Ranking<a class="headerlink" href="#machine-learning-ranking" title="Permalink to this headline">¶</a></h2>
<p><a class="reference external" href="https://mlexplained.com/2019/05/27/learning-to-rank-explained-with-code/">https://mlexplained.com/2019/05/27/learning-to-rank-explained-with-code/</a> :</p>
<p>In supervised machine learning, the most common tasks are classification and regression.  Though these two tasks will get you fairly far, sometimes your problem cannot be formulated in this way. For example, suppose you wanted to build a newsfeed or a recommendation system. In these cases, you don’t just want to know the probability of a user clicking an article or buying an item; you want to be able to prioritize and order the articles/items to maximize your chances of getting a click or purchase.</p>
<p>Let’s take the example of ranking items for the newsfeed of a user. If all we cared about was clicks, then we could just train a model to predict whether a user will click on each item and rank them according to the click probability. However, we might care about more than just clicks; for instance, if the user clicks on an article but does not finish reading it, it might not be that interesting to them and we won’t want to recommend similar articles to them in the future.</p>
<p>This is where learning to rank comes in. Instead of using some proxy measure (e.g. the probability of a user clicking on an item), we directly train the model to rank the items.</p>
<p><a class="reference external" href="https://medium.com/&#64;nikhilbd/intuitive-explanation-of-learning-to-rank-and-ranknet-lambdarank-and-lambdamart-fe1e17fac418">https://medium.com/&#64;nikhilbd/intuitive-explanation-of-learning-to-rank-and-ranknet-lambdarank-and-lambdamart-fe1e17fac418</a> :</p>
<p>Learning to Rank (LTR) is a class of techniques that apply supervised machine learning (ML) to solve ranking problems. The main difference between LTR and traditional supervised ML is this:</p>
<ul class="simple">
<li><p>Traditional ML solves a prediction problem (classification or regression) on a single instance at a time. E.g. if you are doing spam detection on email, you will look at all the features associated with that email and classify it as spam or not. The aim of traditional ML is to come up with a class (spam or no-spam) or a single numerical score for that instance.</p></li>
<li><p>LTR solves a ranking problem on a list of items. The aim of LTR is to come up with optimal ordering of those items. As such, LTR doesn’t care much about the exact score that each item gets, but cares more about the relative ordering among all the items.</p></li>
</ul>
<p>The most common application of LTR is search engine ranking, but it’s useful anywhere you need to produce a ranked list of items.</p>
<p>There are many ranking algo. Here is a list: <a class="reference external" href="https://en.wikipedia.org/wiki/Learning_to_rank">https://en.wikipedia.org/wiki/Learning_to_rank</a></p>
<p>In RankNet, LambdaRank and LambdaMART techniques, ranking is transformed into a pairwise classification or regression problem. That means you look at pairs of items at a time, come up with the optimal ordering for that pair of items, and then use it to come up with the final ranking for all the results.</p>
<div class="section" id="ranknet">
<h3>RankNet<a class="headerlink" href="#ranknet" title="Permalink to this headline">¶</a></h3>
<p>The cost function for RankNet aims to minimize the number of inversions in ranking. Here an inversion means an incorrect order among a pair of results, i.e. when we rank a lower rated result above a higher rated result in a ranked list. RankNet optimizes the cost function using Stochastic Gradient Descent.</p>
<p>In RankNet the loss is agnostic to the actual ranking of the item. In other words, the loss is the same for any pair of items i, j regardless of whether i and j are ranked in 5th and 6th place or if they are in 1st and 200th place.</p>
</div>
<div class="section" id="lambdarank">
<h3>LambdaRank<a class="headerlink" href="#lambdarank" title="Permalink to this headline">¶</a></h3>
<p>LambdaRank addresses the problem of RankNet agnosticity.</p>
<p>During RankNet training procedure, you don’t need the costs, only need the gradients (λ) of the cost with respect to the model score. You can think of these gradients as little arrows attached to each document in the ranked list, indicating the direction we’d like those documents to move.</p>
<p>Further creaters found that scaling the gradients by the change in NDCG (discounted cumulative gain <a class="reference external" href="https://en.wikipedia.org/wiki/Discounted_cumulative_gain">https://en.wikipedia.org/wiki/Discounted_cumulative_gain</a>) found by swapping each pair of documents gave good results. The core idea of LambdaRank is to use this new cost function for training a RankNet. On experimental datasets, this shows both speed and accuracy improvements over the original RankNet.</p>
</div>
<div class="section" id="lambdamart">
<h3>LambdaMart<a class="headerlink" href="#lambdamart" title="Permalink to this headline">¶</a></h3>
<p>LambdaMART is the boosted tree version of LambdaRank.</p>
</div>
</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="Statistics_probabilities_Documentation.html" class="btn btn-neutral float-right" title="Statistics and Probabilities" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="Time_Series_inPython_Documentation.html" class="btn btn-neutral float-left" title="Time Series in Python" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2017, pmeu

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>