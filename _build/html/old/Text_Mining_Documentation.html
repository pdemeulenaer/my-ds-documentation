

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Text Mining in Python &mdash; Pyspark_Documentation 0.1 documentation</title>
  

  
  
  
  

  
  <script type="text/javascript" src="_static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript">
          var DOCUMENTATION_OPTIONS = {
              URL_ROOT:'./',
              VERSION:'0.1',
              LANGUAGE:'None',
              COLLAPSE_INDEX:false,
              FILE_SUFFIX:'.html',
              HAS_SOURCE:  true,
              SOURCELINK_SUFFIX: '.txt'
          };
      </script>
        <script type="text/javascript" src="_static/jquery.js"></script>
        <script type="text/javascript" src="_static/underscore.js"></script>
        <script type="text/javascript" src="_static/doctools.js"></script>
        <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    
    <script type="text/javascript" src="_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Deep Learning" href="DeepLearning_Documentation.html" />
    <link rel="prev" title="Pyspark" href="Pyspark_Documentation.html" />
    <link href="_static/style.css" rel="stylesheet" type="text/css">

</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="index.html" class="icon icon-home"> Pyspark_Documentation
          

          
          </a>

          
            
            
              <div class="version">
                0.1
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="Pandas_Documentation.html">Python, Jupyter and Pandas</a></li>
<li class="toctree-l1"><a class="reference internal" href="Sklearn_Documentation.html">Scikit-learn</a></li>
<li class="toctree-l1"><a class="reference internal" href="Pyspark_Documentation.html">Pyspark</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Text Mining in Python</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#libraries-and-useful-links">Libraries and useful links</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#basic-functions">Basic functions</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#intro-to-regular-expressions-regex">Intro to regular expressions (REGEX)</a></li>
<li class="toctree-l2"><a class="reference internal" href="#bag-of-words-bow">Bag of Words (BOW)</a></li>
<li class="toctree-l2"><a class="reference internal" href="#tf-idf-term-frequency-inverse-document-frequency">TF-IDF (Term Frequency - Inverse Document Frequency)</a></li>
<li class="toctree-l2"><a class="reference internal" href="#cosine-distance-cosine-similarity">Cosine distance, Cosine similarity</a></li>
<li class="toctree-l2"><a class="reference internal" href="#word2vec">Word2Vec</a></li>
<li class="toctree-l2"><a class="reference internal" href="#glove">GloVe</a></li>
<li class="toctree-l2"><a class="reference internal" href="#fasttext">FastText</a></li>
<li class="toctree-l2"><a class="reference internal" href="#bert-bidirectional-encoder-representation-from-transformers">BERT (Bidirectional Encoder Representation from Transformers)</a></li>
<li class="toctree-l2"><a class="reference internal" href="#chatbot">Chatbot</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="DeepLearning_Documentation.html">Deep Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="ReinforcementLearning_Documentation.html">Reinforcement Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="Time_Series_inPython_Documentation.html">Time Series in Python</a></li>
<li class="toctree-l1"><a class="reference internal" href="Algorithms_Documentation.html">Algorithms</a></li>
<li class="toctree-l1"><a class="reference internal" href="Statistics_probabilities_Documentation.html">Statistics and Probabilities</a></li>
<li class="toctree-l1"><a class="reference internal" href="Plot_libraries_Documentation.html">Great plot libraries</a></li>
<li class="toctree-l1"><a class="reference internal" href="SQL_Documentation.html">SQL Server documentation</a></li>
<li class="toctree-l1"><a class="reference internal" href="Bash_Documentation.html">Useful Bash commands (or batch)</a></li>
<li class="toctree-l1"><a class="reference internal" href="Bash_Documentation.html#useful-git-commands">Useful GIT commands</a></li>
<li class="toctree-l1"><a class="reference internal" href="Bash_Documentation.html#useful-vim-commands">Useful VIM commands</a></li>
<li class="toctree-l1"><a class="reference internal" href="Bash_Documentation.html#data-types">Data types</a></li>
<li class="toctree-l1"><a class="reference internal" href="Business_models_Documentation.html">Interesting business models</a></li>
<li class="toctree-l1"><a class="reference internal" href="Sphinx_stuff.html">Sphinx</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">Pyspark_Documentation</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="index.html">Docs</a> &raquo;</li>
        
      <li>Text Mining in Python</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="_sources/Text_Mining_Documentation.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="text-mining-in-python">
<h1>Text Mining in Python<a class="headerlink" href="#text-mining-in-python" title="Permalink to this headline">Â¶</a></h1>
<div class="section" id="libraries-and-useful-links">
<h2>Libraries and useful links<a class="headerlink" href="#libraries-and-useful-links" title="Permalink to this headline">Â¶</a></h2>
<div class="figure">
<a class="reference internal image-reference" href="_images/NLP_libraries.png"><img alt="NLP libraries" src="_images/NLP_libraries.png" style="width: 840.0px; height: 1221.5px;" /></a>
</div>
<p><a class="reference external" href="https://www.kdnuggets.com/2018/07/comparison-top-6-python-nlp-libraries.html">https://www.kdnuggets.com/2018/07/comparison-top-6-python-nlp-libraries.html</a></p>
<p>How to install NLTK behind proxy:</p>
<p>conda install nltk or pip install â€“proxy=https://p998phd:p998phd&#64;proxyvip-se.sbcore.net:8080 â€“trusted-host pypi.python.org -U nltk</p>
<p>then open jupyter notebook:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">nltk</span>
<span class="n">nltk</span><span class="o">.</span><span class="n">set_proxy</span><span class="p">(</span><span class="s1">&#39;https://p998phd:p998phd@proxyvip-se.sbcore.net:8080&#39;</span><span class="p">)</span>
<span class="n">nltk</span><span class="o">.</span><span class="n">download</span><span class="p">()</span>
</pre></div>
</div>
<div class="section" id="basic-functions">
<h3>Basic functions<a class="headerlink" href="#basic-functions" title="Permalink to this headline">Â¶</a></h3>
<p>Here are useful functions for cutting a sentence into words, getting the singular form, getting the root of each word:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">string</span>
<span class="kn">from</span> <span class="nn">nltk.tokenize</span> <span class="kn">import</span> <span class="n">RegexpTokenizer</span>

<span class="k">def</span> <span class="nf">splitToWords</span><span class="p">(</span><span class="n">stringOfWords</span><span class="p">):</span>
  <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">RegexpTokenizer</span><span class="p">(</span><span class="s2">&quot;[\w&#39;]+&quot;</span><span class="p">)</span>
  <span class="n">words</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">tokenize</span><span class="p">(</span><span class="n">stringOfWords</span><span class="p">)</span>
  <span class="n">words</span> <span class="o">=</span> <span class="n">lower_function</span><span class="p">(</span><span class="n">words</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">words</span>


<span class="k">def</span> <span class="nf">lower_function</span><span class="p">(</span><span class="n">list_input</span><span class="p">):</span>
  <span class="k">return</span> <span class="p">[</span><span class="n">x</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">list_input</span><span class="p">]</span>


<span class="k">def</span> <span class="nf">lemmatize_function</span><span class="p">(</span><span class="n">list_input</span><span class="p">):</span>
  <span class="n">wordnet_lemmatizer</span> <span class="o">=</span> <span class="n">WordNetLemmatizer</span><span class="p">()</span>
  <span class="k">return</span> <span class="p">[</span><span class="n">wordnet_lemmatizer</span><span class="o">.</span><span class="n">lemmatize</span><span class="p">(</span><span class="n">i</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">list_input</span><span class="p">]</span>


<span class="k">def</span> <span class="nf">stem_function</span><span class="p">(</span><span class="n">list_input</span><span class="p">):</span>
  <span class="n">snowball_stemmer</span> <span class="o">=</span> <span class="n">SnowballStemmer</span><span class="p">(</span><span class="s2">&quot;english&quot;</span><span class="p">)</span>
  <span class="c1">#lancaster_stemmer = LancasterStemmer()</span>
  <span class="k">return</span> <span class="p">[</span><span class="n">snowball_stemmer</span><span class="o">.</span><span class="n">stem</span><span class="p">(</span><span class="n">i</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">list_input</span><span class="p">]</span>


<span class="k">def</span> <span class="nf">remove_stopWords_function</span><span class="p">(</span><span class="n">list_input</span><span class="p">):</span>
  <span class="n">punctuation</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">string</span><span class="o">.</span><span class="n">punctuation</span><span class="p">)</span>
  <span class="n">stop</span> <span class="o">=</span> <span class="n">stopwords</span><span class="o">.</span><span class="n">words</span><span class="p">(</span><span class="s1">&#39;english&#39;</span><span class="p">)</span> <span class="o">+</span> <span class="n">punctuation</span>
  <span class="k">return</span> <span class="p">[</span><span class="n">term</span> <span class="k">for</span> <span class="n">term</span> <span class="ow">in</span> <span class="n">list_input</span> <span class="k">if</span> <span class="n">term</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">stop</span><span class="p">]</span>

<span class="c1">#INPUT</span>
<span class="n">text</span> <span class="o">=</span> <span class="s2">&quot;Nonsense?  kiss off, geek. what I said is true.  I&#39;ll have your account terminated.&quot;</span>

<span class="n">text</span> <span class="o">=</span> <span class="n">splitToWords</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>                 <span class="c1">#tokenizes (=splits in words)</span>
<span class="n">text</span> <span class="o">=</span> <span class="n">remove_stopWords_function</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>    <span class="c1">#Removes stopwords (&quot;the&quot;,...)</span>
<span class="n">text</span> <span class="o">=</span> <span class="n">lemmatize_function</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>           <span class="c1">#Lemmatiz = gets singular form of words when applicable</span>
<span class="n">text</span> <span class="o">=</span> <span class="n">stem_function</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>                <span class="c1">#Stemming = keeps root of words only</span>
<span class="k">print</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
</pre></div>
</div>
<p>Output: [â€˜nonsensâ€™, â€˜kissâ€™, â€˜geekâ€™, â€˜saidâ€™, â€˜trueâ€™, â€œiâ€™llâ€, â€˜accountâ€™, â€˜terminâ€™]</p>
<p>Another useful text cleaning function can be found here:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">standardize_text</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="n">text_field</span><span class="p">):</span>
  <span class="n">df</span><span class="p">[</span><span class="n">text_field</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="n">text_field</span><span class="p">]</span><span class="o">.</span><span class="n">str</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;http\S+&quot;</span><span class="p">,</span> <span class="s2">&quot;&quot;</span><span class="p">)</span>
  <span class="n">df</span><span class="p">[</span><span class="n">text_field</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="n">text_field</span><span class="p">]</span><span class="o">.</span><span class="n">str</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;http&quot;</span><span class="p">,</span> <span class="s2">&quot;&quot;</span><span class="p">)</span>
  <span class="n">df</span><span class="p">[</span><span class="n">text_field</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="n">text_field</span><span class="p">]</span><span class="o">.</span><span class="n">str</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;@\S+&quot;</span><span class="p">,</span> <span class="s2">&quot;&quot;</span><span class="p">)</span>
  <span class="n">df</span><span class="p">[</span><span class="n">text_field</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="n">text_field</span><span class="p">]</span><span class="o">.</span><span class="n">str</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;[^A-Za-z0-9(),!?@\&#39;\`</span><span class="se">\&quot;</span><span class="s2">\_\n]&quot;</span><span class="p">,</span> <span class="s2">&quot; &quot;</span><span class="p">)</span>
  <span class="n">df</span><span class="p">[</span><span class="n">text_field</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="n">text_field</span><span class="p">]</span><span class="o">.</span><span class="n">str</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;@&quot;</span><span class="p">,</span> <span class="s2">&quot;at&quot;</span><span class="p">)</span>
  <span class="n">df</span><span class="p">[</span><span class="n">text_field</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="n">text_field</span><span class="p">]</span><span class="o">.</span><span class="n">str</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span>
  <span class="k">return</span> <span class="n">df</span>
</pre></div>
</div>
<p>questions = standardize_text(df, â€œtextâ€)</p>
<p>taken from the excellent tutorial on topic classification: <a class="reference external" href="https://github.com/hundredblocks/concrete_NLP_tutorial/blob/master/NLP_notebook.ipynb">https://github.com/hundredblocks/concrete_NLP_tutorial/blob/master/NLP_notebook.ipynb</a>
Here for the blog: <a class="reference external" href="https://blog.insightdatascience.com/how-to-solve-90-of-nlp-problems-a-step-by-step-guide-fda605278e4e?lipi=urn%3Ali%3Apage%3Ad_flagship3_feed%3BqGDpQk2XQQ2DhR08PHkmqg%3D%3D">https://blog.insightdatascience.com/how-to-solve-90-of-nlp-problems-a-step-by-step-guide-fda605278e4e?lipi=urn%3Ali%3Apage%3Ad_flagship3_feed%3BqGDpQk2XQQ2DhR08PHkmqg%3D%3D</a></p>
<p>Other tokenizing, from DataCamp:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Import necessary modules</span>
<span class="kn">from</span> <span class="nn">nltk.tokenize</span> <span class="kn">import</span> <span class="n">sent_tokenize</span>
<span class="kn">from</span> <span class="nn">nltk.tokenize</span> <span class="kn">import</span> <span class="n">word_tokenize</span>

<span class="c1"># Split scene_one into sentences: sentences</span>
<span class="n">sentences</span> <span class="o">=</span> <span class="n">sent_tokenize</span><span class="p">(</span><span class="n">scene_one</span><span class="p">)</span>

<span class="c1"># Use word_tokenize to tokenize the fourth sentence: tokenized_sent</span>
<span class="n">tokenized_sent</span> <span class="o">=</span> <span class="n">word_tokenize</span><span class="p">(</span><span class="n">sentences</span><span class="p">[</span><span class="mi">3</span><span class="p">])</span>

<span class="c1"># Make a set of unique tokens in the entire scene: unique_tokens</span>
<span class="n">unique_tokens</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">word_tokenize</span><span class="p">(</span><span class="n">scene_one</span><span class="p">))</span>

<span class="c1"># Print the unique tokens result</span>
<span class="k">print</span><span class="p">(</span><span class="n">unique_tokens</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="intro-to-regular-expressions-regex">
<h2>Intro to regular expressions (REGEX)<a class="headerlink" href="#intro-to-regular-expressions-regex" title="Permalink to this headline">Â¶</a></h2>
<div class="figure">
<a class="reference internal image-reference" href="_images/Regex_table.png"><img alt="REGEXES" src="_images/Regex_table.png" style="width: 567.0px; height: 336.0px;" /></a>
</div>
<p>Examples of regex patterns:</p>
<p>pattern1 = râ€#w+â€
This says that we want to catch terms like â€˜#thingâ€™</p>
<p>pattern2 = râ€([#|&#64;]w+)â€
This says that we want to catch terms like â€˜#thingâ€™ or <a class="reference external" href="mailto:'&#37;&#52;&#48;thing">â€˜<span>&#64;</span>thing</a>â€™</p>
<p>Letâ€™s say we have some german text like this:</p>
<p>german_text = â€˜Wann gehen wir zum Pizza? ğŸ• Und fÃ¤hrst du mit Ãœber? ğŸš•â€™</p>
<p>1. We want to tokenize all words:
all_words = word_tokenize(german_text)
print(all_words)
Output: [â€˜Wannâ€™, â€˜gehenâ€™, â€˜wirâ€™, â€˜zumâ€™, â€˜Pizzaâ€™, â€˜?â€™, â€˜ğŸ•â€™, â€˜Undâ€™, â€˜fÃ¤hrstâ€™, â€˜duâ€™, â€˜mitâ€™, â€˜Ãœberâ€™, â€˜?â€™, â€˜ğŸš•â€™]</p>
<p>2. We want all words starting by a capital letter (including Ãœ!!!)
capital_words = râ€[A-ZÃœ]w+â€
print(regexp_tokenize(german_text,capital_words))
Output: [â€˜Wannâ€™, â€˜Pizzaâ€™, â€˜Undâ€™, â€˜Ãœberâ€™]</p>
<p>3. We want all symbols! For that we can use the list of them in the pattern:
# Tokenize and print only emoji
emoji = â€œ[â€˜U0001F300-U0001F5FFâ€™|â€™U0001F600-U0001F64Fâ€™|â€™U0001F680-U0001F6FFâ€™|â€™u2600-u26FFu2700-u27BFâ€™]â€
print(regexp_tokenize(german_text,emoji))
Output: [â€˜ğŸ•â€™, â€˜ğŸš•â€™]</p>
<p>So in theory we can capture anything.</p>
</div>
<div class="section" id="bag-of-words-bow">
<h2>Bag of Words (BOW)<a class="headerlink" href="#bag-of-words-bow" title="Permalink to this headline">Â¶</a></h2>
<p>The most primitive method. The bag-of-words is a representation of text that describes the occurrence of words within a document. It involves two things:</p>
<ul class="simple">
<li>A vocabulary of known words.</li>
<li>A measure of the presence of known words.</li>
</ul>
<p>Why is it is called a â€œbagâ€ of words? That is because any information about the order or structure of words in the document is discarded and the model is only concerned with whether the known words occur in the document, not where they occur in the document.</p>
<p>The intuition behind the Bag of Words is that documents are similar if they have similar content. Also, we can learn something about the meaning of the document from its content alone.</p>
<p>For example, if our dictionary contains the words {Learning, is, the, not, great}, and we want to vectorize the text â€œLearning is greatâ€, we would have the following vector: (1, 1, 0, 0, 1).</p>
<p>A problem with the Bag of Words approach is that highly frequent words start to dominate in the document (e.g. larger score), but may not contain as much â€œinformational contentâ€. Also, it will give more weight to longer documents than shorter documents.</p>
</div>
<div class="section" id="tf-idf-term-frequency-inverse-document-frequency">
<h2>TF-IDF (Term Frequency - Inverse Document Frequency)<a class="headerlink" href="#tf-idf-term-frequency-inverse-document-frequency" title="Permalink to this headline">Â¶</a></h2>
<p>A very good intro to TF-IDF: <a class="reference external" href="https://janav.wordpress.com/2013/10/27/tf-idf-and-cosine-similarity/">https://janav.wordpress.com/2013/10/27/tf-idf-and-cosine-similarity/</a></p>
<p>One approach is to rescale the frequency of words by how often they appear in all documents so that the scores for frequent words like â€œtheâ€ that are also frequent across all documents are penalized. This approach to scoring is called Term Frequency-Inverse Document Frequency, or TF-IDF for short, where:</p>
<p>Term Frequency: is a scoring of the frequency of the word in the current document.</p>
<p>TF = (Number of times term t appears in a document)/(Number of terms in the document)</p>
<p>Inverse Document Frequency: is a scoring of how rare the word is across documents.</p>
<p>IDF = 1+log(N/n), where, N is the number of documents and n is the number of documents a term t has appeared in.</p>
<p>Tf-idf weight is a weight often used in information retrieval and text mining. This weight is a statistical measure used to evaluate how important a word is to a document in a collection or corpus.</p>
<blockquote>
<div><p>Consider a document containing 100 words wherein the word â€˜phoneâ€™ appears 5 times.</p>
<p>The term frequency (i.e., tf) for phone is then (5 / 100) = 0.05. Now, assume we have 10 million documents and the word phone appears in one thousand of these. Then, the inverse document frequency (i.e., IDF) is calculated as log(10,000,000 / 1,000) = 4. Thus, the Tf-IDF weight is the product of these quantities: 0.05 * 4 = 0.20.</p>
</div></blockquote>
</div>
<div class="section" id="cosine-distance-cosine-similarity">
<h2>Cosine distance, Cosine similarity<a class="headerlink" href="#cosine-distance-cosine-similarity" title="Permalink to this headline">Â¶</a></h2>
<p>A very good intro to Cosine similarity: <a class="reference external" href="https://janav.wordpress.com/2013/10/27/tf-idf-and-cosine-similarity/">https://janav.wordpress.com/2013/10/27/tf-idf-and-cosine-similarity/</a></p>
<p>Cosine Similarity (d1, d2) =  Dot product(d1, d2) / ||d1|| * ||d2||</p>
<p>Dot product (d1,d2) = d1[0] * d2[0] + d1[1] * d2[1] * â€¦ * d1[n] * d2[n]</p>
<p>||d1|| = square root(d1[0]2 + d1[1]2 + â€¦ + d1[n]2)</p>
<p>||d2|| = square root(d2[0]2 + d2[1]2 + â€¦ + d2[n]2)</p>
<div class="figure">
<a class="reference internal image-reference" href="_images/Cosine_similarity.png"><img alt="Cosine similarity" src="_images/Cosine_similarity.png" style="width: 456.0px; height: 354.0px;" /></a>
</div>
<p>Note we can compute the similarity between words, but also between groups of words, i.e. sentences, documents.</p>
<p>A very straightforward application of the cosine similarity is in chatbots, where a query can be compared to a bunch of documents; the most similar document being selected;</p>
<p>Cosine Similarity(Query,Document1) = Dot product(Query, Document1) / ||Query|| * ||Document1||</p>
</div>
<div class="section" id="word2vec">
<h2>Word2Vec<a class="headerlink" href="#word2vec" title="Permalink to this headline">Â¶</a></h2>
<p>See <a class="reference external" href="https://towardsdatascience.com/word2vec-skip-gram-model-part-1-intuition-78614e4d6e0b">https://towardsdatascience.com/word2vec-skip-gram-model-part-1-intuition-78614e4d6e0b</a> which is a summary of more detailed posts:</p>
<p><a class="reference external" href="http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/">http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/</a></p>
<p><a class="reference external" href="http://mccormickml.com/2017/01/11/word2vec-tutorial-part-2-negative-sampling/">http://mccormickml.com/2017/01/11/word2vec-tutorial-part-2-negative-sampling/</a></p>
<p>The problem of word representation in numbers:</p>
<p>A traditional way of representing words is one-hot vector, which is essentially a vector with only one target element being 1 and the others being 0. The length of the vector is equal to the size of the total unique vocabulary in the corpora. Conventionally, these unique words are encoded in alphabetical order. Namely, you should expect the one-hot vectors for words starting with â€œaâ€ with target â€œ1â€ of lower index, while those for words beginning with â€œzâ€ with target â€œ1â€ of higher index.</p>
<div class="figure">
<a class="reference internal image-reference" href="_images/Word_OHE.png"><img alt="Word_OHE" src="_images/Word_OHE.png" style="width: 337.0px; height: 272.0px;" /></a>
</div>
<p>Word2Vec is an efficient solution to these problems, which leverages the context of the target words. Essentially, we want to use the surrounding words to represent the target words with a Neural Network whose hidden layer encodes the word representation.</p>
<p>Word2Vec is a technique to find continuous embeddings for words. It learns from reading massive amounts of text and memorizing which words tend to appear in similar contexts. After being trained on enough data, it generates a 300-dimension vector for each word in a vocabulary, with words of similar meaning being closer to each other.</p>
<p>Word2vec is a model that was pre-trained on a very large corpus, and provides embeddings that map words that are similar close to each other. A quick way to get a sentence embedding for our classifier, is to average word2vec scores of all words in our sentence.</p>
<p>There are two types of Word2Vec, Skip-gram and Continuous Bag of Words (CBOW). Given a corpus (set of sentences) we can imagine 2 tasks:</p>
<ul class="simple">
<li>Skip-gram: Loop on each word and try to predict its neighbors (=its context, +-N words around it)</li>
<li>CBOW: Loop on each word and use the context (+-N words around it) to predict the word</li>
</ul>
<p>Skip-Gram:</p>
<p>Letâ€™s imagine a sentence like this traditional one: â€œthe quick brown fox jumps over the lazy dogâ€. Here we use a window of size 2 as â€œcontextâ€ of a given word.
The idea is to train a simple neural network (not deep), with only one hidden layer with 300 neurons.
Then the procedure is this one: given a specific word in a sentence, look at words nearby (i.e. in the context), and pick one randomly. The network will tell us the probability for every word in our vocabulary of being the â€œnearby wordâ€ that we choose.</p>
<p>Example: input word: â€œSovietâ€. The output probability will be much higher for â€œUnionâ€ or â€œRussiaâ€ than for â€œWatermelonâ€.</p>
<p>So we will train the network by feeding the words pairs found in the corpus:</p>
<div class="figure">
<a class="reference internal image-reference" href="_images/Skip_Gram_1.png"><img alt="Skip_Gram_1" src="_images/Skip_Gram_1.png" style="width: 468.29999999999995px; height: 295.4px;" /></a>
</div>
<p>with such a network:</p>
<div class="figure">
<a class="reference internal image-reference" href="_images/Skip_Gram_2.png"><img alt="Skip_Gram_2" src="_images/Skip_Gram_2.png" style="width: 640.0px; height: 399.20000000000005px;" /></a>
</div>
<p>Say the vocabulary (each word in the corpus) has size 10000. So that means the input word â€œantsâ€ will be fed like a one-hot-encoded of size 10000 full of zeros and just one â€œ1â€.
The weights of the 300 neurons hidden layers, when trained, are (almost) the embeddings! The hidden layer is going to be represented by a weight matrix with 10,000 rows (one for every word in our vocabulary) and 300 columns (one for every hidden neuron, left on the next picture). If you look at the rows of this weight matrix (right, in the next picture), these are actually what will be our word vectors!</p>
<div class="figure">
<a class="reference internal image-reference" href="_images/Skip_Gram_3.png"><img alt="Skip_Gram_3" src="_images/Skip_Gram_3.png" style="width: 312.8px; height: 268.40000000000003px;" /></a>
</div>
<p>Problem: We need few additional modifications to the basic skip-gram model which are important for actually making it feasible to train. Running gradient descent on a neural network that large is going to be slow. And to make matters worse, you need a huge amount of training data in order to tune that many weights and avoid over-fitting. millions of weights times billions of training samples means that training this model is going to be a beast.</p>
<blockquote>
<div>The authors of Word2Vec addressed these issues in their second paper.</div></blockquote>
<p>There are three innovations in this second paper:</p>
<ul class="simple">
<li>Treating common word pairs or phrases as single â€œwordsâ€ in their model.</li>
<li>Subsampling frequent words to decrease the number of training examples.</li>
<li>Modifying the optimization objective with a technique they called â€œNegative Samplingâ€, which causes each training sample to update only a small percentage of the modelâ€™s weights.</li>
</ul>
<p>Itâ€™s worth noting that subsampling frequent words and applying Negative Sampling not only reduced the compute burden of the training process, but also improved the quality of their resulting word vectors as well.</p>
<p>Subsampling:</p>
<p>There are two â€œproblemsâ€ with common words like â€œtheâ€:</p>
<ul class="simple">
<li>When looking at word pairs, (â€œfoxâ€, â€œtheâ€) doesnâ€™t tell us much about the meaning of â€œfoxâ€. â€œtheâ€ appears in the context of pretty much every word.</li>
<li>We will have many more samples of (â€œtheâ€, â€¦) than we need to learn a good vector for â€œtheâ€.</li>
</ul>
<p>Word2Vec implements a â€œsubsamplingâ€ scheme to address this. For each word we encounter in our training text, there is a chance that we will effectively delete it from the text. The probability that we cut the word is related to the wordâ€™s frequency.</p>
<p>If we have a window size of 10, and we remove a specific instance of â€œtheâ€ from our text:</p>
<ul class="simple">
<li>As we train on the remaining words, â€œtheâ€ will not appear in any of their context windows.</li>
<li>Weâ€™ll have 10 fewer training samples where â€œtheâ€ is the input word.</li>
</ul>
<p>Negative Sampling:</p>
<p>As we discussed above, the size of our word vocabulary means that our skip-gram neural network has a tremendous number of weights, all of which would be updated slightly by every one of our billions of training samples!</p>
<p>Negative sampling addresses this by having each training sample only modify a small percentage of the weights, rather than all of them. Hereâ€™s how it works.</p>
<p>When training the network on the word pair (â€œfoxâ€, â€œquickâ€), recall that the â€œlabelâ€ or â€œcorrect outputâ€ of the network is a one-hot vector. That is, for the output neuron corresponding to â€œquickâ€ to output a 1, and for all of the other thousands of output neurons to output a 0.</p>
<p>With negative sampling, we are instead going to randomly select just a small number of â€œnegativeâ€ words (letâ€™s say 5) to update the weights for. (In this context, a â€œnegativeâ€ word is one for which we want the network to output a 0 for). We will also still update the weights for our â€œpositiveâ€ word (which is the word â€œquickâ€ in our current example).</p>
<p>The paper says that selecting 5â€“20 words works well for smaller datasets, and you can get away with only 2â€“5 words for large datasets.</p>
<p>Recall that the output layer of our model has a weight matrix thatâ€™s 300 x 10,000. So we will just be updating the weights for our positive word (â€œquickâ€), plus the weights for 5 other words that we want to output 0. Thatâ€™s a total of 6 output neurons, and 1,800 weight values total. Thatâ€™s only 0.06% of the 3M weights in the output layer!</p>
<p>In the hidden layer, only the weights for the input word are updated (this is true whether youâ€™re using Negative Sampling or not).</p>
<p>Essentially, the probability for selecting a word as a negative sample is related to its frequency, with more frequent words being more likely to be selected as negative samples.</p>
</div>
<div class="section" id="glove">
<h2>GloVe<a class="headerlink" href="#glove" title="Permalink to this headline">Â¶</a></h2>
</div>
<div class="section" id="fasttext">
<h2>FastText<a class="headerlink" href="#fasttext" title="Permalink to this headline">Â¶</a></h2>
</div>
<div class="section" id="bert-bidirectional-encoder-representation-from-transformers">
<h2>BERT (Bidirectional Encoder Representation from Transformers)<a class="headerlink" href="#bert-bidirectional-encoder-representation-from-transformers" title="Permalink to this headline">Â¶</a></h2>
</div>
<div class="section" id="chatbot">
<h2>Chatbot<a class="headerlink" href="#chatbot" title="Permalink to this headline">Â¶</a></h2>
<p>Great intro to chatbots, using TF-IDF and NLTK: <a class="reference external" href="https://medium.com/analytics-vidhya/building-a-simple-chatbot-in-python-using-nltk-7c8c8215ac6e">https://medium.com/analytics-vidhya/building-a-simple-chatbot-in-python-using-nltk-7c8c8215ac6e</a></p>
<p>Word2Vec could also be used, a quick way to get a sentence embedding for our classifier, is to average word2vec scores of all words in our sentence.</p>
</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="DeepLearning_Documentation.html" class="btn btn-neutral float-right" title="Deep Learning" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="Pyspark_Documentation.html" class="btn btn-neutral float-left" title="Pyspark" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2017, pmeu

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>