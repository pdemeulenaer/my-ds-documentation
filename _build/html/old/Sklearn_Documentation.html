

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Scikit-learn &mdash; Pyspark_Documentation 0.1 documentation</title>
  

  
  
  
  

  
  <script type="text/javascript" src="_static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript">
          var DOCUMENTATION_OPTIONS = {
              URL_ROOT:'./',
              VERSION:'0.1',
              LANGUAGE:'None',
              COLLAPSE_INDEX:false,
              FILE_SUFFIX:'.html',
              HAS_SOURCE:  true,
              SOURCELINK_SUFFIX: '.txt'
          };
      </script>
        <script type="text/javascript" src="_static/jquery.js"></script>
        <script type="text/javascript" src="_static/underscore.js"></script>
        <script type="text/javascript" src="_static/doctools.js"></script>
        <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    
    <script type="text/javascript" src="_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Pyspark" href="Pyspark_Documentation.html" />
    <link rel="prev" title="Python, Jupyter and Pandas" href="Pandas_Documentation.html" />
    <link href="_static/style.css" rel="stylesheet" type="text/css">

</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="index.html" class="icon icon-home"> Pyspark_Documentation
          

          
          </a>

          
            
            
              <div class="version">
                0.1
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="Pandas_Documentation.html">Python, Jupyter and Pandas</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Scikit-learn</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#basic-scikit-learn-documentation">Basic Scikit-learn documentation</a></li>
<li class="toctree-l2"><a class="reference internal" href="#data-preparation">Data Preparation</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#scaling-data">Scaling Data</a></li>
<li class="toctree-l3"><a class="reference internal" href="#encoding-of-data">Encoding of data</a></li>
<li class="toctree-l3"><a class="reference internal" href="#stratified-sampling">Stratified sampling</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#decision-tree">Decision Tree</a></li>
<li class="toctree-l2"><a class="reference internal" href="#random-forest">Random Forest</a></li>
<li class="toctree-l2"><a class="reference internal" href="#neural-networks">Neural Networks</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#introduction-to-keras-package">Introduction to Keras package</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#parameters-tuning">Parameters Tuning</a></li>
<li class="toctree-l2"><a class="reference internal" href="#how-to-measure-the-feature-importance-in-rf-or-other-algo">How to measure the Feature importance in RF (or other algo)</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#mean-decrease-accuracy-i-e-how-is-reduced-the-accuracy-if-we-permute-the-values-of-some-feature">Mean decrease accuracy (i.e. how is reduced the accuracy if we permute the values of some feature?)</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#score-explanation-for-individual-observation-using-lime">Score explanation for individual observation (using LIME)</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#classification-examples">Classification examples</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#score-explanation-for-individual-observation-using-tree-interpreter">Score explanation for individual observation (using tree-interpreter)</a></li>
<li class="toctree-l2"><a class="reference internal" href="#very-important-update-in-the-field-of-interpretation">Very important update in the field of interpretation:</a></li>
<li class="toctree-l2"><a class="reference internal" href="#ensemble-classification">Ensemble Classification</a></li>
<li class="toctree-l2"><a class="reference internal" href="#clustering">Clustering</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#gaussian-mixture-model-gmm">Gaussian Mixture Model (GMM)</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="Pyspark_Documentation.html">Pyspark</a></li>
<li class="toctree-l1"><a class="reference internal" href="Text_Mining_Documentation.html">Text Mining in Python</a></li>
<li class="toctree-l1"><a class="reference internal" href="DeepLearning_Documentation.html">Deep Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="ReinforcementLearning_Documentation.html">Reinforcement Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="Time_Series_inPython_Documentation.html">Time Series in Python</a></li>
<li class="toctree-l1"><a class="reference internal" href="Algorithms_Documentation.html">Algorithms</a></li>
<li class="toctree-l1"><a class="reference internal" href="Statistics_probabilities_Documentation.html">Statistics and Probabilities</a></li>
<li class="toctree-l1"><a class="reference internal" href="Plot_libraries_Documentation.html">Great plot libraries</a></li>
<li class="toctree-l1"><a class="reference internal" href="SQL_Documentation.html">SQL Server documentation</a></li>
<li class="toctree-l1"><a class="reference internal" href="Bash_Documentation.html">Useful Bash commands (or batch)</a></li>
<li class="toctree-l1"><a class="reference internal" href="Bash_Documentation.html#useful-git-commands">Useful GIT commands</a></li>
<li class="toctree-l1"><a class="reference internal" href="Bash_Documentation.html#useful-vim-commands">Useful VIM commands</a></li>
<li class="toctree-l1"><a class="reference internal" href="Bash_Documentation.html#data-types">Data types</a></li>
<li class="toctree-l1"><a class="reference internal" href="Business_models_Documentation.html">Interesting business models</a></li>
<li class="toctree-l1"><a class="reference internal" href="Sphinx_stuff.html">Sphinx</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">Pyspark_Documentation</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="index.html">Docs</a> &raquo;</li>
        
      <li>Scikit-learn</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="_sources/Sklearn_Documentation.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="scikit-learn">
<h1>Scikit-learn<a class="headerlink" href="#scikit-learn" title="Permalink to this headline">¶</a></h1>
<div class="section" id="basic-scikit-learn-documentation">
<h2>Basic Scikit-learn documentation<a class="headerlink" href="#basic-scikit-learn-documentation" title="Permalink to this headline">¶</a></h2>
<div class="topic">
<p class="topic-title first">Introduction</p>
<p>The objective here is to have everything useful for the projects, not to make a complete documentation of the whole package. Here I will try to document both version 1.6 and &gt;2.0. A special enphase will be done on machine learning module ml (mllib is outdated).</p>
</div>
<p>Scikit-learn (<a class="reference external" href="http://scikit-learn.org/stable/">http://scikit-learn.org/stable/</a>) is the Machine Learning well-known library in python. Here is a very short summary from DataCamp:</p>
<div class="figure" id="id1">
<a class="reference internal image-reference" href="_images/Sklearn_Cheat_Sheet.png"><img alt="map to buried treasure" src="_images/Sklearn_Cheat_Sheet.png" style="width: 1403.0px; height: 992.0px;" /></a>
<p class="caption"><span class="caption-text">This Cheatsheet is taken from DataCamp.</span></p>
</div>
</div>
<div class="section" id="data-preparation">
<h2>Data Preparation<a class="headerlink" href="#data-preparation" title="Permalink to this headline">¶</a></h2>
<p>Many algorithms need to have the data normalized or scaled first.</p>
<div class="section" id="scaling-data">
<h3>Scaling Data<a class="headerlink" href="#scaling-data" title="Permalink to this headline">¶</a></h3>
<p>MinMax scaler: get the data in the [0,1] range (can be adapted to [-1,1] or other). Very sensitive to outliers.</p>
<p><a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html">https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html</a></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">StandardScaler</span>

<span class="n">scaler</span> <span class="o">=</span> <span class="n">MinMaxScaler</span><span class="p">()</span>

<span class="c1"># Fit only to the training data</span>
<span class="n">scaler</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>

<span class="c1"># Now apply the transformations to the data:</span>
<span class="n">X_train</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
<span class="n">X_test</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
</pre></div>
</div>
<p>To get back the original scale, use scaler.inverse_transform() function.</p>
<p>Standard scaler: we substract the mean of data and divide by the standard deviation. (we then get the so-called Z values):</p>
<p><a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html">https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html</a></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">StandardScaler</span>

<span class="n">scaler</span> <span class="o">=</span> <span class="n">StandardScaler</span><span class="p">()</span>

<span class="c1"># Fit only to the training data</span>
<span class="n">scaler</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>

<span class="c1"># Now apply the transformations to the data:</span>
<span class="n">X_train</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
<span class="n">X_test</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
</pre></div>
</div>
<p>Standard scaler can still be rather sensitive to outliers. Substracting by the median instead of the mean, and dividing by the inter quantile is much more robust to outliers:</p>
<p><a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.RobustScaler.html">https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.RobustScaler.html</a></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">RobustScaler</span>

<span class="n">scaler</span> <span class="o">=</span> <span class="n">RobustScaler</span><span class="p">()</span>

<span class="c1"># Fit only to the training data</span>
<span class="n">scaler</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>

<span class="c1"># Now apply the transformations to the data:</span>
<span class="n">X_train</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
<span class="n">X_test</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="encoding-of-data">
<h3>Encoding of data<a class="headerlink" href="#encoding-of-data" title="Permalink to this headline">¶</a></h3>
<p>Here is the simple Label encoding. It is usually used to change strings (but also integers) in integer label</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">LabelEncoder</span><span class="p">,</span> <span class="n">OneHotEncoder</span>
<span class="n">data</span><span class="p">[</span><span class="s1">&#39;col_encoded&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">LabelEncoder</span><span class="p">()</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="s1">&#39;col&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">str</span><span class="p">))</span>

<span class="c1">#Also possible from Pandas:</span>
<span class="n">data</span><span class="p">[</span><span class="s1">&#39;TRANSTYPE_label&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">Categorical</span><span class="o">.</span><span class="n">from_array</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="s1">&#39;TRANSTYPE&#39;</span><span class="p">])</span><span class="o">.</span><span class="n">labels</span>
<span class="n">data</span><span class="p">[[</span><span class="s1">&#39;TRANSTYPE&#39;</span><span class="p">,</span><span class="s1">&#39;TRANSTYPE_label&#39;</span><span class="p">]]</span>
</pre></div>
</div>
<div class="figure">
<a class="reference internal image-reference" href="_images/Encoding_pandas.png"><img alt="map to buried treasure" src="_images/Encoding_pandas.png" style="width: 175.79999999999998px; height: 265.8px;" /></a>
</div>
<p>A maybe more general way to label encode several features in one shot (and save the encoders!) would be:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">list_encoders</span><span class="o">=</span><span class="p">[]</span>
<span class="k">for</span> <span class="n">column</span> <span class="ow">in</span> <span class="n">data</span><span class="p">[</span><span class="n">cat_cols</span><span class="p">]:</span>
  <span class="n">le</span> <span class="o">=</span> <span class="n">preprocessing</span><span class="o">.</span><span class="n">LabelEncoder</span><span class="p">()</span>
  <span class="n">data</span><span class="p">[</span><span class="n">column</span> <span class="o">+</span> <span class="s2">&quot;_encoded&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">le</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="n">column</span><span class="p">]</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">str</span><span class="p">))</span>
  <span class="n">list_encoders</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">le</span><span class="p">)</span>

<span class="c1"># Then the lists of columns to encode and associated encoders (already fitted!) are available in a python dictionary:</span>
<span class="n">dico_encoders</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;columns_to_encode&#39;</span><span class="p">:</span><span class="n">cat_cols</span><span class="p">,</span>
               <span class="s1">&#39;list_encoders&#39;</span>    <span class="p">:</span><span class="n">list_encoders</span> <span class="p">}</span>

<span class="c1"># ...so that the dictionary can be reused later. Let&#39;s say we have new data with the same features</span>
<span class="n">le_test</span> <span class="o">=</span> <span class="n">dico_encoders</span><span class="p">[</span><span class="s1">&#39;list_encoders&#39;</span><span class="p">]</span>
<span class="n">data</span><span class="p">[</span><span class="s1">&#39;TRANSTYPE_encoded&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">le_test</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="s1">&#39;TRANSTYPE&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">str</span><span class="p">))</span>

<span class="c1"># We can also convert back to the original feature:</span>
<span class="n">data</span><span class="p">[</span><span class="s1">&#39;TRANSTYPE_back&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">le_test</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">inverse_transform</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="s1">&#39;TRANSTYPE_encoded&#39;</span><span class="p">])</span>
</pre></div>
</div>
<p>Big question: when to use Label encoding, and when to use One Hot Encoding? See <a class="reference external" href="https://datascience.stackexchange.com/questions/9443/when-to-use-one-hot-encoding-vs-labelencoder-vs-dictvectorizor">https://datascience.stackexchange.com/questions/9443/when-to-use-one-hot-encoding-vs-labelencoder-vs-dictvectorizor</a>
In essence, Label Encoding is ok when working with tree methods</p>
</div>
<div class="section" id="stratified-sampling">
<h3>Stratified sampling<a class="headerlink" href="#stratified-sampling" title="Permalink to this headline">¶</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_classification</span>
<span class="kn">from</span> <span class="nn">sklearn.cross_validation</span> <span class="kn">import</span> <span class="n">StratifiedShuffleSplit</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">accuracy_score</span><span class="p">,</span> <span class="n">f1_score</span><span class="p">,</span> <span class="n">precision_score</span><span class="p">,</span> <span class="n">recall_score</span><span class="p">,</span> <span class="n">classification_report</span><span class="p">,</span> <span class="n">confusion_matrix</span>

<span class="c1"># We use a utility to generate artificial classification data.</span>
<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">make_classification</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">n_informative</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">n_classes</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="n">sss</span> <span class="o">=</span> <span class="n">StratifiedShuffleSplit</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">n_iter</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="k">for</span> <span class="n">train_idx</span><span class="p">,</span> <span class="n">test_idx</span> <span class="ow">in</span> <span class="n">sss</span><span class="p">:</span>
    <span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="n">train_idx</span><span class="p">],</span> <span class="n">X</span><span class="p">[</span><span class="n">test_idx</span><span class="p">],</span> <span class="n">y</span><span class="p">[</span><span class="n">train_idx</span><span class="p">],</span> <span class="n">y</span><span class="p">[</span><span class="n">test_idx</span><span class="p">]</span>
    <span class="n">svc</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
    <span class="n">y_pred</span> <span class="o">=</span> <span class="n">svc</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
        <span class="c1">#macro, micro are defined here http://stackoverflow.com/questions/31421413/how-to-compute-precision-recall-accuracy-and-f1-score-for-the-multiclass-case</span>
        <span class="c1">#Macro: Take the average of the f1-score for each class macro</span>
        <span class="c1">#Micro: Compute the f1-score using the global count of true positives / false negatives, etc. (you sum the number of true positives / false negatives for each class) . accuracy_score gives the same!</span>
    <span class="k">print</span><span class="p">(</span><span class="n">f1_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">average</span><span class="o">=</span><span class="s2">&quot;macro&quot;</span><span class="p">))</span>
    <span class="k">print</span><span class="p">(</span><span class="n">precision_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">average</span><span class="o">=</span><span class="s2">&quot;macro&quot;</span><span class="p">))</span>
    <span class="k">print</span><span class="p">(</span><span class="n">recall_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">average</span><span class="o">=</span><span class="s2">&quot;macro&quot;</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="decision-tree">
<span id="rf-sklearn-label"></span><h2>Decision Tree<a class="headerlink" href="#decision-tree" title="Permalink to this headline">¶</a></h2>
<p>Here is a simple example of DT for the Iris dataset:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_iris</span>
<span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">tree</span>
<span class="n">iris</span> <span class="o">=</span> <span class="n">load_iris</span><span class="p">()</span>
<span class="n">clf</span> <span class="o">=</span> <span class="n">tree</span><span class="o">.</span><span class="n">DecisionTreeClassifier</span><span class="p">()</span>
<span class="n">clf</span> <span class="o">=</span> <span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">iris</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">iris</span><span class="o">.</span><span class="n">target</span><span class="p">)</span>
</pre></div>
</div>
<p>We can use a vizualization tool, graphviz, to print the decision tree. It is sometimes useful to understand the classification of particular observation.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">graphviz</span>
<span class="n">dot_data</span> <span class="o">=</span> <span class="n">tree</span><span class="o">.</span><span class="n">export_graphviz</span><span class="p">(</span><span class="n">clf</span><span class="p">,</span> <span class="n">out_file</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
                       <span class="n">feature_names</span><span class="o">=</span><span class="n">iris</span><span class="o">.</span><span class="n">feature_names</span><span class="p">,</span>
                       <span class="n">class_names</span><span class="o">=</span><span class="n">iris</span><span class="o">.</span><span class="n">target_names</span><span class="p">,</span>
                       <span class="n">filled</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">rounded</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
                       <span class="n">special_characters</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">graph</span> <span class="o">=</span> <span class="n">graphviz</span><span class="o">.</span><span class="n">Source</span><span class="p">(</span><span class="n">dot_data</span><span class="p">)</span>
<span class="n">graph</span>
</pre></div>
</div>
<div class="figure">
<a class="reference internal image-reference" href="_images/Graphviz_tree.png"><img alt="map to buried treasure" src="_images/Graphviz_tree.png" style="width: 804.6px; height: 555.6px;" /></a>
</div>
</div>
<div class="section" id="random-forest">
<h2>Random Forest<a class="headerlink" href="#random-forest" title="Permalink to this headline">¶</a></h2>
<p>The RF algo applied on the iris dataset. For the algorithm description see <a class="reference internal" href="Algorithms_Documentation.html#rf-algo-label"><span class="std std-ref">Random Forest</span></a></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="kn">as</span> <span class="nn">pd</span>
<span class="kn">from</span> <span class="nn">IPython.core.pylabtools</span> <span class="kn">import</span> <span class="n">figsize</span>
<span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">import</span> <span class="nn">pylab</span>
<span class="kn">from</span> <span class="nn">pylab</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">import</span> <span class="nn">matplotlib.cm</span> <span class="kn">as</span> <span class="nn">cm</span>
<span class="kn">import</span> <span class="nn">matplotlib.mlab</span> <span class="kn">as</span> <span class="nn">mlab</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="kn">as</span> <span class="nn">sns</span>

<span class="c1">#Import of SKLEARN packages</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">accuracy_score</span><span class="p">,</span> <span class="n">roc_curve</span><span class="p">,</span> <span class="n">auc</span><span class="p">,</span> <span class="n">confusion_matrix</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">RandomForestClassifier</span>
<span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_iris</span>

<span class="c1">#LOADING IRIS DATASET:</span>
<span class="n">iris</span> <span class="o">=</span> <span class="n">load_iris</span><span class="p">()</span>                            <span class="c1">#The Iris dataset is available through the scikit-learn API</span>
<span class="n">idx</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">iris</span><span class="o">.</span><span class="n">target</span><span class="p">)))</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="n">idx</span><span class="p">)</span>                   <span class="c1">#We shuffle it (important if we want to split in train and test sets)</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">iris</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">iris</span><span class="o">.</span><span class="n">target</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>

<span class="c1"># Load data in Pandas dataFrame and then in a Pyspark dataframe</span>
<span class="n">data_pd</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">column_stack</span><span class="p">((</span><span class="n">X</span><span class="p">,</span><span class="n">y</span><span class="p">)),</span> <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;sepal_length&#39;</span><span class="p">,</span> <span class="s1">&#39;sepal_width&#39;</span><span class="p">,</span> <span class="s1">&#39;petal_length&#39;</span><span class="p">,</span> <span class="s1">&#39;petal_width&#39;</span><span class="p">,</span> <span class="s1">&#39;label&#39;</span><span class="p">])</span>
<span class="n">data_pd</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">data_pd</span><span class="p">[</span><span class="s1">&#39;label&#39;</span><span class="p">]</span><span class="o">==</span><span class="mi">0</span><span class="p">,</span><span class="s1">&#39;species&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="s1">&#39;setosa&#39;</span>
<span class="n">data_pd</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">data_pd</span><span class="p">[</span><span class="s1">&#39;label&#39;</span><span class="p">]</span><span class="o">==</span><span class="mi">1</span><span class="p">,</span><span class="s1">&#39;species&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="s1">&#39;versicolor&#39;</span>
<span class="n">data_pd</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">data_pd</span><span class="p">[</span><span class="s1">&#39;label&#39;</span><span class="p">]</span><span class="o">==</span><span class="mi">2</span><span class="p">,</span><span class="s1">&#39;species&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="s1">&#39;virginica&#39;</span>
<span class="n">data_pd</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
<div class="figure">
<a class="reference internal image-reference" href="_images/Iris_Dataset.png"><img alt="Iris dataset" src="_images/Iris_Dataset.png" style="width: 429.0px; height: 153.0px;" /></a>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># A VERY SMALL EDA</span>

<span class="n">g</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">PairGrid</span><span class="p">(</span><span class="n">data_pd</span><span class="p">,</span> <span class="nb">vars</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;sepal_length&#39;</span><span class="p">,</span> <span class="s1">&#39;sepal_width&#39;</span><span class="p">,</span> <span class="s1">&#39;petal_length&#39;</span><span class="p">,</span> <span class="s1">&#39;petal_width&#39;</span><span class="p">],</span> <span class="n">hue</span><span class="o">=</span><span class="s2">&quot;label&quot;</span><span class="p">)</span>
<span class="n">g</span><span class="o">.</span><span class="n">map_diag</span><span class="p">(</span><span class="n">sns</span><span class="o">.</span><span class="n">kdeplot</span><span class="p">)</span>
<span class="n">g</span><span class="o">.</span><span class="n">map_lower</span><span class="p">(</span><span class="n">sns</span><span class="o">.</span><span class="n">kdeplot</span><span class="p">)</span>
<span class="n">g</span><span class="o">.</span><span class="n">map_upper</span><span class="p">(</span><span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">)</span>
</pre></div>
</div>
<div class="figure">
<a class="reference internal image-reference" href="_images/Iris_Seaborn.png"><img alt="Iris with seaborn" src="_images/Iris_Seaborn.png" style="width: 668.0px; height: 679.0px;" /></a>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">sns</span><span class="o">.</span><span class="n">boxplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="s1">&#39;species&#39;</span><span class="p">,</span><span class="n">y</span><span class="o">=</span><span class="s1">&#39;petal_length&#39;</span><span class="p">,</span><span class="n">data</span><span class="o">=</span><span class="n">data_pd</span><span class="p">)</span>
</pre></div>
</div>
<div class="figure">
<a class="reference internal image-reference" href="_images/Iris_boxplot.png"><img alt="Iris with seaborn's boxplot" src="_images/Iris_boxplot.png" style="width: 464.0px; height: 327.0px;" /></a>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1">#FEATURES SELECTION</span>
<span class="n">feature_cols</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;sepal_length&#39;</span><span class="p">,</span> <span class="s1">&#39;sepal_width&#39;</span><span class="p">,</span> <span class="s1">&#39;petal_length&#39;</span><span class="p">,</span> <span class="s1">&#39;petal_width&#39;</span><span class="p">]</span>
<span class="n">target</span>       <span class="o">=</span> <span class="s1">&#39;label&#39;</span>

<span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="n">clf_name</span><span class="p">,</span> <span class="n">clf</span><span class="p">,</span> <span class="n">df_used</span><span class="p">,</span> <span class="n">feature_cols</span><span class="p">,</span> <span class="n">target</span><span class="p">):</span>

      <span class="n">X</span> <span class="o">=</span> <span class="n">df_used</span><span class="p">[</span><span class="n">feature_cols</span><span class="p">]</span><span class="o">.</span><span class="n">values</span>
      <span class="n">y</span> <span class="o">=</span> <span class="n">df_used</span><span class="p">[</span><span class="n">target</span><span class="p">]</span><span class="o">.</span><span class="n">values</span>

      <span class="c1"># CREATION OF TRAIN-TEST SETS</span>
      <span class="n">x_train</span><span class="p">,</span> <span class="n">x_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">y</span><span class="p">,</span><span class="n">train_size</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span> <span class="n">stratify</span><span class="o">=</span><span class="n">y</span><span class="p">)</span> <span class="c1">#stratify=y ensures that the same proportion of labels are in both train and test sets!</span>

      <span class="c1"># THE FIT ON THE TRAINING SET</span>
      <span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

      <span class="c1"># THE CLASSIFICATION</span>
      <span class="n">y_pred</span> <span class="o">=</span> <span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">x_test</span><span class="p">)</span>

      <span class="c1"># EVALUATION OF THE ACCURACY</span>
      <span class="n">accuracy</span> <span class="o">=</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">normalize</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">sample_weight</span><span class="o">=</span><span class="bp">None</span><span class="p">)</span>
      <span class="k">print</span><span class="p">(</span><span class="s1">&#39;Results with the classifier: &#39;</span><span class="p">,</span> <span class="n">clf_name</span><span class="o">.</span><span class="n">upper</span><span class="p">())</span>
      <span class="k">print</span><span class="p">(</span><span class="s1">&#39;Accuracy derived (1=100%): &#39;</span><span class="p">,</span> <span class="n">accuracy</span><span class="p">)</span>

      <span class="k">return</span> <span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span>

<span class="n">clf_name</span> <span class="o">=</span> <span class="s1">&#39;Random Forest&#39;</span>
<span class="n">clf</span> <span class="o">=</span> <span class="n">RandomForestClassifier</span><span class="p">(</span><span class="c1">#max_depth=50,</span>
                           <span class="n">n_estimators</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
                           <span class="n">max_features</span><span class="o">=</span><span class="s1">&#39;auto&#39;</span><span class="p">,</span>
                           <span class="n">criterion</span><span class="o">=</span><span class="s1">&#39;gini&#39;</span><span class="p">,</span><span class="c1">#&#39;entropy&#39;,</span>
                           <span class="n">class_weight</span><span class="o">=</span><span class="s1">&#39;balanced&#39;</span><span class="p">,</span>
                           <span class="n">bootstrap</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
                           <span class="n">random_state</span><span class="o">=</span><span class="mi">21</span><span class="p">,</span>
                           <span class="n">n_jobs</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span> <span class="c1">#n_jobs=-1 uses all available cores!!!</span>

<span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span> <span class="o">=</span> <span class="n">train</span><span class="p">(</span><span class="n">clf_name</span><span class="p">,</span> <span class="n">clf</span><span class="p">,</span> <span class="n">data_pd</span><span class="p">,</span> <span class="n">feature_cols</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>

<span class="c1">#RANKING OF VARIABLES (available only for Random Forest)</span>
<span class="k">print</span><span class="p">(</span><span class="s2">&quot;Ranking of variables from Random Forest:&quot;</span><span class="p">)</span>
<span class="n">feature_importance_index_sorted</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">clf</span><span class="o">.</span><span class="n">feature_importances_</span><span class="p">)[::</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
<span class="k">for</span> <span class="n">jj</span> <span class="ow">in</span> <span class="n">feature_importance_index_sorted</span><span class="p">:</span>
  <span class="k">print</span><span class="p">(</span><span class="n">feature_cols</span><span class="p">[</span><span class="n">jj</span><span class="p">],</span><span class="n">np</span><span class="o">.</span><span class="n">around</span><span class="p">(</span><span class="n">clf</span><span class="o">.</span><span class="n">feature_importances_</span><span class="p">[</span><span class="n">jj</span><span class="p">],</span><span class="n">decimals</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span><span class="o">*</span><span class="mi">100</span><span class="p">,</span><span class="s1">&#39;%&#39;</span><span class="p">)</span>

<span class="c1"># Accuracy and Confusion Matrix</span>
<span class="n">accuracy</span> <span class="o">=</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s1">&#39;Accuracy = &#39;</span><span class="p">,</span><span class="n">accuracy</span><span class="p">)</span>
<span class="c1">#print &#39;ROC_AUC  = &#39;, roc_auc</span>
<span class="k">print</span><span class="p">(</span><span class="s1">&#39;Confusion matrix:&#39;</span><span class="p">)</span>
<span class="n">C</span> <span class="o">=</span> <span class="n">confusion_matrix</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
<span class="n">C_normalized</span> <span class="o">=</span> <span class="n">C</span> <span class="o">/</span> <span class="n">C</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
<span class="c1">#print C_normalized</span>

<span class="n">Classes</span>           <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;setosa&#39;</span><span class="p">,</span><span class="s1">&#39;versicolor&#39;</span><span class="p">,</span><span class="s1">&#39;virginica&#39;</span><span class="p">]</span>
<span class="n">C_normalized_pd</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">C_normalized</span><span class="p">,</span><span class="n">columns</span><span class="o">=</span><span class="n">Classes</span><span class="p">,</span><span class="n">index</span><span class="o">=</span><span class="n">Classes</span><span class="p">)</span>
<span class="n">C_normalized_pd</span>
</pre></div>
</div>
<div class="figure">
<a class="reference internal image-reference" href="_images/Accuracy_and_ConfMat.png"><img alt="Accuracy and Confusion matrix" src="_images/Accuracy_and_ConfMat.png" style="width: 257.0px; height: 160.0px;" /></a>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># SENSITIVITY COMPUTATION (taken from https://stackoverflow.com/questions/31324218/scikit-learn-how-to-obtain-true-positive-true-negative-false-positive-and-fal)</span>
<span class="n">FP</span> <span class="o">=</span> <span class="n">C_normalized_pd</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">C_normalized_pd</span><span class="p">)</span>
<span class="n">FN</span> <span class="o">=</span> <span class="n">C_normalized_pd</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">C_normalized_pd</span><span class="p">)</span>
<span class="n">TP</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">C_normalized_pd</span><span class="p">)</span>
<span class="n">TN</span> <span class="o">=</span> <span class="n">C_normalized_pd</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">-</span> <span class="p">(</span><span class="n">FP</span> <span class="o">+</span> <span class="n">FN</span> <span class="o">+</span> <span class="n">TP</span><span class="p">)</span>

<span class="c1"># Sensitivity, hit rate, recall, or true positive rate</span>
<span class="n">TPR</span> <span class="o">=</span> <span class="n">TP</span><span class="o">/</span><span class="p">(</span><span class="n">TP</span><span class="o">+</span><span class="n">FN</span><span class="p">)</span>
<span class="c1"># Specificity or true negative rate</span>
<span class="n">TNR</span> <span class="o">=</span> <span class="n">TN</span><span class="o">/</span><span class="p">(</span><span class="n">TN</span><span class="o">+</span><span class="n">FP</span><span class="p">)</span>
<span class="c1"># Precision or positive predictive value</span>
<span class="n">PPV</span> <span class="o">=</span> <span class="n">TP</span><span class="o">/</span><span class="p">(</span><span class="n">TP</span><span class="o">+</span><span class="n">FP</span><span class="p">)</span>
<span class="c1"># Negative predictive value</span>
<span class="n">NPV</span> <span class="o">=</span> <span class="n">TN</span><span class="o">/</span><span class="p">(</span><span class="n">TN</span><span class="o">+</span><span class="n">FN</span><span class="p">)</span>
<span class="c1"># Fall out or false positive rate</span>
<span class="n">FPR</span> <span class="o">=</span> <span class="n">FP</span><span class="o">/</span><span class="p">(</span><span class="n">FP</span><span class="o">+</span><span class="n">TN</span><span class="p">)</span>
<span class="c1"># False negative rate</span>
<span class="n">FNR</span> <span class="o">=</span> <span class="n">FN</span><span class="o">/</span><span class="p">(</span><span class="n">TP</span><span class="o">+</span><span class="n">FN</span><span class="p">)</span>
<span class="c1"># False discovery rate</span>
<span class="n">FDR</span> <span class="o">=</span> <span class="n">FP</span><span class="o">/</span><span class="p">(</span><span class="n">TP</span><span class="o">+</span><span class="n">FP</span><span class="p">)</span>

<span class="c1"># Overall accuracy</span>
<span class="n">ACC</span> <span class="o">=</span> <span class="p">(</span><span class="n">TP</span><span class="o">+</span><span class="n">TN</span><span class="p">)</span><span class="o">/</span><span class="p">(</span><span class="n">TP</span><span class="o">+</span><span class="n">FP</span><span class="o">+</span><span class="n">FN</span><span class="o">+</span><span class="n">TN</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="s1">&#39;SENSITIVITY:&#39;</span><span class="p">)</span>
<span class="n">Sensitivity_pd</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">Classes</span><span class="p">,</span><span class="n">TPR</span><span class="p">)),</span><span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;species&#39;</span><span class="p">,</span><span class="s1">&#39;Sensitivity&#39;</span><span class="p">])</span>
<span class="n">Sensitivity_pd</span>
</pre></div>
</div>
<div class="figure">
<a class="reference internal image-reference" href="_images/Sensitivity.png"><img alt="Sensitivity" src="_images/Sensitivity.png" style="width: 157.0px; height: 110.0px;" /></a>
</div>
</div>
<div class="section" id="neural-networks">
<h2>Neural Networks<a class="headerlink" href="#neural-networks" title="Permalink to this headline">¶</a></h2>
<div class="section" id="introduction-to-keras-package">
<h3>Introduction to Keras package<a class="headerlink" href="#introduction-to-keras-package" title="Permalink to this headline">¶</a></h3>
<p>Keras (<a class="reference external" href="https://keras.io/">https://keras.io/</a>) is a python library that works ontop of the Neural Network Theano and TensorFlow libraries. Here is a very short summary from DataCamp:</p>
<div class="figure" id="id2">
<a class="reference internal image-reference" href="_images/Keras_Cheat_Sheet_Python.png"><img alt="map to buried treasure" src="_images/Keras_Cheat_Sheet_Python.png" style="width: 1399.0px; height: 989.0px;" /></a>
<p class="caption"><span class="caption-text">This Cheatsheet is taken from DataCamp.</span></p>
</div>
</div>
</div>
<div class="section" id="parameters-tuning">
<h2>Parameters Tuning<a class="headerlink" href="#parameters-tuning" title="Permalink to this headline">¶</a></h2>
<p>GridSearchCV (<a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html#sklearn.model_selection.GridSearchCV">http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html#sklearn.model_selection.GridSearchCV</a>) is a method to perform a grid search fro parameter tuning. It does a cross-validation by cutting in different pieces the train and test sets.</p>
<p>Here is an example on artificial data, taken from <a class="reference external" href="https://stackoverflow.com/questions/30102973/how-to-get-best-estimator-on-gridsearchcv-random-forest-classifier-scikit">https://stackoverflow.com/questions/30102973/how-to-get-best-estimator-on-gridsearchcv-random-forest-classifier-scikit</a> :</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.grid_search</span> <span class="kn">import</span> <span class="n">GridSearchCV</span>
<span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_classification</span>
<span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">RandomForestClassifier</span>

<span class="c1"># Build a classification task using 3 informative features</span>
<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">make_classification</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span>
                         <span class="n">n_features</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
                         <span class="n">n_informative</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
                         <span class="n">n_redundant</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                         <span class="n">n_repeated</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                         <span class="n">n_classes</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
                         <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                         <span class="n">shuffle</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>

<span class="n">rfc</span> <span class="o">=</span> <span class="n">RandomForestClassifier</span><span class="p">(</span><span class="n">n_jobs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span><span class="n">max_features</span><span class="o">=</span> <span class="s1">&#39;sqrt&#39;</span> <span class="p">,</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">oob_score</span> <span class="o">=</span> <span class="bp">True</span><span class="p">)</span>

<span class="n">param_grid</span> <span class="o">=</span> <span class="p">{</span>
  <span class="s1">&#39;n_estimators&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">200</span><span class="p">,</span> <span class="mi">700</span><span class="p">],</span>
  <span class="s1">&#39;max_features&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;auto&#39;</span><span class="p">,</span> <span class="s1">&#39;sqrt&#39;</span><span class="p">,</span> <span class="s1">&#39;log2&#39;</span><span class="p">]</span>
<span class="p">}</span>

<span class="n">CV_rfc</span> <span class="o">=</span> <span class="n">GridSearchCV</span><span class="p">(</span><span class="n">estimator</span><span class="o">=</span><span class="n">rfc</span><span class="p">,</span> <span class="n">param_grid</span><span class="o">=</span><span class="n">param_grid</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="n">CV_rfc</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">CV_rfc</span><span class="o">.</span><span class="n">best_params_</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">CV_rfc</span><span class="o">.</span><span class="n">best_score_</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">CV_rfc</span><span class="o">.</span><span class="n">best_estimator_</span><span class="p">)</span>
</pre></div>
</div>
<p>Output:</p>
<p>{‘n_estimators’: 200, ‘max_features’: ‘log2’}
0.86
RandomForestClassifier(bootstrap=True, class_weight=None, criterion=’gini’,</p>
<blockquote>
<div>max_depth=None, max_features=’sqrt’, max_leaf_nodes=None,
min_samples_leaf=1, min_samples_split=2,
min_weight_fraction_leaf=0.0, n_estimators=700, n_jobs=-1,
oob_score=True, random_state=None, verbose=0, warm_start=False)</div></blockquote>
</div>
<div class="section" id="how-to-measure-the-feature-importance-in-rf-or-other-algo">
<h2>How to measure the Feature importance in RF (or other algo)<a class="headerlink" href="#how-to-measure-the-feature-importance-in-rf-or-other-algo" title="Permalink to this headline">¶</a></h2>
<div class="section" id="mean-decrease-accuracy-i-e-how-is-reduced-the-accuracy-if-we-permute-the-values-of-some-feature">
<h3>Mean decrease accuracy (i.e. how is reduced the accuracy if we permute the values of some feature?)<a class="headerlink" href="#mean-decrease-accuracy-i-e-how-is-reduced-the-accuracy-if-we-permute-the-values-of-some-feature" title="Permalink to this headline">¶</a></h3>
<p>Taken <a class="reference external" href="http://blog.datadive.net/selecting-good-features-part-iii-random-forests/">http://blog.datadive.net/selecting-good-features-part-iii-random-forests/</a></p>
<p>Another popular (maybe the most robust?) feature selection method is to directly measure the impact of each feature on accuracy of the model. The general idea is to permute the values of each feature and measure how much the permutation decreases the accuracy of the model. Clearly, for unimportant variables, the permutation should have little to no effect on model accuracy, while permuting important variables should significantly decrease it.
This method is not directly exposed in sklearn, but it is straightforward to implement it. Continuing from the previous example of ranking the features in the Boston housing dataset:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.cross_validation</span> <span class="kn">import</span> <span class="n">ShuffleSplit</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">r2_score</span>
<span class="kn">from</span> <span class="nn">collections</span> <span class="kn">import</span> <span class="n">defaultdict</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">boston</span><span class="p">[</span><span class="s2">&quot;data&quot;</span><span class="p">]</span>
<span class="n">Y</span> <span class="o">=</span> <span class="n">boston</span><span class="p">[</span><span class="s2">&quot;target&quot;</span><span class="p">]</span>

<span class="n">rf</span> <span class="o">=</span> <span class="n">RandomForestRegressor</span><span class="p">()</span>
<span class="n">scores</span> <span class="o">=</span> <span class="n">defaultdict</span><span class="p">(</span><span class="nb">list</span><span class="p">)</span>

<span class="c1">#crossvalidate the scores on a number of different random splits of the data</span>
<span class="k">for</span> <span class="n">train_idx</span><span class="p">,</span> <span class="n">test_idx</span> <span class="ow">in</span> <span class="n">ShuffleSplit</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="p">),</span> <span class="mi">100</span><span class="p">,</span> <span class="o">.</span><span class="mi">3</span><span class="p">):</span>
  <span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="n">train_idx</span><span class="p">],</span> <span class="n">X</span><span class="p">[</span><span class="n">test_idx</span><span class="p">]</span>
  <span class="n">Y_train</span><span class="p">,</span> <span class="n">Y_test</span> <span class="o">=</span> <span class="n">Y</span><span class="p">[</span><span class="n">train_idx</span><span class="p">],</span> <span class="n">Y</span><span class="p">[</span><span class="n">test_idx</span><span class="p">]</span>
  <span class="n">r</span> <span class="o">=</span> <span class="n">rf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">Y_train</span><span class="p">)</span>
  <span class="n">acc</span> <span class="o">=</span> <span class="n">r2_score</span><span class="p">(</span><span class="n">Y_test</span><span class="p">,</span> <span class="n">rf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">))</span>
  <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]):</span>
      <span class="n">X_t</span> <span class="o">=</span> <span class="n">X_test</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
      <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="n">X_t</span><span class="p">[:,</span> <span class="n">i</span><span class="p">])</span>
      <span class="n">shuff_acc</span> <span class="o">=</span> <span class="n">r2_score</span><span class="p">(</span><span class="n">Y_test</span><span class="p">,</span> <span class="n">rf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_t</span><span class="p">))</span>
      <span class="n">scores</span><span class="p">[</span><span class="n">names</span><span class="p">[</span><span class="n">i</span><span class="p">]]</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">acc</span><span class="o">-</span><span class="n">shuff_acc</span><span class="p">)</span><span class="o">/</span><span class="n">acc</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span> <span class="s2">&quot;Features sorted by their score:&quot;</span> <span class="p">)</span>
<span class="k">print</span><span class="p">(</span> <span class="nb">sorted</span><span class="p">([(</span><span class="nb">round</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">score</span><span class="p">),</span> <span class="mi">4</span><span class="p">),</span> <span class="n">feat</span><span class="p">)</span> <span class="k">for</span> <span class="n">feat</span><span class="p">,</span> <span class="n">score</span> <span class="ow">in</span> <span class="n">scores</span><span class="o">.</span><span class="n">items</span><span class="p">()],</span> <span class="n">reverse</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span> <span class="p">)</span>
</pre></div>
</div>
<p>Features sorted by their score:
[(0.7276, ‘LSTAT’), (0.5675, ‘RM’), (0.0867, ‘DIS’), (0.0407, ‘NOX’), (0.0351, ‘CRIM’), (0.0233, ‘PTRATIO’), (0.0168, ‘TAX’), (0.0122, ‘AGE’), (0.005, ‘B’), (0.0048, ‘INDUS’), (0.0043, ‘RAD’), (0.0004, ‘ZN’), (0.0001, ‘CHAS’)]
In this example LSTAT and RM are two features that strongly impact model performance: permuting them decreases model performance by ~73% and ~57% respectively. Keep in mind though that these measurements are made only after the model has been trained (and is depending) on all of these features. This doesn’t mean that if we train the model without one these feature, the model performance will drop by that amount, since other, correlated features can be used instead.</p>
</div>
</div>
<div class="section" id="score-explanation-for-individual-observation-using-lime">
<h2>Score explanation for individual observation (using LIME)<a class="headerlink" href="#score-explanation-for-individual-observation-using-lime" title="Permalink to this headline">¶</a></h2>
<p>Taken from <a class="reference external" href="http://pythondata.com/tag/explanation/">http://pythondata.com/tag/explanation/</a></p>
<p>When working with classification and/or regression techniques, its always good to have the ability to ‘explain’ what your model is doing. Using Local Interpretable Model-agnostic Explanations (LIME), you now have the ability to quickly provide visual explanations of your model(s). According to the paper, LIME is ‘an algorithm that can explain the predictions of any classifier or regressor in a faithful way, by approximating it locally with an interpretable model.’</p>
<div class="section" id="classification-examples">
<h3>Classification examples<a class="headerlink" href="#classification-examples" title="Permalink to this headline">¶</a></h3>
<p>Simplest example: using only continuous features (not categorical):</p>
<p>Here is an example based on the Iris dataset, taken from the (<a class="reference external" href="https://marcotcr.github.io/lime/tutorials/Tutorial%20-%20continuous%20and%20categorical%20features.html">https://marcotcr.github.io/lime/tutorials/Tutorial%20-%20continuous%20and%20categorical%20features.html</a>):</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">sklearn</span>
<span class="kn">import</span> <span class="nn">sklearn.datasets</span>
<span class="kn">import</span> <span class="nn">sklearn.ensemble</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">lime</span>
<span class="kn">import</span> <span class="nn">lime.lime_tabular</span>

<span class="n">iris</span> <span class="o">=</span> <span class="n">sklearn</span><span class="o">.</span><span class="n">datasets</span><span class="o">.</span><span class="n">load_iris</span><span class="p">()</span>
<span class="n">train</span><span class="p">,</span> <span class="n">test</span><span class="p">,</span> <span class="n">labels_train</span><span class="p">,</span> <span class="n">labels_test</span> <span class="o">=</span> <span class="n">sklearn</span><span class="o">.</span><span class="n">model_selection</span><span class="o">.</span><span class="n">train_test_split</span><span class="p">(</span><span class="n">iris</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">iris</span><span class="o">.</span><span class="n">target</span><span class="p">,</span> <span class="n">train_size</span><span class="o">=</span><span class="mf">0.80</span><span class="p">)</span>

<span class="n">rf</span> <span class="o">=</span> <span class="n">sklearn</span><span class="o">.</span><span class="n">ensemble</span><span class="o">.</span><span class="n">RandomForestClassifier</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">500</span><span class="p">)</span>
<span class="n">rf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">train</span><span class="p">,</span> <span class="n">labels_train</span><span class="p">)</span>
</pre></div>
</div>
<p>Output:
RandomForestClassifier(bootstrap=True, class_weight=None, criterion=’gini’,</p>
<blockquote>
<div>max_depth=None, max_features=’auto’, max_leaf_nodes=None,
min_impurity_decrease=0.0, min_impurity_split=None,
min_samples_leaf=1, min_samples_split=2,
min_weight_fraction_leaf=0.0, n_estimators=500, n_jobs=1,
oob_score=False, random_state=None, verbose=0,
warm_start=False)</div></blockquote>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">sklearn</span><span class="o">.</span><span class="n">metrics</span><span class="o">.</span><span class="n">accuracy_score</span><span class="p">(</span><span class="n">labels_test</span><span class="p">,</span> <span class="n">rf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">test</span><span class="p">))</span>
</pre></div>
</div>
<p>Output:
0.90</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">explainer</span> <span class="o">=</span> <span class="n">lime</span><span class="o">.</span><span class="n">lime_tabular</span><span class="o">.</span><span class="n">LimeTabularExplainer</span><span class="p">(</span><span class="n">train</span><span class="p">,</span> <span class="n">feature_names</span><span class="o">=</span><span class="n">iris</span><span class="o">.</span><span class="n">feature_names</span><span class="p">,</span> <span class="n">class_names</span><span class="o">=</span><span class="n">iris</span><span class="o">.</span><span class="n">target_names</span><span class="p">,</span> <span class="n">discretize_continuous</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="c1"># We select one random observation (we can also take one particular, e.g. i=20)</span>
<span class="n">i</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">test</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">exp</span> <span class="o">=</span> <span class="n">explainer</span><span class="o">.</span><span class="n">explain_instance</span><span class="p">(</span><span class="n">test</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">rf</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">,</span> <span class="n">num_features</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">top_labels</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="n">exp</span><span class="o">.</span><span class="n">show_in_notebook</span><span class="p">(</span><span class="n">show_table</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">show_all</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
</pre></div>
</div>
<div class="figure">
<a class="reference internal image-reference" href="_images/LIME_on_IRIS_classification_figure.png"><img alt="map to buried treasure" src="_images/LIME_on_IRIS_classification_figure.png" style="width: 771.0px; height: 113.0px;" /></a>
</div>
</div>
</div>
<div class="section" id="score-explanation-for-individual-observation-using-tree-interpreter">
<h2>Score explanation for individual observation (using tree-interpreter)<a class="headerlink" href="#score-explanation-for-individual-observation-using-tree-interpreter" title="Permalink to this headline">¶</a></h2>
<p>This package is designed for tree based methods like decision tree, random forest, extreme randomized forest.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1">#built on top of the RF model on Iris dataset above</span>

<span class="kn">import</span> <span class="nn">treeinterpreter</span> <span class="kn">as</span> <span class="nn">ti</span>
<span class="n">feature_cols</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;sepal_length&#39;</span><span class="p">,</span><span class="s1">&#39;sepal_width&#39;</span><span class="p">,</span><span class="s1">&#39;petal_length&#39;</span><span class="p">,</span><span class="s1">&#39;petal_width&#39;</span><span class="p">]</span>
<span class="n">Classes</span>      <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;setosa&#39;</span><span class="p">,</span><span class="s1">&#39;versicolor&#39;</span><span class="p">,</span><span class="s1">&#39;virginica&#39;</span><span class="p">]</span>

<span class="k">def</span> <span class="nf">TreeInterpreter_contributions</span><span class="p">(</span><span class="n">x_test</span><span class="p">,</span><span class="n">fit</span><span class="p">,</span><span class="n">feature_cols</span><span class="p">):</span>

  <span class="c1"># Let’s predict now for a single instance.</span>
  <span class="n">instance</span> <span class="o">=</span> <span class="n">x_test</span>
  <span class="n">fit</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">instance</span><span class="p">)</span>

  <span class="c1"># Breakdown of feature contributions:</span>
  <span class="n">prediction</span><span class="p">,</span> <span class="n">bias</span><span class="p">,</span> <span class="n">contributions</span> <span class="o">=</span> <span class="n">ti</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">fit</span><span class="p">,</span> <span class="n">instance</span><span class="p">)</span>

  <span class="k">return</span> <span class="n">prediction</span><span class="p">,</span> <span class="n">bias</span><span class="p">,</span> <span class="n">contributions</span>


<span class="k">def</span> <span class="nf">waterfall</span><span class="p">(</span><span class="n">series</span><span class="p">,</span><span class="n">pos_name</span><span class="p">,</span><span class="n">neg_name</span><span class="p">):</span> <span class="c1">#see http://pbpython.com/waterfall-chart.html</span>
  <span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">7</span><span class="p">,</span><span class="mi">7</span><span class="p">))</span>
  <span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="n">pos_name</span><span class="p">:</span><span class="n">np</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="n">series</span><span class="p">,</span><span class="mi">0</span><span class="p">),</span><span class="n">neg_name</span><span class="p">:</span><span class="n">np</span><span class="o">.</span><span class="n">minimum</span><span class="p">(</span><span class="n">series</span><span class="p">,</span><span class="mi">0</span><span class="p">)})</span>
  <span class="n">blank</span> <span class="o">=</span> <span class="n">series</span><span class="o">.</span><span class="n">cumsum</span><span class="p">()</span><span class="o">.</span><span class="n">shift</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">fillna</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
  <span class="n">df</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">kind</span><span class="o">=</span><span class="s1">&#39;bar&#39;</span><span class="p">,</span> <span class="n">stacked</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">bottom</span><span class="o">=</span><span class="n">blank</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;r&#39;</span><span class="p">,</span><span class="s1">&#39;b&#39;</span><span class="p">],</span><span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">)</span>
  <span class="n">ax</span><span class="o">.</span><span class="n">get_children</span><span class="p">()[</span><span class="nb">len</span><span class="p">(</span><span class="n">series</span><span class="p">)</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_color</span><span class="p">(</span><span class="s1">&#39;g&#39;</span><span class="p">)</span> <span class="c1">#Put the prediction in green</span>
  <span class="n">step</span> <span class="o">=</span> <span class="n">blank</span><span class="o">.</span><span class="n">reset_index</span><span class="p">(</span><span class="n">drop</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span><span class="o">.</span><span class="n">shift</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
  <span class="n">step</span><span class="p">[</span><span class="mi">1</span><span class="p">::</span><span class="mi">3</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">nan</span>
  <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">step</span><span class="o">.</span><span class="n">index</span><span class="p">,</span> <span class="n">step</span><span class="o">.</span><span class="n">values</span><span class="p">,</span><span class="s1">&#39;-k&#39;</span><span class="p">,</span><span class="n">linewidth</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">fig</span>


<span class="n">prediction</span><span class="p">,</span> <span class="n">bias</span><span class="p">,</span> <span class="n">contributions</span> <span class="o">=</span> <span class="n">TreeInterpreter_contributions</span><span class="p">(</span><span class="n">x_test</span><span class="p">,</span><span class="n">clf</span><span class="p">,</span><span class="n">feature_cols</span><span class="p">)</span>
<span class="n">class_selected</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">prediction</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">class_name_selected</span> <span class="o">=</span> <span class="n">Classes</span><span class="p">[</span><span class="n">class_selected</span><span class="p">]</span>
<span class="k">print</span><span class="p">(</span><span class="n">class_selected</span><span class="p">,</span><span class="n">class_name_selected</span><span class="p">)</span>
<span class="n">test2</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">([</span><span class="n">bias</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="n">class_selected</span><span class="p">]]</span><span class="o">+</span><span class="nb">list</span><span class="p">(</span><span class="n">contributions</span><span class="p">[</span><span class="mi">0</span><span class="p">][:,</span><span class="n">class_selected</span><span class="p">])</span><span class="o">+</span><span class="p">[</span><span class="o">-</span><span class="n">prediction</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="n">class_selected</span><span class="p">]],</span><span class="n">index</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;bias&#39;</span><span class="p">]</span><span class="o">+</span><span class="n">feature_cols</span><span class="o">+</span><span class="p">[</span><span class="s1">&#39;Net&#39;</span><span class="p">])</span>
<span class="n">fig</span> <span class="o">=</span> <span class="n">waterfall</span><span class="p">(</span><span class="n">test2</span><span class="p">,</span><span class="n">class_name_selected</span><span class="p">,</span><span class="s1">&#39;Other labels&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">([</span><span class="o">-</span><span class="mf">0.1</span><span class="p">,</span><span class="mf">1.1</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axhline</span><span class="p">(</span><span class="n">y</span><span class="o">=</span><span class="mf">0.</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">color</span> <span class="o">=</span> <span class="s1">&#39;k&#39;</span><span class="p">,</span><span class="n">dashes</span><span class="o">=</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span><span class="mi">5</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<div class="figure">
<a class="reference internal image-reference" href="_images/Contributions.png"><img alt="Contributions to the prediction probability by tree-interpreter" src="_images/Contributions.png" style="width: 491.0px; height: 514.0px;" /></a>
</div>
</div>
<div class="section" id="very-important-update-in-the-field-of-interpretation">
<h2>Very important update in the field of interpretation:<a class="headerlink" href="#very-important-update-in-the-field-of-interpretation" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li>ELI5 (Explain me like I am 5): <a class="reference external" href="https://eli5.readthedocs.io/en/latest/">https://eli5.readthedocs.io/en/latest/</a></li>
<li>SHAP:<ul>
<li><a class="reference external" href="https://github.com/slundberg/shap">https://github.com/slundberg/shap</a> : main intro + examples</li>
<li><a class="reference external" href="http://www.f1-predictor.com/model-interpretability-with-shap/">http://www.f1-predictor.com/model-interpretability-with-shap/</a></li>
<li><a class="reference external" href="https://medium.com/civis-analytics/demystifying-black-box-models-with-shap-value-analysis-3e20b536fc80">https://medium.com/civis-analytics/demystifying-black-box-models-with-shap-value-analysis-3e20b536fc80</a></li>
</ul>
</li>
</ul>
</div>
<div class="section" id="ensemble-classification">
<h2>Ensemble Classification<a class="headerlink" href="#ensemble-classification" title="Permalink to this headline">¶</a></h2>
<p>See <a class="reference external" href="http://scikit-learn.org/stable/modules/ensemble.html">http://scikit-learn.org/stable/modules/ensemble.html</a>
<a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingClassifier.html">http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingClassifier.html</a>
<a class="reference external" href="http://scikit-learn.org/stable/auto_examples/ensemble/plot_voting_decision_regions.html#sphx-glr-auto-examples-ensemble-plot-voting-decision-regions-py">http://scikit-learn.org/stable/auto_examples/ensemble/plot_voting_decision_regions.html#sphx-glr-auto-examples-ensemble-plot-voting-decision-regions-py</a>
<a class="reference external" href="http://scikit-learn.org/stable/auto_examples/ensemble/plot_voting_probas.html#sphx-glr-auto-examples-ensemble-plot-voting-probas-py">http://scikit-learn.org/stable/auto_examples/ensemble/plot_voting_probas.html#sphx-glr-auto-examples-ensemble-plot-voting-probas-py</a></p>
<p>Here a case with 3 classifiers, with a parameter grid search:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">VotingClassifier</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">GridSearchCV</span>
<span class="n">clf1</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">clf2</span> <span class="o">=</span> <span class="n">RandomForestClassifier</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">clf3</span> <span class="o">=</span> <span class="n">GaussianNB</span><span class="p">()</span>
<span class="n">eclf</span> <span class="o">=</span> <span class="n">VotingClassifier</span><span class="p">(</span><span class="n">estimators</span><span class="o">=</span><span class="p">[(</span><span class="s1">&#39;lr&#39;</span><span class="p">,</span> <span class="n">clf1</span><span class="p">),</span> <span class="p">(</span><span class="s1">&#39;rf&#39;</span><span class="p">,</span> <span class="n">clf2</span><span class="p">),</span> <span class="p">(</span><span class="s1">&#39;gnb&#39;</span><span class="p">,</span> <span class="n">clf3</span><span class="p">)],</span> <span class="n">voting</span><span class="o">=</span><span class="s1">&#39;soft&#39;</span><span class="p">)</span> <span class="c1">#voting=hard is majority voting, while voting soft is sum of probabilities, less strong.</span>
<span class="c1">#eclf = VotingClassifier(estimators=[(&#39;lr&#39;, clf1), (&#39;rf&#39;, clf2), (&#39;gnb&#39;, clf3)], voting=&#39;soft&#39;, weights=[2,5,1]) #if we want to weigth the classifiers</span>

<span class="n">params</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;lr__C&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">100.0</span><span class="p">],</span> <span class="s1">&#39;rf__n_estimators&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">20</span><span class="p">,</span> <span class="mi">200</span><span class="p">],}</span>

<span class="n">grid</span> <span class="o">=</span> <span class="n">GridSearchCV</span><span class="p">(</span><span class="n">estimator</span><span class="o">=</span><span class="n">eclf</span><span class="p">,</span> <span class="n">param_grid</span><span class="o">=</span><span class="n">params</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="n">grid</span> <span class="o">=</span> <span class="n">grid</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">iris</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">iris</span><span class="o">.</span><span class="n">target</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="clustering">
<h2>Clustering<a class="headerlink" href="#clustering" title="Permalink to this headline">¶</a></h2>
<p>Excellent intro to the field: <a class="reference external" href="https://towardsdatascience.com/the-5-clustering-algorithms-data-scientists-need-to-know-a36d136ef68">https://towardsdatascience.com/the-5-clustering-algorithms-data-scientists-need-to-know-a36d136ef68</a></p>
<div class="section" id="gaussian-mixture-model-gmm">
<h3>Gaussian Mixture Model (GMM)<a class="headerlink" href="#gaussian-mixture-model-gmm" title="Permalink to this headline">¶</a></h3>
<p><a class="reference external" href="http://scikit-learn.org/stable/modules/mixture.html">http://scikit-learn.org/stable/modules/mixture.html</a></p>
<ol class="arabic simple">
<li>Classical GMM</li>
<li>Bayesian GMM</li>
</ol>
<p>Here we can detect the number of component automatically (in theory).</p>
<p>Due to its Bayesian nature, the variational algorithm needs more hyper- parameters than expectation-maximization, the most important of these being the concentration parameter weight_concentration_prior. Specifying a low value for the concentration prior will make the model put most of the weight on few components set the remaining components weights very close to zero. High values of the concentration prior will allow a larger number of components to be active in the mixture.</p>
<p>The parameters implementation of the BayesianGaussianMixture class proposes two types of prior for the weights distribution: a finite mixture model with Dirichlet distribution and an infinite mixture model with the Dirichlet Process. In practice Dirichlet Process inference algorithm is approximated and uses a truncated distribution with a fixed maximum number of components (called the Stick-breaking representation). The number of components actually used almost always depends on the data.</p>
<p>Important parameters:</p>
<ul class="simple">
<li>weight_concentration_prior_type : str, defaults to ‘dirichlet_process’.</li>
</ul>
<p>String describing the type of the weight concentration prior. Must be one of:</p>
<p>‘dirichlet_process’ (using the Stick-breaking representation),
‘dirichlet_distribution’ (can favor more uniform weights).</p>
<ul class="simple">
<li>weight_concentration_prior : float | None, optional.</li>
</ul>
<p>The dirichlet concentration of each component on the weight distribution (Dirichlet). This is commonly called gamma in the literature. The higher concentration puts more mass in the center and will lead to more components being active, while a lower concentration parameter will lead to more mass at the edge of the mixture weights simplex. The value of the parameter must be greater than 0. If it is None, it’s set to 1. / n_components.</p>
<p>See See <a class="reference external" href="http://scikit-learn.org/stable/modules/mixture.html#variational-bayesian-gaussian-mixture">http://scikit-learn.org/stable/modules/mixture.html#variational-bayesian-gaussian-mixture</a></p>
<p>See <a class="reference external" href="http://scikit-learn.org/stable/modules/generated/sklearn.mixture.BayesianGaussianMixture.html#sklearn.mixture.BayesianGaussianMixture">http://scikit-learn.org/stable/modules/generated/sklearn.mixture.BayesianGaussianMixture.html#sklearn.mixture.BayesianGaussianMixture</a></p>
<p>Based on <a class="reference external" href="http://scikit-learn.org/stable/auto_examples/mixture/plot_concentration_prior.html#sphx-glr-auto-examples-mixture-plot-concentration-prior-py">http://scikit-learn.org/stable/auto_examples/mixture/plot_concentration_prior.html#sphx-glr-auto-examples-mixture-plot-concentration-prior-py</a></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="kn">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>

<span class="kn">from</span> <span class="nn">sklearn.grid_search</span> <span class="kn">import</span> <span class="n">GridSearchCV</span>
<span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_classification</span>
<span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">RandomForestClassifier</span>
<span class="kn">from</span> <span class="nn">sklearn.mixture</span> <span class="kn">import</span> <span class="n">BayesianGaussianMixture</span>


<span class="kn">import</span> <span class="nn">matplotlib</span> <span class="kn">as</span> <span class="nn">mpl</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="kn">as</span> <span class="nn">plt</span>

<span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>

<span class="c1"># THE GENERATED DATASET</span>

<span class="c1"># Parameters of the dataset</span>
<span class="n">random_state</span><span class="p">,</span> <span class="n">n_components</span><span class="p">,</span> <span class="n">n_features</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">2</span>
<span class="n">colors</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="s1">&#39;#0072B2&#39;</span><span class="p">,</span> <span class="s1">&#39;#F0E442&#39;</span><span class="p">,</span> <span class="s1">&#39;#D55E00&#39;</span><span class="p">])</span>

<span class="n">covars</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[[</span><span class="o">.</span><span class="mi">7</span><span class="p">,</span> <span class="o">.</span><span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="o">.</span><span class="mi">0</span><span class="p">,</span> <span class="o">.</span><span class="mi">1</span><span class="p">]],</span>
                   <span class="p">[[</span><span class="o">.</span><span class="mi">5</span><span class="p">,</span> <span class="o">.</span><span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="o">.</span><span class="mi">0</span><span class="p">,</span> <span class="o">.</span><span class="mi">1</span><span class="p">]],</span>
                   <span class="p">[[</span><span class="o">.</span><span class="mi">5</span><span class="p">,</span> <span class="o">.</span><span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="o">.</span><span class="mi">0</span><span class="p">,</span> <span class="o">.</span><span class="mi">1</span><span class="p">]]])</span>
<span class="n">samples</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">500</span><span class="p">,</span> <span class="mi">500</span><span class="p">,</span> <span class="mi">500</span><span class="p">])</span>
<span class="n">means</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
                  <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
                  <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">]])</span>


<span class="c1">#covars = np.array([[[.7, .0], [.0, .1]],</span>
<span class="c1">#                   [[.5, .0], [.0, .2]],</span>
<span class="c1">#                   [[.5, .0], [.0, .3]]])</span>
<span class="c1">#samples = np.array([5000, 5000, 5000])</span>
<span class="c1">#means = np.array([[1, 0],</span>
<span class="c1">#                  [2, 1],</span>
<span class="c1">#                  [1, 1]])</span>

<span class="c1"># Build a classification task using 3 informative features  (COULD USE THIS)</span>
<span class="c1">#X, y = make_classification(n_samples=1000,</span>
<span class="c1">#                         n_features=10,</span>
<span class="c1">#                         n_informative=3,</span>
<span class="c1">#                         n_redundant=0,</span>
<span class="c1">#                         n_repeated=0,</span>
<span class="c1">#                         n_classes=2,</span>
<span class="c1">#                         random_state=0,</span>
<span class="c1">#                         shuffle=False)</span>

<span class="c1"># Generate data</span>
<span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">RandomState</span><span class="p">(</span><span class="n">random_state</span><span class="p">)</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">([</span>
    <span class="n">rng</span><span class="o">.</span><span class="n">multivariate_normal</span><span class="p">(</span><span class="n">means</span><span class="p">[</span><span class="n">j</span><span class="p">],</span> <span class="n">covars</span><span class="p">[</span><span class="n">j</span><span class="p">],</span> <span class="n">samples</span><span class="p">[</span><span class="n">j</span><span class="p">])</span>
    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_components</span><span class="p">)])</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">([</span><span class="n">j</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">samples</span><span class="p">[</span><span class="n">j</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">int</span><span class="p">)</span>
                    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_components</span><span class="p">)])</span>

<span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span><span class="n">y</span><span class="o">.</span><span class="n">shape</span>

<span class="n">estimator</span> <span class="o">=</span> <span class="n">BayesianGaussianMixture</span><span class="p">(</span>
      <span class="n">weight_concentration_prior_type</span><span class="o">=</span><span class="s2">&quot;dirichlet_process&quot;</span><span class="p">,</span> <span class="c1">#(i.e. infinite)  #or &quot;dirichlet_distribution&quot; (i.e. finite)</span>
      <span class="n">n_components</span><span class="o">=</span><span class="mi">5</span><span class="o">*</span><span class="n">n_components</span><span class="p">,</span> <span class="n">reg_covar</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">init_params</span><span class="o">=</span><span class="s1">&#39;random&#39;</span><span class="p">,</span>
      <span class="n">max_iter</span><span class="o">=</span><span class="mi">1500</span><span class="p">,</span> <span class="n">mean_precision_prior</span><span class="o">=.</span><span class="mi">8</span><span class="p">,</span>
      <span class="n">random_state</span><span class="o">=</span><span class="n">random_state</span><span class="p">)</span>  <span class="c1">#, [1, 1000, 100000])</span>

<span class="n">estimator</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">plot_ellipses</span><span class="p">(</span><span class="n">ax</span><span class="p">,</span> <span class="n">weights</span><span class="p">,</span> <span class="n">means</span><span class="p">,</span> <span class="n">covars</span><span class="p">,</span><span class="n">edgecolor</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span><span class="n">by_weights</span><span class="o">=</span><span class="s1">&#39;no&#39;</span><span class="p">):</span>
  <span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">means</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
      <span class="n">eig_vals</span><span class="p">,</span> <span class="n">eig_vecs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">eigh</span><span class="p">(</span><span class="n">covars</span><span class="p">[</span><span class="n">n</span><span class="p">])</span>
      <span class="n">unit_eig_vec</span> <span class="o">=</span> <span class="n">eig_vecs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">eig_vecs</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
      <span class="n">angle</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arctan2</span><span class="p">(</span><span class="n">unit_eig_vec</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">unit_eig_vec</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
      <span class="c1"># Ellipse needs degrees</span>
      <span class="n">angle</span> <span class="o">=</span> <span class="mi">180</span> <span class="o">*</span> <span class="n">angle</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">pi</span>
      <span class="c1"># eigenvector normalization</span>
      <span class="n">eig_vals</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">eig_vals</span><span class="p">)</span>
      <span class="n">ell</span> <span class="o">=</span> <span class="n">mpl</span><span class="o">.</span><span class="n">patches</span><span class="o">.</span><span class="n">Ellipse</span><span class="p">(</span><span class="n">means</span><span class="p">[</span><span class="n">n</span><span class="p">],</span> <span class="n">eig_vals</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">eig_vals</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
                                <span class="mi">180</span> <span class="o">+</span> <span class="n">angle</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="n">edgecolor</span><span class="p">)</span>
      <span class="n">ell</span><span class="o">.</span><span class="n">set_clip_box</span><span class="p">(</span><span class="n">ax</span><span class="o">.</span><span class="n">bbox</span><span class="p">)</span>
      <span class="k">if</span> <span class="n">by_weights</span><span class="o">==</span><span class="s1">&#39;yes&#39;</span><span class="p">:</span> <span class="n">ell</span><span class="o">.</span><span class="n">set_alpha</span><span class="p">(</span><span class="mi">5</span><span class="o">*</span><span class="n">weights</span><span class="p">[</span><span class="n">n</span><span class="p">])</span>
      <span class="c1">#ell.set_facecolor(&#39;#56B4E9&#39;)</span>
      <span class="n">ell</span><span class="o">.</span><span class="n">set_facecolor</span><span class="p">(</span><span class="s1">&#39;none&#39;</span><span class="p">)</span>
      <span class="n">ax</span><span class="o">.</span><span class="n">add_artist</span><span class="p">(</span><span class="n">ell</span><span class="p">)</span>

<span class="n">fig</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="mi">1</span><span class="p">,(</span><span class="mi">17</span><span class="p">,</span><span class="mi">6</span><span class="p">))</span>

<span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">121</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span><span class="n">X</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;o&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">colors</span><span class="p">[</span><span class="n">y</span><span class="p">])</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;recency&#39;</span><span class="p">,</span><span class="n">fontsize</span><span class="o">=</span><span class="mi">15</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;frequency&#39;</span><span class="p">,</span><span class="n">fontsize</span><span class="o">=</span><span class="mi">15</span><span class="p">)</span>

<span class="n">plot_ellipses</span><span class="p">(</span><span class="n">ax</span><span class="p">,</span> <span class="n">estimator</span><span class="o">.</span><span class="n">weights_</span><span class="p">,</span> <span class="n">estimator</span><span class="o">.</span><span class="n">means_</span><span class="p">,</span>
              <span class="n">estimator</span><span class="o">.</span><span class="n">covariances_</span><span class="p">,</span><span class="n">edgecolor</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">)</span>
<span class="n">plot_ellipses</span><span class="p">(</span><span class="n">ax</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span> <span class="n">means</span><span class="p">,</span>
              <span class="n">covars</span><span class="p">)</span>


<span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">122</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span><span class="n">X</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;o&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">colors</span><span class="p">[</span><span class="n">y</span><span class="p">])</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;recency&#39;</span><span class="p">,</span><span class="n">fontsize</span><span class="o">=</span><span class="mi">15</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;frequency&#39;</span><span class="p">,</span><span class="n">fontsize</span><span class="o">=</span><span class="mi">15</span><span class="p">)</span>

<span class="n">plot_ellipses</span><span class="p">(</span><span class="n">ax</span><span class="p">,</span> <span class="n">estimator</span><span class="o">.</span><span class="n">weights_</span><span class="p">,</span> <span class="n">estimator</span><span class="o">.</span><span class="n">means_</span><span class="p">,</span>
              <span class="n">estimator</span><span class="o">.</span><span class="n">covariances_</span><span class="p">,</span><span class="n">edgecolor</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span><span class="n">by_weights</span><span class="o">=</span><span class="s1">&#39;yes&#39;</span><span class="p">)</span>
<span class="n">plot_ellipses</span><span class="p">(</span><span class="n">ax</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span> <span class="n">means</span><span class="p">,</span>
              <span class="n">covars</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="n">estimator</span><span class="o">.</span><span class="n">weights_</span>

<span class="n">array</span><span class="p">([</span><span class="mf">3.23183804e-01</span><span class="p">,</span> <span class="mf">1.07006400e-01</span><span class="p">,</span> <span class="mf">3.24464101e-01</span><span class="p">,</span> <span class="mf">2.45301497e-01</span><span class="p">,</span>
     <span class="mf">4.14353274e-05</span><span class="p">,</span> <span class="mf">2.58970793e-06</span><span class="p">,</span> <span class="mf">1.61856745e-07</span><span class="p">,</span> <span class="mf">1.01160466e-08</span><span class="p">,</span>
     <span class="mf">6.32252912e-10</span><span class="p">,</span> <span class="mf">3.95158070e-11</span><span class="p">,</span> <span class="mf">2.46973794e-12</span><span class="p">,</span> <span class="mf">1.54358621e-13</span><span class="p">,</span>
     <span class="mf">9.64741381e-15</span><span class="p">,</span> <span class="mf">6.02963363e-16</span><span class="p">,</span> <span class="mf">3.76852102e-17</span><span class="p">])</span>
</pre></div>
</div>
<div class="figure">
<a class="reference internal image-reference" href="_images/GMM_bayes.png"><img alt="map to buried treasure" src="_images/GMM_bayes.png" style="width: 989.0px; height: 366.0px;" /></a>
</div>
</div>
</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="Pyspark_Documentation.html" class="btn btn-neutral float-right" title="Pyspark" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="Pandas_Documentation.html" class="btn btn-neutral float-left" title="Python, Jupyter and Pandas" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2017, pmeu

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>